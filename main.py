"""
Hybrid Search ä¸»ç¨‹å¼ï¼šæ•´åˆ BM25 å’Œå‘é‡æª¢ç´¢
"""
import os
import json
from typing import List, Dict, Optional
from src import (
    DocumentProcessor, 
    BM25Retriever, 
    VectorRetriever, 
    HybridSearch,
    Reranker,
    RAGPipeline,
    PromptFormatter,
    OllamaLLM
)


def get_test_queries() -> Dict[str, List[str]]:
    """
    ç²å–åˆ†é¡çš„æ¸¬è©¦æŸ¥è©¢
    
    Returns:
        åŒ…å«ä¸åŒé¡å‹æŸ¥è©¢çš„å­—å…¸
    """
    return {
        # é—œéµè©åŒ¹é…æ¸¬è©¦ï¼šæ¸¬è©¦ BM25 çš„å„ªå‹¢ï¼ˆç²¾ç¢ºé—œéµè©åŒ¹é…ï¼‰
        "keyword": [
            "transformer architecture",
            "attention mechanism",
            "gradient descent",
            "neural network",
        ],
        
        # èªç¾©ç†è§£æ¸¬è©¦ï¼šæ¸¬è©¦å‘é‡æª¢ç´¢çš„å„ªå‹¢ï¼ˆåŒç¾©è©å’Œèªç¾©ç›¸ä¼¼ï¼‰
        "semantic": [
            "How do deep learning models learn?",
            "What is the difference between supervised and unsupervised learning?",
            "methods for improving model generalization",
            "ways to reduce overfitting in neural networks",
        ],
        
        # è¤‡é›œæŸ¥è©¢æ¸¬è©¦ï¼šæ¸¬è©¦æ··åˆæœå°‹å’Œé‡æ’åºçš„å„ªå‹¢
        "complex": [
            "transformer models with attention mechanism for natural language processing",
            "optimization techniques for training large language models efficiently",
            "recent advances in few-shot learning and meta-learning",
            "comparison between BERT and GPT architectures",
        ],
        
        # æŠ½è±¡æ¦‚å¿µæŸ¥è©¢ï¼šæ¸¬è©¦èªç¾©ç†è§£æ·±åº¦
        "abstract": [
            "How can AI systems understand context?",
            "What makes a model interpretable?",
            "scalability challenges in machine learning",
        ],
        
        # é‚Šç•Œæƒ…æ³æ¸¬è©¦
        "edge_cases": [
            "AI",  # å¤ªçŸ­
            "machine learning deep learning neural networks artificial intelligence",  # å¤ªé•·ï¼Œé—œéµè©å †ç Œ
        ],
    }


def compare_retrieval_methods(
    bm25_retriever: BM25Retriever,
    vector_retriever: VectorRetriever,
    hybrid_search: HybridSearch,
    rag_pipeline: RAGPipeline,
    queries: List[str]
) -> List[Dict]:
    """
    å°æ¯”ä¸åŒæª¢ç´¢æ–¹æ³•çš„æ•ˆæœ
    
    Args:
        bm25_retriever: BM25 æª¢ç´¢å™¨
        vector_retriever: å‘é‡æª¢ç´¢å™¨
        hybrid_search: æ··åˆæœå°‹å¯¦ä¾‹
        rag_pipeline: RAG ç®¡ç·šå¯¦ä¾‹
        queries: æ¸¬è©¦æŸ¥è©¢åˆ—è¡¨
        
    Returns:
        å°æ¯”çµæœåˆ—è¡¨
    """
    results_comparison = []
    
    for query in queries:
        print(f"\n{'='*60}")
        print(f"æŸ¥è©¢: '{query}'")
        print(f"{'='*60}")
        
        try:
            # 1. åƒ… BM25
            bm25_results = bm25_retriever.retrieve(query, top_k=5)
            
            # 2. åƒ…å‘é‡æª¢ç´¢
            vector_results = vector_retriever.retrieve(query, top_k=5)
            
            # 3. æ··åˆæœå°‹ï¼ˆç„¡é‡æ’åºï¼‰
            hybrid_results = hybrid_search.retrieve(query, top_k=5)
            
            # 4. å®Œæ•´ RAG Pipelineï¼ˆæœ‰é‡æ’åºï¼‰
            rag_results, stats = rag_pipeline.query(
                query, 
                top_k=5, 
                enable_rerank=True,
                return_stats=True
            )
            
            # åˆ†æçµæœå·®ç•°
            comparison = {
                "query": query,
                "bm25_top1": bm25_results[0]['metadata']['title'] if bm25_results else None,
                "vector_top1": vector_results[0]['metadata']['title'] if vector_results else None,
                "hybrid_top1": hybrid_results[0]['metadata']['title'] if hybrid_results else None,
                "rag_top1": rag_results[0]['metadata']['title'] if rag_results else None,
                "stats": stats
            }
            
            results_comparison.append(comparison)
            
            # æ‰“å°å°æ¯”
            print("\n[BM25 æœ€ä½³çµæœ]")
            if bm25_results:
                print(f"  {bm25_results[0]['metadata']['title']}")
                print(f"  åˆ†æ•¸: {bm25_results[0].get('score', 0):.4f}")
            
            print("\n[å‘é‡æª¢ç´¢æœ€ä½³çµæœ]")
            if vector_results:
                print(f"  {vector_results[0]['metadata']['title']}")
                print(f"  ç›¸ä¼¼åº¦åˆ†æ•¸: {vector_results[0].get('score', 0):.4f}")
            
            print("\n[æ··åˆæœå°‹æœ€ä½³çµæœ]")
            if hybrid_results:
                print(f"  {hybrid_results[0]['metadata']['title']}")
                print(f"  æ··åˆåˆ†æ•¸: {hybrid_results[0].get('hybrid_score', 0):.4f}")
            
            print("\n[RAG Pipeline æœ€ä½³çµæœï¼ˆé‡æ’åºå¾Œï¼‰]")
            if rag_results:
                print(f"  {rag_results[0]['metadata']['title']}")
                print(f"  é‡æ’åºåˆ†æ•¸: {rag_results[0].get('rerank_score', 0):.4f}")
                print(f"  åŸå§‹æ··åˆåˆ†æ•¸: {rag_results[0].get('hybrid_score', 0):.4f}")
                print(f"  æ€§èƒ½: {stats['total_time']:.2f}s")
                print(f"    - å¬å›: {stats['recall_time']:.2f}s")
                print(f"    - é‡æ’: {stats['rerank_time']:.2f}s")
            
        except Exception as e:
            print(f"âš ï¸  æŸ¥è©¢è™•ç†å‡ºéŒ¯: {e}")
            continue
    
    return results_comparison


def evaluate_retrieval_quality(
    results: List[Dict], 
    query: str, 
    expected_keywords: Optional[List[str]] = None
) -> Dict:
    """
    è©•ä¼°æª¢ç´¢è³ªé‡
    
    Args:
        results: æª¢ç´¢çµæœåˆ—è¡¨
        query: æŸ¥è©¢æ–‡æœ¬
        expected_keywords: æœŸæœ›çš„é—œéµè©åˆ—è¡¨ï¼ˆå¯é¸ï¼‰
        
    Returns:
        åŒ…å«è©•ä¼°æŒ‡æ¨™çš„å­—å…¸
    """
    metrics = {
        "diversity": 0.0,  # çµæœå¤šæ¨£æ€§ï¼ˆä¸åŒè«–æ–‡çš„æ•¸é‡ï¼‰
        "score_distribution": {},  # åˆ†æ•¸åˆ†ä½ˆ
        "coverage": 0.0,  # è¦†è“‹åº¦ï¼ˆæ˜¯å¦æ¶µè“‹æŸ¥è©¢çš„å„å€‹æ–¹é¢ï¼‰
    }
    
    if not results:
        return metrics
    
    # 1. æª¢æŸ¥çµæœå¤šæ¨£æ€§
    unique_titles = set(r['metadata']['title'] for r in results)
    metrics["diversity"] = len(unique_titles) / len(results)
    
    # 2. åˆ†æ•¸åˆ†ä½ˆ
    scores = []
    for r in results:
        score = r.get('rerank_score') or r.get('hybrid_score') or r.get('score', 0)
        scores.append(score)
    
    if scores:
        metrics["score_distribution"] = {
            "max": max(scores),
            "min": min(scores),
            "avg": sum(scores) / len(scores),
        }
        
        # è¨ˆç®—æ¨™æº–å·®
        avg = metrics["score_distribution"]["avg"]
        variance = sum((s - avg) ** 2 for s in scores) / len(scores)
        metrics["score_distribution"]["std"] = variance ** 0.5
    
    # 3. é—œéµè©è¦†è“‹ï¼ˆå¦‚æœæä¾›äº†æœŸæœ›é—œéµè©ï¼‰
    if expected_keywords:
        content_text = " ".join([r['content'].lower() for r in results])
        found_keywords = sum(1 for kw in expected_keywords if kw.lower() in content_text)
        metrics["coverage"] = found_keywords / len(expected_keywords) if expected_keywords else 0
    
    return metrics


def comprehensive_rag_test(
    bm25_retriever: BM25Retriever,
    vector_retriever: VectorRetriever,
    hybrid_search: HybridSearch,
    rag_pipeline: RAGPipeline
):
    """
    å…¨é¢çš„ RAG ç³»çµ±æ¸¬è©¦
    
    Args:
        bm25_retriever: BM25 æª¢ç´¢å™¨
        vector_retriever: å‘é‡æª¢ç´¢å™¨
        hybrid_search: æ··åˆæœå°‹å¯¦ä¾‹
        rag_pipeline: RAG ç®¡ç·šå¯¦ä¾‹
    """
    print("\n" + "=" * 60)
    print("å…¨é¢çš„ RAG ç³»çµ±åŠŸæ•ˆæ¸¬è©¦")
    print("=" * 60)
    
    # ç²å–æ¸¬è©¦æŸ¥è©¢
    test_queries_dict = get_test_queries()
    
    # 1. åŸºç¤åŠŸèƒ½æ¸¬è©¦ï¼ˆä½¿ç”¨é—œéµè©å’Œèªç¾©æŸ¥è©¢ï¼‰
    print("\n" + "-" * 60)
    print("1. åŸºç¤åŠŸèƒ½æ¸¬è©¦")
    print("-" * 60)
    
    basic_queries = test_queries_dict["keyword"][:2] + test_queries_dict["semantic"][:2]
    
    for query in basic_queries:
        print(f"\næŸ¥è©¢: '{query}'")
        print("-" * 40)
        
        try:
            results, stats = rag_pipeline.query(
                query, 
                top_k=5, 
                return_stats=True
            )
            
            print(f"æ‰¾åˆ° {len(results)} å€‹çµæœ")
            print(f"è€—æ™‚: {stats['total_time']:.2f}s (å¬å›: {stats['recall_time']:.2f}s, é‡æ’: {stats['rerank_time']:.2f}s)")
            
            # è©•ä¼°è³ªé‡
            quality = evaluate_retrieval_quality(results, query)
            print(f"çµæœå¤šæ¨£æ€§: {quality['diversity']:.2%}")
            if quality['score_distribution']:
                print(f"åˆ†æ•¸ç¯„åœ: [{quality['score_distribution']['min']:.4f}, {quality['score_distribution']['max']:.4f}]")
                print(f"å¹³å‡åˆ†æ•¸: {quality['score_distribution']['avg']:.4f}")
            
            # é¡¯ç¤ºå‰ 3 å€‹çµæœ
            for i, r in enumerate(results[:3], 1):
                print(f"\n  {i}. {r['metadata']['title']}")
                print(f"     é‡æ’åºåˆ†æ•¸: {r.get('rerank_score', 0):.4f}")
                print(f"     å…§å®¹é è¦½: {r['content'][:100]}...")
                
        except Exception as e:
            print(f"âš ï¸  æŸ¥è©¢è™•ç†å‡ºéŒ¯: {e}")
            continue
    
    # 2. è¤‡é›œæŸ¥è©¢æ¸¬è©¦
    print("\n" + "-" * 60)
    print("2. è¤‡é›œæŸ¥è©¢æ¸¬è©¦")
    print("-" * 60)
    
    complex_queries = test_queries_dict["complex"][:2]
    
    for query in complex_queries:
        print(f"\næŸ¥è©¢: '{query}'")
        print("-" * 40)
        
        try:
            results, stats = rag_pipeline.query(
                query, 
                top_k=5, 
                return_stats=True
            )
            
            print(f"æ‰¾åˆ° {len(results)} å€‹çµæœ")
            print(f"å¯¦éš›å¬å›æ•¸é‡: {stats['recall_k']} ç­†")
            print(f"è€—æ™‚: {stats['total_time']:.2f}s")
            
            for i, r in enumerate(results[:3], 1):
                print(f"\n  {i}. {r['metadata']['title']}")
                print(f"     é‡æ’åºåˆ†æ•¸: {r.get('rerank_score', 0):.4f}")
                
        except Exception as e:
            print(f"âš ï¸  æŸ¥è©¢è™•ç†å‡ºéŒ¯: {e}")
            continue
    
    # 3. æ–¹æ³•å°æ¯”æ¸¬è©¦
    print("\n" + "-" * 60)
    print("3. æ–¹æ³•å°æ¯”æ¸¬è©¦")
    print("-" * 60)
    
    comparison_queries = [
        "transformer architecture",
        "How do neural networks learn?",
        "optimization methods for deep learning"
    ]
    
    compare_retrieval_methods(
        bm25_retriever,
        vector_retriever,
        hybrid_search,
        rag_pipeline,
        comparison_queries
    )
    
    # 4. æ€§èƒ½çµ±è¨ˆ
    print("\n" + "-" * 60)
    print("4. ç´¯ç©æ€§èƒ½çµ±è¨ˆ")
    print("-" * 60)
    
    stats = rag_pipeline.get_stats()
    print(f"ç¸½æŸ¥è©¢æ•¸: {stats['total_queries']}")
    print(f"å¹³å‡å¬å›æ™‚é–“: {stats['avg_recall_time']:.2f}s")
    print(f"å¹³å‡é‡æ’æ™‚é–“: {stats['avg_rerank_time']:.2f}s")
    print(f"å¹³å‡ç¸½æ™‚é–“: {stats['avg_total_time']:.2f}s")
    
    # 5. æœ€ä½³å¯¦è¸æŸ¥è©¢ç¤ºä¾‹
    print("\n" + "-" * 60)
    print("5. æœ€ä½³å¯¦è¸æŸ¥è©¢ç¤ºä¾‹")
    print("-" * 60)
    
    best_practice_queries = [
        "How do transformer models use self-attention?",
        "What are the advantages of BERT over traditional NLP models?",
        "recent techniques for fine-tuning large language models",
    ]
    
    for query in best_practice_queries:
        print(f"\næŸ¥è©¢: '{query}'")
        print("-" * 40)
        
        try:
            results, stats = rag_pipeline.query(
                query, 
                top_k=3, 
                return_stats=True
            )
            
            if results:
                print(f"æœ€ä½³çµæœ: {results[0]['metadata']['title']}")
                print(f"é‡æ’åºåˆ†æ•¸: {results[0].get('rerank_score', 0):.4f}")
                print(f"è€—æ™‚: {stats['total_time']:.2f}s")
                
        except Exception as e:
            print(f"âš ï¸  æŸ¥è©¢è™•ç†å‡ºéŒ¯: {e}")
            continue


def detect_document_type(file_path: Optional[str] = None, results: Optional[List[Dict]] = None) -> str:
    """
    è‡ªå‹•æª¢æ¸¬æ–‡æª”é¡å‹
    
    Args:
        file_path: æ–‡ä»¶è·¯å¾‘ï¼ˆå¯é¸ï¼‰
        results: æª¢ç´¢çµæœåˆ—è¡¨ï¼ˆå¯é¸ï¼Œç”¨æ–¼å¾ metadata æ¨æ–·ï¼‰
        
    Returns:
        æ–‡æª”é¡å‹ ("paper", "cv", "general")
    """
    # å„ªå…ˆå¾æ–‡ä»¶è·¯å¾‘åˆ¤æ–·
    if file_path:
        file_path_lower = file_path.lower()
        if any(keyword in file_path_lower for keyword in ["cv", "resume", "å±¥æ­·", "ç°¡æ­·"]):
            return "cv"
        elif any(keyword in file_path_lower for keyword in ["arxiv", "paper", "è«–æ–‡"]):
            return "paper"
    
    # å¾æª¢ç´¢çµæœçš„ metadata åˆ¤æ–·
    if results:
        for result in results:
            metadata = result.get("metadata", {})
            # å¦‚æœæœ‰ arxiv_idï¼Œå¾ˆå¯èƒ½æ˜¯è«–æ–‡
            if "arxiv_id" in metadata:
                return "paper"
            # å¦‚æœæœ‰ file_pathï¼Œæª¢æŸ¥æ–‡ä»¶å
            if "file_path" in metadata:
                file_path_lower = str(metadata.get("file_path", "")).lower()
                if any(keyword in file_path_lower for keyword in ["cv", "resume", "å±¥æ­·", "ç°¡æ­·"]):
                    return "cv"
            # æª¢æŸ¥æ¨™é¡Œ
            title = str(metadata.get("title", "")).lower()
            if any(keyword in title for keyword in ["cv", "resume", "curriculum vitae", "å±¥æ­·", "ç°¡æ­·"]):
                return "cv"
    
    # é è¨­ç‚ºé€šç”¨é¡å‹
    return "general"


def test_rag_vs_no_rag(
    llm: OllamaLLM,
    rag_pipeline: RAGPipeline,
    formatter: PromptFormatter,
    query: str,
    test_file_path: Optional[str] = None
):
    """
    å°æ¯”æ¸¬è©¦ï¼šæœ‰ RAG vs ç„¡ RAG
    
    é€™å€‹æ¸¬è©¦å¯ä»¥é©—è­‰ RAG ç³»çµ±çš„æ•ˆæœï¼š
    - ç„¡ RAGï¼šLLM åªèƒ½åŸºæ–¼è¨“ç·´æ•¸æ“šå›ç­”ï¼ˆå¯èƒ½ç„¡æ³•å›ç­”ç§æœ‰æ–‡æª”çš„å•é¡Œï¼‰
    - æœ‰ RAGï¼šLLM å¯ä»¥åŸºæ–¼æª¢ç´¢åˆ°çš„ç§æœ‰æ–‡æª”å›ç­”ï¼ˆæ‡‰è©²èƒ½æ­£ç¢ºå›ç­”ï¼‰
    
    Args:
        llm: LLM å¯¦ä¾‹
        rag_pipeline: RAG ç®¡ç·š
        formatter: Prompt æ ¼å¼åŒ–å™¨
        query: æ¸¬è©¦å•é¡Œï¼ˆæ‡‰è©²æ¶‰åŠç§æœ‰æ–‡æª”çš„å…§å®¹ï¼‰
        test_file_path: æ¸¬è©¦æ–‡ä»¶è·¯å¾‘ï¼ˆå¯é¸ï¼Œç”¨æ–¼é¡¯ç¤ºæ–‡ä»¶ä¿¡æ¯ï¼‰
    """
    print("\n" + "="*60)
    print("RAG æ•ˆæœå°æ¯”æ¸¬è©¦")
    print("="*60)
    print(f"\næ¸¬è©¦å•é¡Œ: '{query}'")
    if test_file_path:
        print(f"æ¸¬è©¦æ–‡ä»¶: {test_file_path}")
    print("-"*60)
    
    # æª¢æ¸¬æŸ¥è©¢èªè¨€
    detected_lang = PromptFormatter.detect_language(query)
    is_chinese = detected_lang == "zh"
    
    # 1. ç„¡ RAGï¼šç›´æ¥å• LLM
    print("\n[æ¸¬è©¦ 1] ç„¡ RAG - ç›´æ¥å• LLMï¼ˆä¸æä¾›ä»»ä½•ä¸Šä¸‹æ–‡ï¼‰")
    print("-"*60)
    try:
        if is_chinese:
            no_rag_prompt = f"""è«‹å›ç­”ä»¥ä¸‹å•é¡Œï¼š

{query}

è«‹æ³¨æ„ï¼šä½ åªèƒ½åŸºæ–¼ä½ çš„è¨“ç·´æ•¸æ“šä¾†å›ç­”ï¼Œç„¡æ³•è¨ªå•ä»»ä½•å¤–éƒ¨æ–‡æª”ã€‚
è«‹ä½¿ç”¨ä¸­æ–‡å›ç­”ã€‚"""
        else:
            no_rag_prompt = f"""Please answer the following question:

{query}

Note: You can only answer based on your training data and cannot access any external documents.
Please answer in English."""
        
        print("ç”Ÿæˆå›ç­”ä¸­...")
        no_rag_answer = llm.generate(
            prompt=no_rag_prompt,
            temperature=0.7,
            max_tokens=2048
        )
        print("\nLLM å›ç­”ï¼ˆç„¡ RAGï¼‰ï¼š")
        print("-" * 40)
        print(no_rag_answer)
        print("-" * 40)
        print("\nâœ… ç„¡ RAG å›ç­”å®Œæˆ")
    except Exception as e:
        print(f"âŒ ç„¡ RAG æ¸¬è©¦å¤±æ•—: {e}")
        no_rag_answer = None
    
    # 2. æœ‰ RAGï¼šä½¿ç”¨æª¢ç´¢å¢å¼·
    print("\n" + "-"*60)
    print("[æ¸¬è©¦ 2] æœ‰ RAG - ä½¿ç”¨æª¢ç´¢å¢å¼·ï¼ˆæä¾›ç›¸é—œæ–‡æª”ç‰‡æ®µï¼‰")
    print("-"*60)
    
    try:
        # æª¢ç´¢ç›¸é—œæ–‡æª”
        print("æª¢ç´¢ç›¸é—œæ–‡æª”ä¸­...")
        rag_results = rag_pipeline.query(
            text=query,
            top_k=3,
            enable_rerank=True
        )
        
        if not rag_results:
            print("âš ï¸  æœªæ‰¾åˆ°ç›¸é—œæ–‡æª”")
            print("é€™å¯èƒ½æ„å‘³è‘—ï¼š")
            print("  1. æŸ¥è©¢èˆ‡æ–‡æª”å…§å®¹ä¸åŒ¹é…")
            print("  2. æ–‡æª”å°šæœªè¼‰å…¥åˆ°æª¢ç´¢ç³»çµ±")
            return
        
        print(f"âœ… æ‰¾åˆ° {len(rag_results)} å€‹ç›¸é—œç‰‡æ®µ")
        
        # è‡ªå‹•æª¢æ¸¬æ–‡æª”é¡å‹
        document_type = detect_document_type(test_file_path, rag_results)
        print(f"  æª¢æ¸¬åˆ°çš„æ–‡æª”é¡å‹: {document_type}")
        
        # æ ¼å¼åŒ–ä¸¦ç”Ÿæˆå›ç­”ï¼ˆå‚³å…¥æ–‡æª”é¡å‹ï¼‰
        formatted_context = formatter.format_context(
            rag_results, 
            format_style="detailed",
            document_type=document_type
        )
        rag_prompt = formatter.create_prompt(
            query, 
            formatted_context,
            document_type=document_type
        )
        
        print("\nç”Ÿæˆå›ç­”ä¸­...")
        rag_answer = llm.generate(
            prompt=rag_prompt,
            temperature=0.7,
            max_tokens=2048
        )
        
        print("\nLLM å›ç­”ï¼ˆæœ‰ RAGï¼‰ï¼š")
        print("-" * 40)
        print(rag_answer)
        print("-" * 40)
        
        # é¡¯ç¤ºæª¢ç´¢åˆ°çš„æ–‡æª”ç‰‡æ®µ
        print("\n" + "-"*60)
        print("æª¢ç´¢åˆ°çš„æ–‡æª”ç‰‡æ®µï¼ˆLLM çš„åƒè€ƒä¾†æºï¼‰ï¼š")
        print("-"*60)
        for i, result in enumerate(rag_results, 1):
            print(f"\nç‰‡æ®µ {i}:")
            metadata = result['metadata']
            print(f"  ä¾†æº: {metadata.get('title', 'N/A')}")
            if 'file_path' in metadata:
                print(f"  æ–‡ä»¶: {metadata['file_path']}")
            elif 'arxiv_id' in metadata:
                print(f"  arXiv ID: {metadata['arxiv_id']}")
            print(f"  ç›¸é—œæ€§åˆ†æ•¸: {result.get('rerank_score', result.get('hybrid_score', 0)):.4f}")
            print(f"  å…§å®¹é è¦½: {result['content'][:200]}...")
        
        print("\nâœ… æœ‰ RAG å›ç­”å®Œæˆ")
        
    except Exception as e:
        print(f"âŒ æœ‰ RAG æ¸¬è©¦å¤±æ•—: {e}")
        import traceback
        traceback.print_exc()
        rag_answer = None
    
    # 3. å°æ¯”ç¸½çµ
    print("\n" + "="*60)
    print("å°æ¯”ç¸½çµ")
    print("="*60)
    
    if no_rag_answer and rag_answer:
        print("\nâœ… å…©å€‹æ¸¬è©¦éƒ½æˆåŠŸå®Œæˆ")
        print("\né—œéµå·®ç•°ï¼š")
        print("1. ç„¡ RAGï¼šLLM åªèƒ½åŸºæ–¼è¨“ç·´æ•¸æ“šå›ç­”")
        print("   - å¦‚æœå•é¡Œæ¶‰åŠç§æœ‰æ–‡æª”å…§å®¹ï¼ŒLLM å¯èƒ½ç„¡æ³•å›ç­”æˆ–å›ç­”éŒ¯èª¤")
        print("2. æœ‰ RAGï¼šLLM å¯ä»¥åŸºæ–¼æª¢ç´¢åˆ°çš„ç§æœ‰æ–‡æª”å›ç­”")
        print("   - å³ä½¿å•é¡Œæ¶‰åŠç§æœ‰æ–‡æª”å…§å®¹ï¼ŒLLM ä¹Ÿèƒ½åŸºæ–¼æª¢ç´¢çµæœæ­£ç¢ºå›ç­”")
        print("\nğŸ’¡ å¦‚æœå•é¡Œæ¶‰åŠç§æœ‰æ–‡æª”å…§å®¹ï¼Œæœ‰ RAG çš„å›ç­”æ‡‰è©²æ›´æº–ç¢ºã€æ›´å…·é«”ï¼")
    elif no_rag_answer:
        print("\nâš ï¸  ç„¡ RAG æ¸¬è©¦æˆåŠŸï¼Œä½†æœ‰ RAG æ¸¬è©¦å¤±æ•—")
        print("   è«‹æª¢æŸ¥æª¢ç´¢ç³»çµ±æ˜¯å¦æ­£å¸¸å·¥ä½œ")
    elif rag_answer:
        print("\nâš ï¸  æœ‰ RAG æ¸¬è©¦æˆåŠŸï¼Œä½†ç„¡ RAG æ¸¬è©¦å¤±æ•—")
        print("   è«‹æª¢æŸ¥ LLM é€£æ¥æ˜¯å¦æ­£å¸¸")
    else:
        print("\nâŒ å…©å€‹æ¸¬è©¦éƒ½å¤±æ•—")
        print("   è«‹æª¢æŸ¥ç³»çµ±é…ç½®")


def main():
    """
    ä¸»ç¨‹å¼ï¼šç¤ºç¯„ hybrid search çš„ä½¿ç”¨
    
    æ”¯æ´å…©ç¨®åˆ†å¡Šæ¨¡å¼ï¼š
    - å­—ç¬¦åˆ†å¡Šï¼ˆé è¨­ï¼‰ï¼šå¿«é€Ÿã€ç©©å®š
    - èªç¾©åˆ†å¡Šï¼ˆå¯é¸ï¼‰ï¼šæ›´æ™ºèƒ½ï¼Œèƒ½ä¿æŒèªç¾©å®Œæ•´æ€§
    
    å¯ä»¥é€šéç’°å¢ƒè®Šæ•¸ USE_SEMANTIC_CHUNKING=true å•Ÿç”¨èªç¾©åˆ†å¡Š
    """
    
    print("=" * 60)
    print("Hybrid Search ç³»çµ±åˆå§‹åŒ–ä¸­...")
    print("ä½¿ç”¨ Hugging Face embedding æ¨¡å‹ï¼ˆå®Œå…¨å…è²»ï¼Œæœ¬åœ°é‹è¡Œï¼‰")
    print("=" * 60)
    
    # [æ­¥é©Ÿ 0] å¯é¸ï¼šåˆå§‹åŒ–å…±ç”¨çš„ Embedding æ¨¡å‹ï¼ˆç”¨æ–¼èªç¾©åˆ†å¡Šï¼‰
    # æª¢æŸ¥æ˜¯å¦å•Ÿç”¨èªç¾©åˆ†å¡Š
    use_semantic_chunking = os.getenv("USE_SEMANTIC_CHUNKING", "false").lower() == "true"
    shared_embeddings = None
    
    if use_semantic_chunking:
        print("\n[0/6] åˆå§‹åŒ–å…±ç”¨çš„ Embedding æ¨¡å‹ï¼ˆç”¨æ–¼èªç¾©åˆ†å¡Šï¼‰...")
        try:
            from langchain_community.embeddings import HuggingFaceEmbeddings
            from src.retrievers.vector_retriever import get_device
            
            # è¨­ç½® Hugging Face æ¨¡å‹ç·©å­˜ç›®éŒ„ï¼ˆå¯é¸ï¼šå¤–æ¥ç¡¬ç¢Ÿè·¯å¾‘ï¼‰
            hf_cache_dir = os.getenv("HF_CACHE_DIR", None)
            
            # è‡ªå‹•æª¢æ¸¬è¨­å‚™ï¼ˆä½¿ç”¨èˆ‡ VectorRetriever ç›¸åŒçš„é‚è¼¯ï¼‰
            device = get_device()
            
            device_name_map = {
                'mps': 'MPS (macOS GPU)',
                'cuda': 'CUDA (NVIDIA GPU)',
                'cpu': 'CPU'
            }
            print(f"  ä½¿ç”¨è¨­å‚™: {device_name_map.get(device, device)}")
            
            # æ§‹å»º model_kwargs
            model_kwargs = {'device': device}
            if hf_cache_dir:
                model_kwargs['cache_dir'] = hf_cache_dir
                print(f"  æ¨¡å‹ç·©å­˜ç›®éŒ„: {hf_cache_dir}")
            
            # åˆå§‹åŒ–å…±ç”¨çš„ embedding æ¨¡å‹
            shared_embeddings = HuggingFaceEmbeddings(
                model_name="sentence-transformers/all-MiniLM-L6-v2",
                model_kwargs=model_kwargs,
                encode_kwargs={'normalize_embeddings': True}
            )
            print("  âœ“ å…±ç”¨ Embedding æ¨¡å‹åˆå§‹åŒ–å®Œæˆ")
            print("  ğŸ’¡ æ­¤æ¨¡å‹å°‡åŒæ™‚ç”¨æ–¼èªç¾©åˆ†å¡Šå’Œå‘é‡æª¢ç´¢ï¼Œç¯€çœå…§å­˜å’Œæ™‚é–“")
        except ImportError as e:
            print(f"  âš ï¸  ç„¡æ³•åˆå§‹åŒ–èªç¾©åˆ†å¡Šæ‰€éœ€çš„ä¾è³´: {e}")
            print("  å°‡å›é€€åˆ°å­—ç¬¦åˆ†å¡Šæ¨¡å¼")
            use_semantic_chunking = False
        except Exception as e:
            print(f"  âš ï¸  åˆå§‹åŒ– Embedding æ¨¡å‹å¤±æ•—: {e}")
            print("  å°‡å›é€€åˆ°å­—ç¬¦åˆ†å¡Šæ¨¡å¼")
            use_semantic_chunking = False
    
    # 1. åˆå§‹åŒ–æ–‡æª”è™•ç†å™¨
    print("\n[1/6] åˆå§‹åŒ–æ–‡æª”è™•ç†å™¨...")
    if use_semantic_chunking and shared_embeddings:
        # ä½¿ç”¨èªç¾©åˆ†å¡Šæ¨¡å¼
        processor = DocumentProcessor(
            embeddings=shared_embeddings,
            use_semantic_chunking=True,
            breakpoint_threshold_amount=1.5,  # èªç¾©åˆ†å¡Šæ•æ„Ÿåº¦
            min_chunk_size=100  # æœ€å° chunk å¤§å°ï¼ˆå­—ç¬¦æ•¸ï¼‰
        )
    else:
        # ä½¿ç”¨å­—ç¬¦åˆ†å¡Šæ¨¡å¼ï¼ˆé è¨­ï¼‰
        processor = DocumentProcessor(chunk_size=1000, chunk_overlap=200)
    
    # 2. ç²å– arXiv è«–æ–‡
    print("\n[2/6] å¾ arXiv ç²å–è«–æ–‡...")
    print("  æœå°‹ AIã€æ©Ÿå™¨å­¸ç¿’å’Œè‡ªç„¶èªè¨€è™•ç†ç›¸é—œè«–æ–‡...")
    papers = processor.fetch_papers(
        query="cat:cs.AI OR cat:cs.LG OR cat:cs.CL",  # AI + æ©Ÿå™¨å­¸ç¿’ + è‡ªç„¶èªè¨€è™•ç†
        max_results=40  # å¢åŠ åˆ° 40 ç¯‡ä»¥ç²å¾—æ›´å¤šæ¨£åŒ–çš„çµæœ
    )
    print(f"ç²å–äº† {len(papers)} ç¯‡è«–æ–‡")
    
    # 3. è™•ç†æ–‡æª”ï¼ˆåˆ†å‰²æˆ chunksï¼‰
    print("\n[3/6] è™•ç†æ–‡æª”ä¸¦åˆ†å‰²æˆ chunks...")
    if use_semantic_chunking:
        print("  âš ï¸  èªç¾©åˆ†å¡Šéœ€è¦è¨ˆç®— embeddingï¼Œå¯èƒ½éœ€è¦è¼ƒé•·æ™‚é–“ï¼Œè«‹ç¨å€™...")
    documents = processor.process_documents(papers)
    print(f"ç¸½å…±å‰µå»ºäº† {len(documents)} å€‹æ–‡æª” chunks")
    
    # é¡¯ç¤ºä¸€äº›ç¯„ä¾‹æ–‡æª”
    if documents:
        print("\nç¯„ä¾‹æ–‡æª”ï¼ˆç¬¬ä¸€å€‹ chunkï¼‰ï¼š")
        print(f"æ¨™é¡Œ: {documents[0]['metadata']['title']}")
        chunking_method = documents[0]['metadata'].get('chunking_method', 'character')
        print(f"åˆ†å¡Šæ–¹æ³•: {chunking_method}")
        print(f"å…§å®¹é è¦½: {documents[0]['content'][:200]}...")
    
    # 4. åˆå§‹åŒ–æª¢ç´¢å™¨
    print("\n[4/6] åˆå§‹åŒ–æª¢ç´¢å™¨...")
    
    # ç¨€ç–æª¢ç´¢å™¨ (BM25)
    print("  - åˆå§‹åŒ–ç¨€ç–æª¢ç´¢å™¨ (BM25)...")
    bm25_retriever = BM25Retriever(documents)
    
    # å¯†é›†æª¢ç´¢å™¨ (å‘é‡)
    print("  - åˆå§‹åŒ–å¯†é›†æª¢ç´¢å™¨ï¼ˆä½¿ç”¨å…è²»çš„ Hugging Face æ¨¡å‹ï¼‰...")
    
    # è¨­ç½® Hugging Face æ¨¡å‹ç·©å­˜ç›®éŒ„ï¼ˆå¯é¸ï¼šå¤–æ¥ç¡¬ç¢Ÿè·¯å¾‘ï¼‰
    hf_cache_dir = os.getenv("HF_CACHE_DIR", None)
    
    # å¦‚æœä½¿ç”¨èªç¾©åˆ†å¡Šï¼Œå‚³å…¥å…±ç”¨çš„ embeddings
    vector_retriever = VectorRetriever(
        documents,
        embedding_model="sentence-transformers/all-MiniLM-L6-v2",
        persist_directory="./chroma_db",
        hf_cache_dir=hf_cache_dir,
        embeddings=shared_embeddings  # å‚³å…¥å…±ç”¨çš„ embeddingsï¼ˆå¦‚æœæœ‰çš„è©±ï¼‰
    )
    
    # 5. åˆå§‹åŒ– Hybrid Search
    print("\n[5/6] åˆå§‹åŒ– Hybrid Search...")
    print("  ä½¿ç”¨ RRF (Reciprocal Rank Fusion) æ–¹æ³•ï¼ˆé è¨­ï¼‰")
    print("  RRF ä¸éœ€è¦åˆ†æ•¸æ­£è¦åŒ–ï¼Œå°ä¸åŒåˆ†æ•¸åˆ†ä½ˆæ›´é­¯æ£’")
    hybrid_search = HybridSearch(
        sparse_retriever=bm25_retriever,
        dense_retriever=vector_retriever,
        fusion_method="rrf",  # ä½¿ç”¨ RRF æ–¹æ³•ï¼ˆé è¨­ï¼‰
        rrf_k=60,  # RRF å¸¸æ•¸ kï¼Œé€šå¸¸è¨­ç‚º 60
        # å¦‚æœä½¿ç”¨ weighted_sum æ–¹æ³•ï¼Œå¯ä»¥è¨­ç½®æ¬Šé‡ï¼š
        # sparse_weight=0.4,
        # dense_weight=0.6,
    )
    
    print("\n" + "=" * 60)
    print("ç³»çµ±åˆå§‹åŒ–å®Œæˆï¼")
    print("=" * 60)
    
    # 6. åŸ·è¡Œæœå°‹æ¸¬è©¦
    print("\né–‹å§‹æœå°‹æ¸¬è©¦...")
    print("-" * 60)
    
    test_queries = [
        "machine learning",
        "neural networks",
        "natural language processing",
    ]
    
    for query in test_queries:
        print(f"\næŸ¥è©¢: '{query}'")
        print("-" * 60)
        
        # ä½¿ç”¨ Hybrid Search
        results = hybrid_search.retrieve(query, top_k=3)
        
        print(f"\næ‰¾åˆ° {len(results)} å€‹ç›¸é—œçµæœï¼š\n")
        
        for i, result in enumerate(results, 1):
            print(f"çµæœ {i}:")
            print(f"  æ¨™é¡Œ: {result['metadata']['title']}")
            print(f"  arXiv ID: {result['metadata']['arxiv_id']}")
            print(f"  æ··åˆåˆ†æ•¸: {result['hybrid_score']:.4f}")
            
            # æ ¹æ“šèåˆæ–¹æ³•é¡¯ç¤ºä¸åŒçš„è³‡è¨Š
            if 'rrf_score' in result:
                # RRF æ–¹æ³•
                print(f"    - RRF åˆ†æ•¸: {result['rrf_score']:.4f}")
                if result.get('sparse_rank') is not None:
                    print(f"    - BM25 æ’å: {result['sparse_rank']} (åˆ†æ•¸: {result.get('sparse_score', 0.0):.4f})")
                if result.get('dense_rank') is not None:
                    print(f"    - å‘é‡æ’å: {result['dense_rank']} (åˆ†æ•¸: {result.get('dense_score', 0.0):.4f})")
            else:
                # Weighted Sum æ–¹æ³•
                print(f"    - ç¨€ç–(BM25)åˆ†æ•¸: {result.get('sparse_score', 0.0):.4f}")
                print(f"    - å¯†é›†(å‘é‡)åˆ†æ•¸: {result.get('dense_score', 0.0):.4f}")
            
            print(f"  å…§å®¹é è¦½: {result['content'][:150]}...")
            print()
    
    # 7. æ¯”è¼ƒä¸åŒæª¢ç´¢æ–¹æ³•
    print("\n" + "=" * 60)
    print("æ¯”è¼ƒä¸åŒæª¢ç´¢æ–¹æ³•")
    print("=" * 60)
    
    comparison_query = "deep learning"
    print(f"\næŸ¥è©¢: '{comparison_query}'")
    print("-" * 60)
    
    # BM25 çµæœ
    print("\n[BM25 æª¢ç´¢çµæœ] (åˆ†æ•¸è¶Šé«˜è¶Šå¥½)")
    bm25_results = bm25_retriever.retrieve(comparison_query, top_k=3)
    for i, result in enumerate(bm25_results, 1):
        print(f"{i}. {result['metadata']['title']} (åˆ†æ•¸: {result['score']:.4f})")
    
    # å‘é‡æª¢ç´¢çµæœ
    print("\n[å‘é‡æª¢ç´¢çµæœ] (åˆ†æ•¸è¶Šé«˜è¶Šå¥½)")
    vector_results = vector_retriever.retrieve(comparison_query, top_k=3)
    for i, result in enumerate(vector_results, 1):
        score = result.get('score')
        if score is not None:
            print(f"{i}. {result['metadata']['title']} (ç›¸ä¼¼åº¦åˆ†æ•¸: {score:.4f})")
        else:
            print(f"{i}. {result['metadata']['title']} (åˆ†æ•¸: N/A)")
    
    # Hybrid Search çµæœï¼ˆä½¿ç”¨ RRFï¼‰
    print("\n[Hybrid Search çµæœ (RRF)] (åˆ†æ•¸è¶Šé«˜è¶Šå¥½)")
    hybrid_results = hybrid_search.retrieve(comparison_query, top_k=3)
    for i, result in enumerate(hybrid_results, 1):
        if 'rrf_score' in result:
            print(f"{i}. {result['metadata']['title']} (RRF åˆ†æ•¸: {result['rrf_score']:.4f})")
        else:
            print(f"{i}. {result['metadata']['title']} (æ··åˆåˆ†æ•¸: {result['hybrid_score']:.4f})")
    
    # æ¯”è¼ƒ RRF å’Œ Weighted Sum æ–¹æ³•
    print("\n" + "=" * 60)
    print("æ¯”è¼ƒ RRF å’Œ Weighted Sum èåˆæ–¹æ³•")
    print("=" * 60)
    
    comparison_query2 = "transformer architecture"
    print(f"\næŸ¥è©¢: '{comparison_query2}'")
    print("-" * 60)
    
    # RRF æ–¹æ³•
    print("\n[RRF æ–¹æ³•çµæœ]")
    hybrid_search_rrf = HybridSearch(
        sparse_retriever=bm25_retriever,
        dense_retriever=vector_retriever,
        fusion_method="rrf",
        rrf_k=60
    )
    rrf_results = hybrid_search_rrf.retrieve(comparison_query2, top_k=3)
    for i, result in enumerate(rrf_results, 1):
        print(f"{i}. {result['metadata']['title']}")
        print(f"   RRF åˆ†æ•¸: {result['rrf_score']:.4f}")
        if result.get('sparse_rank'):
            print(f"   BM25 æ’å: {result['sparse_rank']}, åˆ†æ•¸: {result.get('sparse_score', 0.0):.4f}")
        if result.get('dense_rank'):
            print(f"   å‘é‡æ’å: {result['dense_rank']}, åˆ†æ•¸: {result.get('dense_score', 0.0):.4f}")
    
    # Weighted Sum æ–¹æ³•
    print("\n[Weighted Sum æ–¹æ³•çµæœ]")
    hybrid_search_weighted = HybridSearch(
        sparse_retriever=bm25_retriever,
        dense_retriever=vector_retriever,
        fusion_method="weighted_sum",
        sparse_weight=0.4,
        dense_weight=0.6
    )
    weighted_results = hybrid_search_weighted.retrieve(comparison_query2, top_k=3)
    for i, result in enumerate(weighted_results, 1):
        print(f"{i}. {result['metadata']['title']}")
        print(f"   æ··åˆåˆ†æ•¸: {result['hybrid_score']:.4f}")
        print(f"   BM25 åˆ†æ•¸: {result.get('sparse_score', 0.0):.4f}")
        print(f"   å‘é‡åˆ†æ•¸: {result.get('dense_score', 0.0):.4f}")
    
    # 8. ç¤ºç¯„ Metadata Filtering åŠŸèƒ½
    print("\n" + "=" * 60)
    print("Metadata Filtering åŠŸèƒ½ç¤ºç¯„")
    print("=" * 60)
    
    if documents:
        # ç²å–ç¬¬ä¸€å€‹è«–æ–‡çš„ arxiv_id ä½œç‚ºç¯„ä¾‹
        first_paper_id = documents[0]['metadata'].get('arxiv_id', None)
        first_paper_title = documents[0]['metadata'].get('title', '')
        
        if first_paper_id:
            print(f"\nç¤ºç¯„ï¼šåªæª¢ç´¢ç‰¹å®šè«–æ–‡ (arXiv ID: {first_paper_id})")
            print("-" * 60)
            
            # ä½¿ç”¨ metadata_filter åªæª¢ç´¢ç‰¹å®šè«–æ–‡çš„ chunks
            filtered_results = hybrid_search.retrieve(
                query="machine learning",
                top_k=5,
                metadata_filter={"arxiv_id": first_paper_id}
            )
            
            print(f"\næ‰¾åˆ° {len(filtered_results)} å€‹ä¾†è‡ªè©²è«–æ–‡çš„çµæœï¼š\n")
            for i, result in enumerate(filtered_results, 1):
                print(f"çµæœ {i}:")
                print(f"  æ¨™é¡Œ: {result['metadata']['title']}")
                print(f"  arXiv ID: {result['metadata']['arxiv_id']}")
                print(f"  Chunk ç´¢å¼•: {result['metadata'].get('chunk_index', 'N/A')}")
                print(f"  æ··åˆåˆ†æ•¸: {result['hybrid_score']:.4f}")
                print(f"  å…§å®¹é è¦½: {result['content'][:150]}...")
                print()
        
        # ç¤ºç¯„æŒ‰æ¨™é¡Œéæ¿¾ï¼ˆéƒ¨åˆ†åŒ¹é…ï¼‰
        if first_paper_title:
            print(f"\nç¤ºç¯„ï¼šæŒ‰æ¨™é¡Œé—œéµå­—éæ¿¾ï¼ˆåŒ…å« '{first_paper_title[:30]}...'ï¼‰")
            print("-" * 60)
            
            # æå–æ¨™é¡Œçš„å‰å¹¾å€‹å­—ä½œç‚ºéæ¿¾æ¢ä»¶
            title_keyword = first_paper_title.split()[0] if first_paper_title else ""
            if title_keyword:
                filtered_by_title = hybrid_search.retrieve(
                    query="machine learning",
                    top_k=3,
                    metadata_filter={"title": title_keyword}
                )
                
                print(f"\næ‰¾åˆ° {len(filtered_by_title)} å€‹åŒ¹é…æ¨™é¡Œçš„çµæœï¼š\n")
                for i, result in enumerate(filtered_by_title, 1):
                    print(f"{i}. {result['metadata']['title']} (æ··åˆåˆ†æ•¸: {result['hybrid_score']:.4f})")
    
    # 9. ç¤ºç¯„é‡æ’åºåŠŸèƒ½ï¼ˆReranker + RAGPipelineï¼‰
    print("\n" + "=" * 60)
    print("é‡æ’åºåŠŸèƒ½ç¤ºç¯„ (Reranker + RAGPipeline)")
    print("=" * 60)
    
    try:
        # åˆå§‹åŒ–é‡æ’åºå™¨
        print("\nåˆå§‹åŒ–é‡æ’åºå™¨...")
        reranker = Reranker(
            model_name="BAAI/bge-reranker-base",
            device=None,  # è‡ªå‹•æª¢æ¸¬è¨­å‚™
            batch_size=16  # æ‰¹è™•ç†å¤§å°
        )
        
        # åˆå§‹åŒ– RAG ç®¡ç·š
        print("åˆå§‹åŒ– RAG ç®¡ç·š...")
        rag_pipeline = RAGPipeline(
            hybrid_search=hybrid_search,
            reranker=reranker,
            recall_k=25,  # ç¬¬ä¸€éšæ®µå¬å› 25 ç­†
            adaptive_recall=True,  # å•Ÿç”¨å‹•æ…‹èª¿æ•´
            min_recall_k=10,
            max_recall_k=50
        )
        
        # æ¸¬è©¦æŸ¥è©¢
        test_query = "deep learning neural networks"
        print(f"\næ¸¬è©¦æŸ¥è©¢: '{test_query}'")
        print("-" * 60)
        
        # ä½¿ç”¨ RAG ç®¡ç·šé€²è¡Œæœå°‹ï¼ˆåŒ…å«é‡æ’åºï¼‰
        results, stats = rag_pipeline.query(
            text=test_query,
            top_k=5,
            enable_rerank=True,
            return_stats=True
        )
        
        print(f"\næ‰¾åˆ° {len(results)} å€‹çµæœï¼ˆç¶“éé‡æ’åºï¼‰ï¼š\n")
        for i, result in enumerate(results, 1):
            print(f"çµæœ {i}:")
            print(f"  æ¨™é¡Œ: {result['metadata']['title']}")
            print(f"  arXiv ID: {result['metadata']['arxiv_id']}")
            print(f"  é‡æ’åºåˆ†æ•¸: {result.get('rerank_score', 'N/A'):.4f}")
            if 'hybrid_score' in result:
                print(f"  åŸå§‹æ··åˆåˆ†æ•¸: {result['hybrid_score']:.4f}")
            print(f"  å…§å®¹é è¦½: {result['content'][:150]}...")
            print()
        
        # é¡¯ç¤ºæ€§èƒ½çµ±è¨ˆ
        print("\næ€§èƒ½çµ±è¨ˆ:")
        print(f"  å¬å›éšæ®µè€—æ™‚: {stats['recall_time']:.2f}s")
        print(f"  é‡æ’éšæ®µè€—æ™‚: {stats['rerank_time']:.2f}s")
        print(f"  ç¸½è€—æ™‚: {stats['total_time']:.2f}s")
        print(f"  å¯¦éš›å¬å›æ•¸é‡: {stats['recall_k']} ç­†")
        print(f"  å€™é¸çµæœæ•¸: {stats['candidates_found']} ç­†")
        print(f"  æœ€çµ‚çµæœæ•¸: {stats['final_results']} ç­†")
        
        # æ¯”è¼ƒï¼šæœ‰é‡æ’åº vs ç„¡é‡æ’åº
        print("\n" + "-" * 60)
        print("æ¯”è¼ƒï¼šæœ‰é‡æ’åº vs ç„¡é‡æ’åº")
        print("-" * 60)
        
        # ç„¡é‡æ’åº
        print("\n[ç„¡é‡æ’åº] åƒ…ä½¿ç”¨ Hybrid Search:")
        no_rerank_results = hybrid_search.retrieve(test_query, top_k=5)
        for i, result in enumerate(no_rerank_results, 1):
            print(f"{i}. {result['metadata']['title']} (æ··åˆåˆ†æ•¸: {result.get('hybrid_score', 0.0):.4f})")
        
        # æœ‰é‡æ’åº
        print("\n[æœ‰é‡æ’åº] ä½¿ç”¨ RAG Pipeline:")
        rerank_results = rag_pipeline.query(test_query, top_k=5, enable_rerank=True)
        for i, result in enumerate(rerank_results, 1):
            rerank_score = result.get('rerank_score', 0.0)
            hybrid_score = result.get('hybrid_score', 0.0)
            print(f"{i}. {result['metadata']['title']}")
            print(f"   é‡æ’åºåˆ†æ•¸: {rerank_score:.4f}, åŸå§‹æ··åˆåˆ†æ•¸: {hybrid_score:.4f}")
        
        # é¡¯ç¤ºç´¯ç©çµ±è¨ˆ
        print("\n" + "-" * 60)
        print("ç´¯ç©æ€§èƒ½çµ±è¨ˆ:")
        print("-" * 60)
        cumulative_stats = rag_pipeline.get_stats()
        print(f"  ç¸½æŸ¥è©¢æ•¸: {cumulative_stats['total_queries']}")
        print(f"  å¹³å‡å¬å›æ™‚é–“: {cumulative_stats['avg_recall_time']:.2f}s")
        print(f"  å¹³å‡é‡æ’æ™‚é–“: {cumulative_stats['avg_rerank_time']:.2f}s")
        print(f"  å¹³å‡ç¸½æ™‚é–“: {cumulative_stats['avg_total_time']:.2f}s")
        
        # 10. åŸ·è¡Œå…¨é¢çš„ RAG åŠŸæ•ˆæ¸¬è©¦
        print("\n" + "=" * 60)
        print("åŸ·è¡Œå…¨é¢çš„ RAG åŠŸæ•ˆæ¸¬è©¦")
        print("=" * 60)
        
        comprehensive_rag_test(
            bm25_retriever=bm25_retriever,
            vector_retriever=vector_retriever,
            hybrid_search=hybrid_search,
            rag_pipeline=rag_pipeline
        )
        
        # 11. ç¤ºç¯„ Prompt æ ¼å¼åŒ–å’Œ LLM é›†æˆ
        print("\n" + "=" * 60)
        print("Prompt æ ¼å¼åŒ–å’Œ LLM é›†æˆç¤ºç¯„")
        print("=" * 60)
        
        try:
            # åˆå§‹åŒ– Prompt æ ¼å¼åŒ–å™¨
            print("\nåˆå§‹åŒ– Prompt æ ¼å¼åŒ–å™¨...")
            formatter = PromptFormatter(
                include_metadata=True,
                format_style="detailed"
            )
            
            # æ¸¬è©¦æŸ¥è©¢
            test_query_llm = "How do transformer models work?"
            print(f"\næ¸¬è©¦æŸ¥è©¢: '{test_query_llm}'")
            print("-" * 60)
            
            # æª¢ç´¢ç›¸é—œæ–‡æª”
            llm_results = rag_pipeline.query(
                text=test_query_llm,
                top_k=3,
                enable_rerank=True
            )
            
            if llm_results:
                # æ ¼å¼åŒ–æª¢ç´¢çµæœï¼ˆarXiv è«–æ–‡ä½¿ç”¨ "paper" é¡å‹ï¼‰
                print("\næ ¼å¼åŒ–æª¢ç´¢çµæœ...")
                formatted_context = formatter.format_context(llm_results, document_type="paper")
                print("\næ ¼å¼åŒ–å¾Œçš„ä¸Šä¸‹æ–‡ï¼ˆå‰ 500 å­—ç¬¦ï¼‰ï¼š")
                print(formatted_context[:500] + "...")
                
                # å‰µå»ºå®Œæ•´çš„ promptï¼ˆarXiv è«–æ–‡ä½¿ç”¨ "paper" é¡å‹ï¼‰
                full_prompt = formatter.create_prompt(test_query_llm, formatted_context, document_type="paper")
                print("\nå®Œæ•´ Prompt é•·åº¦:", len(full_prompt), "å­—ç¬¦")
                
                # å˜—è©¦ä½¿ç”¨ Ollama LLM
                print("\n" + "-" * 60)
                print("å˜—è©¦é€£æ¥ Ollama LLM...")
                print("-" * 60)
                
                try:
                    # é¡¯ç¤ºæ¨è–¦çš„æ¨¡å‹
                    OllamaLLM.print_recommended_models()
                    
                    # åˆå§‹åŒ– LLMï¼ˆä½¿ç”¨æ¨è–¦çš„å°æ¨¡å‹ï¼‰
                    llm = OllamaLLM(
                        model_name="llama3.2:3b",  # é©åˆ 16GB å…§å­˜
                        timeout=180
                    )
                    
                    print(f"\nä½¿ç”¨æ¨¡å‹: {llm.model_name}")
                    print("ç”Ÿæˆå›ç­”ä¸­ï¼ˆé€™å¯èƒ½éœ€è¦ä¸€äº›æ™‚é–“ï¼‰...\n")
                    print("-" * 60)
                    
                    # ç”Ÿæˆå›ç­”
                    answer = llm.generate(
                        prompt=full_prompt,
                        temperature=0.7,
                        max_tokens=2048,
                        stream=False
                    )
                    
                    print("\n" + "-" * 60)
                    print("LLM ç”Ÿæˆçš„å›ç­”ï¼š")
                    print("-" * 60)
                    print(answer)
                    
                except ConnectionError as e:
                    print(f"\nâš ï¸  Ollama é€£æ¥éŒ¯èª¤: {e}")
                    print("\nè«‹æŒ‰ç…§ä»¥ä¸‹æ­¥é©Ÿè¨­ç½® Ollamaï¼š")
                    print("  1. å®‰è£ Ollama: https://ollama.ai/download")
                    print("  2. å•Ÿå‹• Ollama æœå‹™ï¼ˆé€šå¸¸æœƒè‡ªå‹•å•Ÿå‹•ï¼‰")
                    print("  3. ä¸‹è¼‰æ¨¡å‹: ollama pull llama3.2:3b")
                    print("  4. é‡æ–°é‹è¡Œæ­¤ç¨‹åº")
                except Exception as e:
                    print(f"\nâš ï¸  LLM ç”Ÿæˆå‡ºéŒ¯: {e}")
                    print("æ‚¨å¯ä»¥ç¹¼çºŒä½¿ç”¨æ ¼å¼åŒ–åŠŸèƒ½ï¼Œåªæ˜¯ä¸ç”Ÿæˆ LLM å›ç­”ã€‚")
                
                # é¡¯ç¤ºæ ¼å¼åŒ–å¾Œçš„ promptï¼ˆå³ä½¿ LLM å¤±æ•—ï¼‰
                print("\n" + "-" * 60)
                print("æ ¼å¼åŒ–å¾Œçš„ Promptï¼ˆå¯ç”¨æ–¼æ‰‹å‹•æ¸¬è©¦ï¼‰ï¼š")
                print("-" * 60)
                print(full_prompt[:1000] + f"...\nï¼ˆå·²æˆªæ–·ï¼Œå®Œæ•´é•·åº¦: {len(full_prompt)} å­—ç¬¦ï¼‰")
            else:
                print("âš ï¸  æœªæ‰¾åˆ°ç›¸é—œæ–‡æª”ï¼Œç„¡æ³•é€²è¡Œæ ¼å¼åŒ–")
                
        except Exception as e:
            print(f"\nâš ï¸  Prompt æ ¼å¼åŒ–å‡ºéŒ¯: {e}")
            import traceback
            traceback.print_exc()
        
    except ImportError as e:
        print(f"\nâš ï¸  é‡æ’åºåŠŸèƒ½éœ€è¦é¡å¤–ä¾è³´: {e}")
        print("è«‹ç¢ºä¿å·²å®‰è£ sentence-transformers: pip install sentence-transformers")
    except Exception as e:
        print(f"\nâš ï¸  é‡æ’åºåŠŸèƒ½å‡ºéŒ¯: {e}")
        print("é€™å¯èƒ½æ˜¯å› ç‚ºæ¨¡å‹ä¸‹è¼‰å¤±æ•—æˆ–è¨­å‚™ä¸æ”¯æŒã€‚")
        print("æ‚¨å¯ä»¥ç¹¼çºŒä½¿ç”¨ Hybrid Search åŠŸèƒ½ã€‚")
    
    print("\n" + "=" * 60)
    print("å®Œæˆï¼")
    print("=" * 60)
    
    # æç¤ºï¼šå¦‚ä½•å•Ÿç”¨èªç¾©åˆ†å¡Š
    if not use_semantic_chunking:
        print("\nğŸ’¡ æç¤ºï¼šè¦å•Ÿç”¨èªç¾©åˆ†å¡Šæ¨¡å¼ï¼Œè«‹è¨­ç½®ç’°å¢ƒè®Šæ•¸ï¼š")
        print("  export USE_SEMANTIC_CHUNKING=true")
        print("  æˆ–")
        print("  USE_SEMANTIC_CHUNKING=true python main.py")
        print("\nèªç¾©åˆ†å¡Šçš„å„ªé»ï¼š")
        print("  - èƒ½ä¿æŒèªç¾©å®Œæ•´æ€§ï¼Œä¸æœƒåœ¨å¥å­ä¸­é–“åˆ‡åˆ†")
        print("  - æ ¹æ“šèªç¾©ç›¸ä¼¼åº¦è‡ªå‹•æ±ºå®šåˆ‡åˆ†é»")
        print("  - å¯èƒ½æå‡æª¢ç´¢æ•ˆæœ")
        print("\næ³¨æ„ï¼šèªç¾©åˆ†å¡Šéœ€è¦å®‰è£ langchain-experimentalï¼š")
        print("  pip install langchain-experimental")


if __name__ == "__main__":
    main()

