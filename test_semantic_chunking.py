"""
æ¸¬è©¦èªç¾©åˆ†å¡ŠåŠŸèƒ½çš„è…³æœ¬

é€™å€‹è…³æœ¬æ¼”ç¤ºå¦‚ä½•ï¼š
1. ä½¿ç”¨èªç¾©åˆ†å¡Šè™•ç†ç§æœ‰æª”æ¡ˆï¼ˆPDF, DOCX, TXTï¼‰
2. å°æ¯”èªç¾©åˆ†å¡Š vs å­—ç¬¦åˆ†å¡Šçš„æ•ˆæœ
3. å»ºç«‹ RAG ç³»çµ±ä¸¦æ¸¬è©¦æª¢ç´¢æ•ˆæœ
4. å°æ¯”æ¸¬è©¦ï¼šæœ‰ RAG vs ç„¡ RAG çš„æ•ˆæœ
"""
import os
import sys
from pathlib import Path
from src import (
    DocumentProcessor,
    BM25Retriever,
    VectorRetriever,
    HybridSearch,
    Reranker,
    RAGPipeline,
    PromptFormatter,
    OllamaLLM
)
from main import test_rag_vs_no_rag


def compare_chunking_methods(file_path: str):
    """
    å°æ¯”èªç¾©åˆ†å¡Šå’Œå­—ç¬¦åˆ†å¡Šçš„æ•ˆæœ
    
    Args:
        file_path: è¦æ¸¬è©¦çš„æª”æ¡ˆè·¯å¾‘
    """
    print("\n" + "=" * 60)
    print("å°æ¯”ï¼šèªç¾©åˆ†å¡Š vs å­—ç¬¦åˆ†å¡Š")
    print("=" * 60)
    
    # åˆå§‹åŒ–å…±ç”¨çš„ Embedding æ¨¡å‹
    print("\n[åˆå§‹åŒ–] è¼‰å…¥ Embedding æ¨¡å‹...")
    try:
        from langchain_community.embeddings import HuggingFaceEmbeddings
        from src.retrievers.vector_retriever import get_device
        
        hf_cache_dir = os.getenv("HF_CACHE_DIR", None)
        device = get_device()
        
        device_name_map = {
            'mps': 'MPS (macOS GPU)',
            'cuda': 'CUDA (NVIDIA GPU)',
            'cpu': 'CPU'
        }
        print(f"  ä½¿ç”¨è¨­å‚™: {device_name_map.get(device, device)}")
        
        model_kwargs = {'device': device}
        if hf_cache_dir:
            model_kwargs['cache_dir'] = hf_cache_dir
        
        shared_embeddings = HuggingFaceEmbeddings(
            model_name="sentence-transformers/all-MiniLM-L6-v2",
            model_kwargs=model_kwargs,
            encode_kwargs={'normalize_embeddings': True}
        )
        print("  âœ“ Embedding æ¨¡å‹è¼‰å…¥å®Œæˆ")
    except Exception as e:
        print(f"  âŒ è¼‰å…¥ Embedding æ¨¡å‹å¤±æ•—: {e}")
        print("  å°‡åªæ¸¬è©¦å­—ç¬¦åˆ†å¡Šæ¨¡å¼")
        shared_embeddings = None
    
    # 1. å­—ç¬¦åˆ†å¡Š
    print("\n[æ–¹æ³• 1] å­—ç¬¦åˆ†å¡Šï¼ˆå›ºå®šå¤§å°ï¼‰")
    print("-" * 60)
    try:
        processor_char = DocumentProcessor(chunk_size=1000, chunk_overlap=200)
        documents_char = processor_char.process_file(str(file_path))
        print(f"  âœ“ å‰µå»ºäº† {len(documents_char)} å€‹ chunks")
        
        if documents_char:
            print(f"\n  ç¯„ä¾‹ chunkï¼ˆç¬¬ä¸€å€‹ï¼‰ï¼š")
            print(f"    é•·åº¦: {len(documents_char[0]['content'])} å­—ç¬¦")
            print(f"    å…§å®¹é è¦½: {documents_char[0]['content'][:150]}...")
            
            # çµ±è¨ˆè³‡è¨Š
            chunk_sizes = [len(doc['content']) for doc in documents_char]
            avg_size = sum(chunk_sizes) / len(chunk_sizes)
            min_size = min(chunk_sizes)
            max_size = max(chunk_sizes)
            print(f"\n  çµ±è¨ˆè³‡è¨Šï¼š")
            print(f"    å¹³å‡å¤§å°: {avg_size:.0f} å­—ç¬¦")
            print(f"    æœ€å°å¤§å°: {min_size} å­—ç¬¦")
            print(f"    æœ€å¤§å¤§å°: {max_size} å­—ç¬¦")
    except Exception as e:
        print(f"  âŒ å­—ç¬¦åˆ†å¡Šå¤±æ•—: {e}")
        documents_char = None
    
    # 2. èªç¾©åˆ†å¡Š
    print("\n[æ–¹æ³• 2] èªç¾©åˆ†å¡Šï¼ˆåŸºæ–¼èªç¾©ç›¸ä¼¼åº¦ï¼‰")
    print("-" * 60)
    documents_semantic = None
    
    if shared_embeddings:
        try:
            processor_semantic = DocumentProcessor(
                embeddings=shared_embeddings,
                use_semantic_chunking=True,
                breakpoint_threshold_amount=1.5,
                min_chunk_size=100
            )
            print("  âš ï¸  èªç¾©åˆ†å¡Šéœ€è¦è¨ˆç®— embeddingï¼Œå¯èƒ½éœ€è¦è¼ƒé•·æ™‚é–“ï¼Œè«‹ç¨å€™...")
            documents_semantic = processor_semantic.process_file(str(file_path))
            print(f"  âœ“ å‰µå»ºäº† {len(documents_semantic)} å€‹ chunks")
            
            if documents_semantic:
                print(f"\n  ç¯„ä¾‹ chunkï¼ˆç¬¬ä¸€å€‹ï¼‰ï¼š")
                print(f"    é•·åº¦: {len(documents_semantic[0]['content'])} å­—ç¬¦")
                print(f"    å…§å®¹é è¦½: {documents_semantic[0]['content'][:150]}...")
                
                # çµ±è¨ˆè³‡è¨Š
                chunk_sizes = [len(doc['content']) for doc in documents_semantic]
                avg_size = sum(chunk_sizes) / len(chunk_sizes)
                min_size = min(chunk_sizes)
                max_size = max(chunk_sizes)
                print(f"\n  çµ±è¨ˆè³‡è¨Šï¼š")
                print(f"    å¹³å‡å¤§å°: {avg_size:.0f} å­—ç¬¦")
                print(f"    æœ€å°å¤§å°: {min_size} å­—ç¬¦")
                print(f"    æœ€å¤§å¤§å°: {max_size} å­—ç¬¦")
        except ImportError as e:
            print(f"  âŒ èªç¾©åˆ†å¡Šéœ€è¦å®‰è£ langchain-experimental")
            print(f"    è«‹åŸ·è¡Œ: pip install langchain-experimental")
        except Exception as e:
            print(f"  âŒ èªç¾©åˆ†å¡Šå¤±æ•—: {e}")
            import traceback
            traceback.print_exc()
    else:
        print("  âš ï¸  ç„¡æ³•é€²è¡Œèªç¾©åˆ†å¡Šï¼ˆEmbedding æ¨¡å‹è¼‰å…¥å¤±æ•—ï¼‰")
    
    # 3. å°æ¯”ç¸½çµ
    print("\n" + "=" * 60)
    print("å°æ¯”ç¸½çµ")
    print("=" * 60)
    
    if documents_char and documents_semantic:
        print(f"\nå­—ç¬¦åˆ†å¡Š: {len(documents_char)} å€‹ chunks")
        print(f"èªç¾©åˆ†å¡Š: {len(documents_semantic)} å€‹ chunks")
        print(f"å·®ç•°: {abs(len(documents_char) - len(documents_semantic))} å€‹ chunks")
        
        print("\nğŸ’¡ è§€å¯Ÿï¼š")
        print("  - èªç¾©åˆ†å¡Šæœƒæ ¹æ“šèªç¾©é‚Šç•Œåˆ‡åˆ†ï¼Œä¸æœƒåœ¨å¥å­ä¸­é–“åˆ‡æ–·")
        print("  - å­—ç¬¦åˆ†å¡Šä½¿ç”¨å›ºå®šå¤§å°ï¼Œå¯èƒ½æœƒåˆ‡æ–·å¥å­")
        print("  - èªç¾©åˆ†å¡Šçš„ chunks å¤§å°å¯èƒ½æ›´ä¸è¦å‰‡ï¼Œä½†èªç¾©æ›´å®Œæ•´")
    elif documents_char:
        print(f"\nå­—ç¬¦åˆ†å¡Š: {len(documents_char)} å€‹ chunks")
        print("èªç¾©åˆ†å¡Š: æœªå®Œæˆï¼ˆè«‹æª¢æŸ¥éŒ¯èª¤è³‡è¨Šï¼‰")
    
    return documents_char, documents_semantic, shared_embeddings


def test_with_semantic_chunking(
    file_path: str,
    test_query: str,
    use_semantic: bool = True
):
    """
    ä½¿ç”¨èªç¾©åˆ†å¡Šï¼ˆæˆ–å­—ç¬¦åˆ†å¡Šï¼‰å»ºç«‹ RAG ç³»çµ±ä¸¦æ¸¬è©¦
    
    Args:
        file_path: æª”æ¡ˆè·¯å¾‘
        test_query: æ¸¬è©¦å•é¡Œ
        use_semantic: æ˜¯å¦ä½¿ç”¨èªç¾©åˆ†å¡Šï¼ˆTrue: èªç¾©åˆ†å¡Š, False: å­—ç¬¦åˆ†å¡Šï¼‰
    """
    print("\n" + "=" * 60)
    print(f"ä½¿ç”¨ {'èªç¾©åˆ†å¡Š' if use_semantic else 'å­—ç¬¦åˆ†å¡Š'} å»ºç«‹ RAG ç³»çµ±")
    print("=" * 60)
    
    # åˆå§‹åŒ–å…±ç”¨çš„ Embedding æ¨¡å‹ï¼ˆå¦‚æœéœ€è¦èªç¾©åˆ†å¡Šï¼‰
    shared_embeddings = None
    if use_semantic:
        print("\n[æ­¥é©Ÿ 0] åˆå§‹åŒ–å…±ç”¨çš„ Embedding æ¨¡å‹...")
        try:
            from langchain_community.embeddings import HuggingFaceEmbeddings
            from src.retrievers.vector_retriever import get_device
            
            hf_cache_dir = os.getenv("HF_CACHE_DIR", None)
            device = get_device()
            
            model_kwargs = {'device': device}
            if hf_cache_dir:
                model_kwargs['cache_dir'] = hf_cache_dir
            
            shared_embeddings = HuggingFaceEmbeddings(
                model_name="sentence-transformers/all-MiniLM-L6-v2",
                model_kwargs=model_kwargs,
                encode_kwargs={'normalize_embeddings': True}
            )
            print("  âœ“ Embedding æ¨¡å‹åˆå§‹åŒ–å®Œæˆ")
        except Exception as e:
            print(f"  âŒ åˆå§‹åŒ–å¤±æ•—: {e}")
            print("  å°‡å›é€€åˆ°å­—ç¬¦åˆ†å¡Šæ¨¡å¼")
            use_semantic = False
    
    # 1. è™•ç†æª”æ¡ˆ
    print(f"\n[æ­¥é©Ÿ 1] è™•ç†æª”æ¡ˆï¼ˆä½¿ç”¨{'èªç¾©åˆ†å¡Š' if use_semantic else 'å­—ç¬¦åˆ†å¡Š'}ï¼‰...")
    print("-" * 60)
    
    try:
        if use_semantic and shared_embeddings:
            processor = DocumentProcessor(
                embeddings=shared_embeddings,
                use_semantic_chunking=True,
                breakpoint_threshold_amount=1.5,
                min_chunk_size=100
            )
            print("  âš ï¸  èªç¾©åˆ†å¡Šéœ€è¦è¨ˆç®— embeddingï¼Œå¯èƒ½éœ€è¦è¼ƒé•·æ™‚é–“ï¼Œè«‹ç¨å€™...")
        else:
            processor = DocumentProcessor(chunk_size=1000, chunk_overlap=200)
        
        documents = processor.process_file(str(file_path))
        print(f"  âœ“ è™•ç†å®Œæˆï¼Œå‰µå»ºäº† {len(documents)} å€‹ chunks")
        
        if documents:
            chunking_method = documents[0]['metadata'].get('chunking_method', 'character')
            print(f"  åˆ†å¡Šæ–¹æ³•: {chunking_method}")
            print(f"  ç¯„ä¾‹ chunk é•·åº¦: {len(documents[0]['content'])} å­—ç¬¦")
    except ImportError as e:
        print(f"  âŒ éœ€è¦å®‰è£ langchain-experimental: pip install langchain-experimental")
        return
    except Exception as e:
        print(f"  âŒ è™•ç†æª”æ¡ˆå¤±æ•—: {e}")
        import traceback
        traceback.print_exc()
        return
    
    # 2. åˆå§‹åŒ–æª¢ç´¢ç³»çµ±
    print("\n[æ­¥é©Ÿ 2] åˆå§‹åŒ–æª¢ç´¢ç³»çµ±...")
    print("-" * 60)
    
    try:
        print("  - åˆå§‹åŒ– BM25 æª¢ç´¢å™¨...")
        bm25_retriever = BM25Retriever(documents)
        
        print("  - åˆå§‹åŒ–å‘é‡æª¢ç´¢å™¨...")
        # ä½¿ç”¨ä¸åŒçš„è³‡æ–™åº«ç›®éŒ„é¿å…è¡çª
        db_dir = "./chroma_db_semantic" if use_semantic else "./chroma_db_character"
        vector_retriever = VectorRetriever(
            documents,
            embedding_model="sentence-transformers/all-MiniLM-L6-v2",
            persist_directory=db_dir,
            embeddings=shared_embeddings  # å‚³å…¥å…±ç”¨çš„ embeddings
        )
        
        print("  - åˆå§‹åŒ–æ··åˆæœå°‹...")
        hybrid_search = HybridSearch(
            sparse_retriever=bm25_retriever,
            dense_retriever=vector_retriever,
            fusion_method="rrf",
            rrf_k=60
        )
        
        print("  âœ“ æª¢ç´¢ç³»çµ±åˆå§‹åŒ–å®Œæˆ")
    except Exception as e:
        print(f"  âŒ æª¢ç´¢ç³»çµ±åˆå§‹åŒ–å¤±æ•—: {e}")
        import traceback
        traceback.print_exc()
        return
    
    # 3. åˆå§‹åŒ–é‡æ’åºå’Œ RAG ç®¡ç·š
    print("\n[æ­¥é©Ÿ 3] åˆå§‹åŒ–é‡æ’åºå’Œ RAG ç®¡ç·š...")
    print("-" * 60)
    
    try:
        print("  - åˆå§‹åŒ–é‡æ’åºå™¨...")
        reranker = Reranker(
            model_name="BAAI/bge-reranker-base",
            batch_size=16
        )
        
        print("  - åˆå§‹åŒ– RAG ç®¡ç·š...")
        rag_pipeline = RAGPipeline(
            hybrid_search=hybrid_search,
            reranker=reranker,
            recall_k=20,
            adaptive_recall=True
        )
        
        print("  âœ“ RAG ç®¡ç·šåˆå§‹åŒ–å®Œæˆ")
    except Exception as e:
        print(f"  âŒ RAG ç®¡ç·šåˆå§‹åŒ–å¤±æ•—: {e}")
        print("   é€™å¯èƒ½æ˜¯å› ç‚ºé‡æ’åºæ¨¡å‹ä¸‹è¼‰å¤±æ•—")
        print("   ä½ å¯ä»¥ç¹¼çºŒä½¿ç”¨æ··åˆæœå°‹ï¼ˆä¸é€²è¡Œé‡æ’åºï¼‰")
        return
    
    # 4. åˆå§‹åŒ– LLM å’Œæ ¼å¼åŒ–å™¨
    print("\n[æ­¥é©Ÿ 4] åˆå§‹åŒ– LLM å’Œæ ¼å¼åŒ–å™¨...")
    print("-" * 60)
    
    try:
        print("  - åˆå§‹åŒ– Prompt æ ¼å¼åŒ–å™¨...")
        formatter = PromptFormatter(format_style="detailed")
        
        print("  - åˆå§‹åŒ– LLM...")
        llm = OllamaLLM(
            model_name="llama3.2:3b",
            timeout=180
        )
        
        print("  âœ“ LLM å’Œæ ¼å¼åŒ–å™¨åˆå§‹åŒ–å®Œæˆ")
    except ConnectionError as e:
        print(f"  âŒ LLM é€£æ¥å¤±æ•—: {e}")
        return
    except Exception as e:
        print(f"  âŒ åˆå§‹åŒ–å¤±æ•—: {e}")
        import traceback
        traceback.print_exc()
        return
    
    # 5. åŸ·è¡Œå°æ¯”æ¸¬è©¦
    print("\n" + "=" * 60)
    print("é–‹å§‹åŸ·è¡Œ RAG å°æ¯”æ¸¬è©¦")
    print("=" * 60)
    
    test_rag_vs_no_rag(
        llm=llm,
        rag_pipeline=rag_pipeline,
        formatter=formatter,
        query=test_query,
        test_file_path=str(file_path)
    )
    
    print("\n" + "=" * 60)
    print("æ¸¬è©¦å®Œæˆï¼")
    print("=" * 60)


def main():
    """ä¸»å‡½æ•¸"""
    print("=" * 60)
    print("èªç¾©åˆ†å¡ŠåŠŸèƒ½æ¸¬è©¦")
    print("=" * 60)
    print("\né€™å€‹è…³æœ¬å¯ä»¥ï¼š")
    print("  1. å°æ¯”èªç¾©åˆ†å¡Šå’Œå­—ç¬¦åˆ†å¡Šçš„æ•ˆæœ")
    print("  2. ä½¿ç”¨èªç¾©åˆ†å¡Šå»ºç«‹ RAG ç³»çµ±ä¸¦æ¸¬è©¦")
    print("  3. å°æ¯”æœ‰ RAG vs ç„¡ RAG çš„æ•ˆæœ")
    
    # ========== æ­¥é©Ÿ 1: æº–å‚™æª”æ¡ˆ ==========
    print("\n[æ­¥é©Ÿ 1] æº–å‚™æ¸¬è©¦æª”æ¡ˆ")
    print("-" * 60)
    
    if len(sys.argv) > 1:
        file_path = sys.argv[1]
    else:
        file_path = input("\nè«‹è¼¸å…¥æª”æ¡ˆè·¯å¾‘ï¼ˆPDF, DOCX, æˆ– TXTï¼‰: ").strip()
        
        if not file_path:
            print("\nâš ï¸  æœªæä¾›æª”æ¡ˆè·¯å¾‘")
            print("\nä½¿ç”¨æ–¹æ³•ï¼š")
            print("  python test_semantic_chunking.py <æª”æ¡ˆè·¯å¾‘> [æ¸¬è©¦å•é¡Œ]")
            print("\nç¯„ä¾‹ï¼š")
            print("  python test_semantic_chunking.py ./documents/my_document.pdf")
            return
    
    file_path = Path(file_path)
    
    if not file_path.exists():
        print(f"\nâŒ æª”æ¡ˆä¸å­˜åœ¨: {file_path}")
        return
    
    print(f"âœ“ æ‰¾åˆ°æª”æ¡ˆ: {file_path}")
    print(f"  æª”æ¡ˆé¡å‹: {file_path.suffix}")
    print(f"  æª”æ¡ˆå¤§å°: {file_path.stat().st_size / 1024:.2f} KB")
    
    # ========== æ­¥é©Ÿ 2: é¸æ“‡æ¸¬è©¦æ¨¡å¼ ==========
    print("\n[æ­¥é©Ÿ 2] é¸æ“‡æ¸¬è©¦æ¨¡å¼")
    print("-" * 60)
    print("\nè«‹é¸æ“‡æ¸¬è©¦æ¨¡å¼ï¼š")
    print("  1. å°æ¯”èªç¾©åˆ†å¡Šå’Œå­—ç¬¦åˆ†å¡Šï¼ˆåªé¡¯ç¤ºåˆ†å¡Šæ•ˆæœï¼Œä¸å»ºç«‹ RAGï¼‰")
    print("  2. ä½¿ç”¨èªç¾©åˆ†å¡Šå»ºç«‹ RAG ç³»çµ±ä¸¦æ¸¬è©¦")
    print("  3. ä½¿ç”¨å­—ç¬¦åˆ†å¡Šå»ºç«‹ RAG ç³»çµ±ä¸¦æ¸¬è©¦ï¼ˆå°æ¯”ç”¨ï¼‰")
    print("  4. å…¨éƒ¨åŸ·è¡Œï¼ˆå°æ¯”åˆ†å¡Š + èªç¾©åˆ†å¡Š RAG + å­—ç¬¦åˆ†å¡Š RAGï¼‰")
    
    if len(sys.argv) > 2:
        # å¾å‘½ä»¤è¡Œåƒæ•¸ç²å–æ¨¡å¼
        mode = sys.argv[2]
    else:
        mode = input("\nè«‹è¼¸å…¥é¸é … (1/2/3/4ï¼Œé è¨­ 2): ").strip() or "2"
    
    # ========== æ­¥é©Ÿ 3: ç²å–æ¸¬è©¦å•é¡Œ ==========
    test_query = None
    if mode in ["2", "3", "4"]:
        print("\n[æ­¥é©Ÿ 3] æº–å‚™æ¸¬è©¦å•é¡Œ")
        print("-" * 60)
        
        if len(sys.argv) > 3:
            test_query = " ".join(sys.argv[3:])
        else:
            print("\nè«‹è¼¸å…¥ä¸€å€‹æ¸¬è©¦å•é¡Œï¼ˆæ‡‰è©²æ¶‰åŠä½ çš„æ–‡æª”å…§å®¹ï¼‰ï¼š")
            print("ç¯„ä¾‹ï¼š")
            print("  - 'é€™ä»½æ–‡æª”çš„ä¸»è¦å…§å®¹æ˜¯ä»€éº¼ï¼Ÿ'")
            print("  - 'æ–‡æª”ä¸­æåˆ°äº†å“ªäº›é—œéµæ¦‚å¿µï¼Ÿ'")
            test_query = input("\nä½ çš„å•é¡Œ: ").strip()
        
        if not test_query:
            test_query = "é€™ä»½æ–‡æª”çš„ä¸»è¦å…§å®¹æ˜¯ä»€éº¼ï¼Ÿ"
        
        print(f"âœ“ æ¸¬è©¦å•é¡Œ: '{test_query}'")
    
    # ========== åŸ·è¡Œæ¸¬è©¦ ==========
    if mode == "1":
        # åªå°æ¯”åˆ†å¡Šæ•ˆæœ
        compare_chunking_methods(str(file_path))
    
    elif mode == "2":
        # ä½¿ç”¨èªç¾©åˆ†å¡Šå»ºç«‹ RAG
        test_with_semantic_chunking(
            file_path=str(file_path),
            test_query=test_query,
            use_semantic=True
        )
    
    elif mode == "3":
        # ä½¿ç”¨å­—ç¬¦åˆ†å¡Šå»ºç«‹ RAGï¼ˆå°æ¯”ç”¨ï¼‰
        test_with_semantic_chunking(
            file_path=str(file_path),
            test_query=test_query,
            use_semantic=False
        )
    
    elif mode == "4":
        # å…¨éƒ¨åŸ·è¡Œ
        print("\n" + "=" * 60)
        print("åŸ·è¡Œå®Œæ•´æ¸¬è©¦æµç¨‹")
        print("=" * 60)
        
        # 1. å°æ¯”åˆ†å¡Š
        documents_char, documents_semantic, shared_embeddings = compare_chunking_methods(str(file_path))
        
        # 2. èªç¾©åˆ†å¡Š RAG
        if documents_semantic:
            print("\n\n" + "=" * 60)
            print("æ¸¬è©¦ 1: ä½¿ç”¨èªç¾©åˆ†å¡Šçš„ RAG ç³»çµ±")
            print("=" * 60)
            test_with_semantic_chunking(
                file_path=str(file_path),
                test_query=test_query,
                use_semantic=True
            )
        
        # 3. å­—ç¬¦åˆ†å¡Š RAGï¼ˆå°æ¯”ï¼‰
        if documents_char:
            print("\n\n" + "=" * 60)
            print("æ¸¬è©¦ 2: ä½¿ç”¨å­—ç¬¦åˆ†å¡Šçš„ RAG ç³»çµ±ï¼ˆå°æ¯”ç”¨ï¼‰")
            print("=" * 60)
            test_with_semantic_chunking(
                file_path=str(file_path),
                test_query=test_query,
                use_semantic=False
            )
    
    else:
        print(f"\nâŒ ç„¡æ•ˆçš„é¸é …: {mode}")
        print("è«‹é¸æ“‡ 1, 2, 3, æˆ– 4")
    
    print("\n" + "=" * 60)
    print("æ‰€æœ‰æ¸¬è©¦å®Œæˆï¼")
    print("=" * 60)
    print("\nğŸ’¡ æç¤ºï¼š")
    print("  - èªç¾©åˆ†å¡Šèƒ½ä¿æŒèªç¾©å®Œæ•´æ€§ï¼Œä¸æœƒåœ¨å¥å­ä¸­é–“åˆ‡æ–·")
    print("  - å­—ç¬¦åˆ†å¡Šé€Ÿåº¦æ›´å¿«ï¼Œä½†å¯èƒ½æœƒåˆ‡æ–·å¥å­")
    print("  - å¯ä»¥å°æ¯”å…©ç¨®åˆ†å¡Šæ–¹å¼çš„æª¢ç´¢æ•ˆæœï¼Œé¸æ“‡æœ€é©åˆä½ çš„æ–¹å¼")


if __name__ == "__main__":
    main()

