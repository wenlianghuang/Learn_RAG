"""
æ—¥å¸¸å•é¡Œ RAG å°æ¯”æ¸¬è©¦ï¼šä½¿ç”¨ Wikipedia ç­‰å¤§çœ¾åŒ–è³‡æ–™ä¾†æº
æ¸¬è©¦æ›´ç°¡å–®ã€ä¸€èˆ¬äººèƒ½ç†è§£çš„å•é¡Œ
"""
import os
import sys
import time
import hashlib
from typing import List, Dict
from src import (
    DocumentProcessor,
    BM25Retriever,
    VectorRetriever,
    HybridSearch,
    Reranker,
    RAGPipeline,
    PromptFormatter,
    OllamaLLM,
    HyDERAG,
    SubQueryDecompositionRAG,
    HybridSubqueryHyDERAG
)

try:
    import wikipedia
    WIKIPEDIA_AVAILABLE = True
except ImportError:
    WIKIPEDIA_AVAILABLE = False
    print("âš ï¸  æœªå®‰è£ wikipedia å¥—ä»¶ï¼Œå°‡ä½¿ç”¨ç¤ºä¾‹æ–‡æª”")
    print("   å®‰è£å‘½ä»¤: pip install wikipedia")


def fetch_wikipedia_articles(titles: List[str]) -> List[Dict]:
    """
    å¾ Wikipedia ç²å–æ–‡ç« 
    
    Args:
        titles: æ–‡ç« æ¨™é¡Œåˆ—è¡¨
        
    Returns:
        æ–‡ç« åˆ—è¡¨ï¼Œæ¯å€‹åŒ…å«æ¨™é¡Œå’Œå…§å®¹
    """
    articles = []
    
    if not WIKIPEDIA_AVAILABLE:
        # å¦‚æœæ²’æœ‰å®‰è£ wikipediaï¼Œä½¿ç”¨ç¤ºä¾‹æ–‡æª”
        print("âš ï¸  ä½¿ç”¨ç¤ºä¾‹æ–‡æª”ï¼ˆå»ºè­°å®‰è£ wikipedia å¥—ä»¶ä»¥ç²å–çœŸå¯¦è³‡æ–™ï¼‰")
        example_articles = [
            {
                "title": "äººå·¥æ™ºæ…§",
                "content": """äººå·¥æ™ºæ…§ï¼ˆArtificial Intelligence, AIï¼‰æ˜¯é›»è…¦ç§‘å­¸çš„ä¸€å€‹åˆ†æ”¯ï¼Œæ—¨åœ¨å‰µå»ºèƒ½å¤ åŸ·è¡Œé€šå¸¸éœ€è¦äººé¡æ™ºèƒ½çš„ä»»å‹™çš„ç³»çµ±ã€‚
äººå·¥æ™ºæ…§åŒ…æ‹¬æ©Ÿå™¨å­¸ç¿’ã€æ·±åº¦å­¸ç¿’ã€è‡ªç„¶èªè¨€è™•ç†ç­‰æŠ€è¡“ã€‚æ©Ÿå™¨å­¸ç¿’æ˜¯äººå·¥æ™ºæ…§çš„ä¸€å€‹å­é ˜åŸŸï¼Œå®ƒä½¿é›»è…¦èƒ½å¤ å¾æ•¸æ“šä¸­å­¸ç¿’ï¼Œè€Œç„¡éœ€æ˜ç¢ºç·¨ç¨‹ã€‚
æ·±åº¦å­¸ç¿’æ˜¯æ©Ÿå™¨å­¸ç¿’çš„ä¸€å€‹å­é›†ï¼Œä½¿ç”¨ç¥ç¶“ç¶²è·¯ä¾†æ¨¡æ“¬äººè…¦çš„å·¥ä½œæ–¹å¼ã€‚è‡ªç„¶èªè¨€è™•ç†ä½¿é›»è…¦èƒ½å¤ ç†è§£å’Œç”Ÿæˆäººé¡èªè¨€ã€‚
äººå·¥æ™ºæ…§åœ¨é†«ç™‚ã€é‡‘èã€äº¤é€šã€æ•™è‚²ç­‰é ˜åŸŸéƒ½æœ‰å»£æ³›æ‡‰ç”¨ã€‚ç¾ä»£ AI ç³»çµ±å¯ä»¥é€²è¡Œåœ–åƒè­˜åˆ¥ã€èªéŸ³è­˜åˆ¥ã€è‡ªå‹•é§•é§›ã€æ™ºèƒ½æ¨è–¦ç­‰ä»»å‹™ã€‚
AI æŠ€è¡“æ­£åœ¨æ”¹è®Šæˆ‘å€‘çš„ç”Ÿæ´»æ–¹å¼ï¼Œå¾æ™ºèƒ½æ‰‹æ©Ÿçš„èªéŸ³åŠ©æ‰‹åˆ°è‡ªå‹•é§•é§›æ±½è»Šï¼Œå¾é†«ç™‚è¨ºæ–·åˆ°é‡‘èåˆ†æï¼Œç„¡è™•ä¸åœ¨ã€‚"""
            },
            {
                "title": "æ©Ÿå™¨å­¸ç¿’",
                "content": """æ©Ÿå™¨å­¸ç¿’ï¼ˆMachine Learningï¼‰æ˜¯äººå·¥æ™ºæ…§çš„ä¸€å€‹åˆ†æ”¯ï¼Œå°ˆæ³¨æ–¼é–‹ç™¼èƒ½å¤ å¾æ•¸æ“šä¸­å­¸ç¿’çš„ç®—æ³•å’Œçµ±è¨ˆæ¨¡å‹ã€‚
æ©Ÿå™¨å­¸ç¿’ç®—æ³•é€šéåˆ†æå¤§é‡æ•¸æ“šä¾†è­˜åˆ¥æ¨¡å¼ä¸¦åšå‡ºé æ¸¬æˆ–æ±ºç­–ã€‚ä¸»è¦é¡å‹åŒ…æ‹¬ç›£ç£å­¸ç¿’ã€ç„¡ç›£ç£å­¸ç¿’å’Œå¼·åŒ–å­¸ç¿’ã€‚
ç›£ç£å­¸ç¿’ä½¿ç”¨æ¨™è¨˜çš„æ•¸æ“šä¾†è¨“ç·´æ¨¡å‹ï¼Œä¾‹å¦‚åˆ†é¡å’Œå›æ­¸å•é¡Œã€‚å¸¸è¦‹æ‡‰ç”¨åŒ…æ‹¬åƒåœ¾éƒµä»¶éæ¿¾ã€åœ–åƒåˆ†é¡ã€åƒ¹æ ¼é æ¸¬ç­‰ã€‚
ç„¡ç›£ç£å­¸ç¿’å¾æœªæ¨™è¨˜çš„æ•¸æ“šä¸­ç™¼ç¾éš±è—çš„æ¨¡å¼ï¼Œä¾‹å¦‚èšé¡åˆ†æã€‚æ‡‰ç”¨åŒ…æ‹¬å®¢æˆ¶åˆ†ç¾¤ã€ç•°å¸¸æª¢æ¸¬ç­‰ã€‚
å¼·åŒ–å­¸ç¿’é€šéèˆ‡ç’°å¢ƒäº’å‹•ä¾†å­¸ç¿’ï¼Œé€šéçå‹µå’Œæ‡²ç½°ä¾†æ”¹é€²è¡Œç‚ºã€‚æ‡‰ç”¨åŒ…æ‹¬éŠæˆ² AIã€æ©Ÿå™¨äººæ§åˆ¶ç­‰ã€‚
æ©Ÿå™¨å­¸ç¿’åœ¨æ¨è–¦ç³»çµ±ã€æœç´¢å¼•æ“ã€èªéŸ³åŠ©æ‰‹ã€è‡ªå‹•é§•é§›ç­‰é ˜åŸŸæœ‰å»£æ³›æ‡‰ç”¨ã€‚"""
            },
            {
                "title": "æ·±åº¦å­¸ç¿’",
                "content": """æ·±åº¦å­¸ç¿’ï¼ˆDeep Learningï¼‰æ˜¯æ©Ÿå™¨å­¸ç¿’çš„ä¸€å€‹å­é›†ï¼Œä½¿ç”¨å¤šå±¤ç¥ç¶“ç¶²è·¯ä¾†å­¸ç¿’æ•¸æ“šçš„è¡¨ç¤ºã€‚
æ·±åº¦å­¸ç¿’æ¨¡å‹å¯ä»¥è‡ªå‹•å¾åŸå§‹æ•¸æ“šä¸­æå–ç‰¹å¾µï¼Œç„¡éœ€äººå·¥ç‰¹å¾µå·¥ç¨‹ã€‚å¸¸è¦‹çš„æ·±åº¦å­¸ç¿’æ¶æ§‹åŒ…æ‹¬å·ç©ç¥ç¶“ç¶²è·¯ï¼ˆCNNï¼‰ã€å¾ªç’°ç¥ç¶“ç¶²è·¯ï¼ˆRNNï¼‰å’Œ Transformerã€‚
å·ç©ç¥ç¶“ç¶²è·¯ä¸»è¦ç”¨æ–¼åœ–åƒè™•ç†å’Œè¨ˆç®—æ©Ÿè¦–è¦ºä»»å‹™ï¼Œå¦‚äººè‡‰è­˜åˆ¥ã€ç‰©é«”æª¢æ¸¬ç­‰ã€‚
å¾ªç’°ç¥ç¶“ç¶²è·¯é©åˆè™•ç†åºåˆ—æ•¸æ“šï¼Œå¦‚è‡ªç„¶èªè¨€å’Œæ™‚é–“åºåˆ—ï¼Œæ‡‰ç”¨åŒ…æ‹¬èªéŸ³è­˜åˆ¥ã€æ–‡æœ¬ç”Ÿæˆç­‰ã€‚
Transformer æ¶æ§‹åœ¨è‡ªç„¶èªè¨€è™•ç†é ˜åŸŸå–å¾—äº†é‡å¤§çªç ´ï¼Œå¦‚ BERT å’Œ GPT æ¨¡å‹ï¼Œèƒ½å¤ ç†è§£èªè¨€çš„èªç¾©å’Œä¸Šä¸‹æ–‡ã€‚
æ·±åº¦å­¸ç¿’åœ¨èªéŸ³è­˜åˆ¥ã€åœ–åƒè­˜åˆ¥ã€è‡ªç„¶èªè¨€è™•ç†ã€è‡ªå‹•ç¿»è­¯ç­‰é ˜åŸŸè¡¨ç¾å‡ºè‰²ã€‚"""
            },
            {
                "title": "è‡ªç„¶èªè¨€è™•ç†",
                "content": """è‡ªç„¶èªè¨€è™•ç†ï¼ˆNatural Language Processing, NLPï¼‰æ˜¯äººå·¥æ™ºæ…§å’Œèªè¨€å­¸çš„äº¤å‰é ˜åŸŸï¼Œæ—¨åœ¨ä½¿é›»è…¦èƒ½å¤ ç†è§£ã€è§£é‡‹å’Œç”Ÿæˆäººé¡èªè¨€ã€‚
NLP çš„ä»»å‹™åŒ…æ‹¬æ–‡æœ¬åˆ†é¡ã€æƒ…æ„Ÿåˆ†æã€æ©Ÿå™¨ç¿»è­¯ã€å•ç­”ç³»çµ±ã€æ–‡æœ¬æ‘˜è¦ç­‰ã€‚ç¾ä»£ NLP ä¸»è¦ä¾è³´æ·±åº¦å­¸ç¿’å’Œ Transformer æ¶æ§‹ã€‚
è©åµŒå…¥æŠ€è¡“å¦‚ Word2Vec å’Œ GloVe å°‡è©èªè½‰æ›ç‚ºæ•¸å€¼å‘é‡ï¼Œä½¿é›»è…¦èƒ½å¤ ç†è§£è©èªçš„èªç¾©é—œä¿‚ã€‚
é è¨“ç·´èªè¨€æ¨¡å‹å¦‚ BERT å’Œ GPT åœ¨å„ç¨® NLP ä»»å‹™ä¸Šå–å¾—äº†å„ªç•°çš„è¡¨ç¾ï¼Œèƒ½å¤ ç†è§£èªè¨€çš„ä¸Šä¸‹æ–‡å’Œèªç¾©ã€‚
NLP æ‡‰ç”¨åŒ…æ‹¬æ™ºèƒ½åŠ©æ‰‹ï¼ˆå¦‚ Siriã€Alexaï¼‰ã€æœç´¢å¼•æ“ã€è‡ªå‹•ç¿»è­¯ã€å…§å®¹æ¨è–¦ã€æƒ…æ„Ÿåˆ†æç­‰ã€‚
é€™äº›æ‡‰ç”¨è®“é›»è…¦èƒ½å¤ ç†è§£å’Œè™•ç†äººé¡èªè¨€ï¼Œå¤§å¤§æå‡äº†äººæ©Ÿäº’å‹•çš„é«”é©—ã€‚"""
            },
            {
                "title": "ç¥ç¶“ç¶²è·¯",
                "content": """ç¥ç¶“ç¶²è·¯ï¼ˆNeural Networkï¼‰æ˜¯å—ç”Ÿç‰©ç¥ç¶“ç³»çµ±å•Ÿç™¼çš„è¨ˆç®—æ¨¡å‹ï¼Œç”±ç›¸äº’é€£æ¥çš„ç¯€é»ï¼ˆç¥ç¶“å…ƒï¼‰çµ„æˆã€‚
äººå·¥ç¥ç¶“ç¶²è·¯ç”±è¼¸å…¥å±¤ã€éš±è—å±¤å’Œè¼¸å‡ºå±¤çµ„æˆã€‚æ¯å€‹é€£æ¥éƒ½æœ‰æ¬Šé‡ï¼Œé€šéè¨“ç·´éç¨‹èª¿æ•´é€™äº›æ¬Šé‡ä¾†å­¸ç¿’æ¨¡å¼ã€‚
åå‘å‚³æ’­ç®—æ³•æ˜¯è¨“ç·´ç¥ç¶“ç¶²è·¯çš„é—œéµæŠ€è¡“ï¼Œé€šéè¨ˆç®—æ¢¯åº¦ä¾†æ›´æ–°æ¬Šé‡ï¼Œä½¿ç¶²è·¯èƒ½å¤ å¾éŒ¯èª¤ä¸­å­¸ç¿’ã€‚
æ·±åº¦ç¥ç¶“ç¶²è·¯æœ‰å¤šå€‹éš±è—å±¤ï¼Œèƒ½å¤ å­¸ç¿’æ›´è¤‡é›œçš„æ¨¡å¼å’Œç‰¹å¾µã€‚å±¤æ•¸è¶Šå¤šï¼Œç¶²è·¯è¶Šæ·±ï¼Œå­¸ç¿’èƒ½åŠ›è¶Šå¼·ã€‚
ç¥ç¶“ç¶²è·¯åœ¨åœ–åƒè­˜åˆ¥ã€èªéŸ³è­˜åˆ¥ã€è‡ªç„¶èªè¨€è™•ç†ã€éŠæˆ² AI ç­‰é ˜åŸŸå–å¾—äº†é‡å¤§æˆåŠŸã€‚
é€™äº›ç¶²è·¯èƒ½å¤ è‡ªå‹•å¾æ•¸æ“šä¸­å­¸ç¿’è¤‡é›œçš„æ¨¡å¼ï¼Œç„¡éœ€äººå·¥è¨­è¨ˆç‰¹å¾µï¼Œé€™æ˜¯å®ƒå€‘å¼·å¤§çš„åŸå› ã€‚"""
            }
        ]
        return example_articles
    
    # è¨­ç½® Wikipedia èªè¨€ï¼ˆä¸­æ–‡ï¼‰
    try:
        wikipedia.set_lang("zh")
    except:
        pass
    
    for title in titles:
        try:
            print(f"  æ­£åœ¨ç²å–: {title}...")
            page = wikipedia.page(title, auto_suggest=False)
            articles.append({
                "title": page.title,
                "content": page.content
            })
            print(f"  âœ… æˆåŠŸç²å–: {page.title} ({len(page.content)} å­—ç¬¦)")
        except wikipedia.exceptions.DisambiguationError as e:
            # å¦‚æœæœ‰æ­§ç¾©ï¼Œä½¿ç”¨ç¬¬ä¸€å€‹é¸é …
            try:
                page = wikipedia.page(e.options[0])
                articles.append({
                    "title": page.title,
                    "content": page.content
                })
                print(f"  âœ… æˆåŠŸç²å–ï¼ˆæ­§ç¾©è§£æ±ºï¼‰: {page.title}")
            except:
                print(f"  âš ï¸  ç„¡æ³•ç²å–: {title}")
        except Exception as e:
            print(f"  âš ï¸  ç„¡æ³•ç²å–: {title} ({e})")
            continue
    
    return articles


def process_wikipedia_articles(articles: List[Dict], processor: DocumentProcessor) -> List[Dict]:
    """
    è™•ç† Wikipedia æ–‡ç« ï¼Œè½‰æ›ç‚ºæ–‡æª”æ ¼å¼
    
    Args:
        articles: æ–‡ç« åˆ—è¡¨
        processor: æ–‡æª”è™•ç†å™¨
        
    Returns:
        è™•ç†å¾Œçš„æ–‡æª” chunks
    """
    documents = []
    
    for article in articles:
        # ä½¿ç”¨æ¨™é¡Œå’Œå…§å®¹
        full_text = f"æ¨™é¡Œ: {article['title']}\n\nå…§å®¹: {article['content']}"
        
        # åˆ†å‰²æ–‡å­—
        chunks = processor.text_splitter.split_text(full_text)
        
        # ç‚ºæ¯å€‹ chunk å‰µå»ºæ–‡æª”ç‰©ä»¶
        for i, chunk in enumerate(chunks):
            doc = {
                "content": chunk,
                "metadata": {
                    "title": article['title'],
                    "source": "Wikipedia",
                    "chunk_index": i,
                    "total_chunks": len(chunks),
                    "chunking_method": "character"
                }
            }
            documents.append(doc)
    
    return documents


def evaluate_answer_quality(answer: str, query: str) -> dict:
    """è©•ä¼°ç­”æ¡ˆè³ªé‡"""
    query_keywords = set(query.lower().split())
    answer_lower = answer.lower()
    
    # é—œéµè©è¦†è“‹ç‡
    matched = sum(1 for kw in query_keywords if kw in answer_lower)
    keyword_coverage = matched / len(query_keywords) if query_keywords else 0
    
    # ç­”æ¡ˆè©³ç´°ç¨‹åº¦
    detail_score = min(len(answer) / 500, 1.0)
    
    # å°ˆæ¥­è¡“èªæ•¸é‡ï¼ˆç°¡åŒ–ç‰ˆï¼‰
    technical_terms = ['æ–¹æ³•', 'æŠ€è¡“', 'æ‡‰ç”¨', 'åŸç†', 'ç³»çµ±', 'ç®—æ³•', 
                      'method', 'technique', 'application', 'principle', 'system', 'algorithm']
    tech_count = sum(1 for term in technical_terms if term in answer_lower)
    tech_score = min(tech_count / 5, 1.0)
    
    overall_score = (keyword_coverage * 0.4 + detail_score * 0.3 + tech_score * 0.3)
    
    return {
        'keyword_coverage': keyword_coverage,
        'detail_score': detail_score,
        'tech_score': tech_score,
        'overall_score': overall_score
    }


def test_everyday_rag_comparison():
    """æ—¥å¸¸å•é¡Œ RAG å°æ¯”æ¸¬è©¦"""
    print("=" * 80)
    print("ğŸ¯ æ—¥å¸¸å•é¡Œ RAG å°æ¯”æ¸¬è©¦ - ä½¿ç”¨å¤§çœ¾åŒ–è³‡æ–™ä¾†æº")
    print("=" * 80)
    
    # 1. ç²å– Wikipedia æ–‡ç« 
    print("\n[1/6] ç²å– Wikipedia æ–‡ç« ...")
    article_titles = [
        "äººå·¥æ™ºæ…§",
        "æ©Ÿå™¨å­¸ç¿’",
        "æ·±åº¦å­¸ç¿’",
        "è‡ªç„¶èªè¨€è™•ç†",
        "ç¥ç¶“ç¶²è·¯"
    ]
    
    articles = fetch_wikipedia_articles(article_titles)
    print(f"âœ… ç²å–äº† {len(articles)} ç¯‡æ–‡ç« ")
    
    # 2. è™•ç†æ–‡æª”
    print("\n[2/6] è™•ç†æ–‡æª”ä¸¦åˆ†å‰²æˆ chunks...")
    processor = DocumentProcessor(chunk_size=800, chunk_overlap=150)
    documents = process_wikipedia_articles(articles, processor)
    print(f"âœ… ç¸½å…±å‰µå»ºäº† {len(documents)} å€‹æ–‡æª” chunks")
    
    # 3. åˆå§‹åŒ–æª¢ç´¢å™¨
    print("\n[3/6] åˆå§‹åŒ–æª¢ç´¢å™¨...")
    bm25_retriever = BM25Retriever(documents)
    vector_retriever = VectorRetriever(
        documents,
        embedding_model="sentence-transformers/all-MiniLM-L6-v2",
        persist_directory="./chroma_db_everyday"
    )
    hybrid_search = HybridSearch(
        sparse_retriever=bm25_retriever,
        dense_retriever=vector_retriever,
        fusion_method="rrf",
        rrf_k=60
    )
    
    reranker = Reranker(
        model_name="BAAI/bge-reranker-base",
        device=None,
        batch_size=16
    )
    rag_pipeline = RAGPipeline(
        hybrid_search=hybrid_search,
        reranker=reranker,
        recall_k=20,
        adaptive_recall=True
    )
    
    # 4. åˆå§‹åŒ– LLM å’Œæ ¼å¼åŒ–å™¨
    print("\n[4/6] åˆå§‹åŒ– LLM å’Œæ ¼å¼åŒ–å™¨...")
    try:
        llm = OllamaLLM(model_name="llama3.2:3b", timeout=180)
        print(f"âœ… LLM åˆå§‹åŒ–å®Œæˆ: {llm.model_name}")
    except Exception as e:
        print(f"âš ï¸  LLM åˆå§‹åŒ–å¤±æ•—: {e}")
        print("è«‹ç¢ºä¿ Ollama æ­£åœ¨é‹è¡Œä¸¦å·²ä¸‹è¼‰æ¨¡å‹")
        return
    
    formatter = PromptFormatter(
        include_metadata=True,
        format_style="detailed"
    )
    
    # 5. åˆå§‹åŒ–æ‰€æœ‰ RAG æ–¹æ³•
    print("\n[5/6] åˆå§‹åŒ–æ‰€æœ‰ RAG æ–¹æ³•...")
    subquery_rag = SubQueryDecompositionRAG(
        rag_pipeline=rag_pipeline,
        llm=llm,
        max_sub_queries=3,
        top_k_per_subquery=4,
        enable_parallel=True
    )
    
    hyde_rag = HyDERAG(
        rag_pipeline=rag_pipeline,
        vector_retriever=vector_retriever,
        llm=llm,
        hypothetical_length=150,
        temperature=0.7
    )
    
    hybrid_rag = HybridSubqueryHyDERAG(
        rag_pipeline=rag_pipeline,
        vector_retriever=vector_retriever,
        llm=llm,
        max_sub_queries=3,
        top_k_per_subquery=4,
        hypothetical_length=150,
        temperature_subquery=0.3,
        temperature_hyde=0.7,
        enable_parallel=True
    )
    print("âœ… ç³»çµ±åˆå§‹åŒ–å®Œæˆï¼")
    
    # 6. æ¸¬è©¦æŸ¥è©¢ï¼ˆç°¡å–®ã€æ—¥å¸¸çš„å•é¡Œï¼‰
    print("\n[6/6] é–‹å§‹æ¸¬è©¦...")
    print("=" * 80)
    
    # ç°¡å–®ã€ä¸€èˆ¬äººèƒ½ç†è§£çš„å•é¡Œ
    test_queries = [
        "ä»€éº¼æ˜¯äººå·¥æ™ºæ…§ï¼Ÿå®ƒæœ‰å“ªäº›æ‡‰ç”¨ï¼Ÿ",
        "æ©Ÿå™¨å­¸ç¿’å’Œæ·±åº¦å­¸ç¿’æœ‰ä»€éº¼ä¸åŒï¼Ÿ",
        "è‡ªç„¶èªè¨€è™•ç†å¯ä»¥ç”¨ä¾†åšä»€éº¼ï¼Ÿ",
    ]
    
    for query in test_queries:
        print("\n" + "=" * 80)
        print(f"ğŸ“ æ¸¬è©¦å•é¡Œ: '{query}'")
        print("=" * 80)
        
        methods_results = {}
        
        # === æ–¹æ³• 1: æ­£å¸¸ RAG ===
        print("\n" + "ğŸ”µ" * 40)
        print("ã€æ–¹æ³• 1ã€‘æ­£å¸¸ RAG")
        print("ğŸ”µ" * 40)
        try:
            normal_start = time.time()
            normal_docs = vector_retriever.retrieve(query=query, top_k=3)
            normal_retrieval_time = time.time() - normal_start
            
            print(f"\nğŸ“š æª¢ç´¢åˆ°çš„æ–‡æª”ï¼ˆå‰ 3 å€‹ï¼‰:")
            for i, doc in enumerate(normal_docs, 1):
                score = doc.get('score', 0)
                title = doc['metadata'].get('title', 'N/A')
                print(f"\n  {i}. ğŸ“„ {title}")
                print(f"     ç›¸é—œæ€§: {'â­' * int(score * 5)} ({score:.3f})")
                print(f"     å…§å®¹é è¦½: {doc['content'][:150]}...")
            
            # ç”Ÿæˆç­”æ¡ˆ
            normal_context = formatter.format_context(normal_docs, document_type="general")
            normal_prompt = formatter.create_prompt(query, normal_context, document_type="general")
            normal_answer_start = time.time()
            normal_answer = llm.generate(prompt=normal_prompt, temperature=0.7, max_tokens=400)
            normal_answer_time = time.time() - normal_answer_start
            normal_total_time = time.time() - normal_start
            
            print(f"\nğŸ’¬ ç”Ÿæˆçš„ç­”æ¡ˆ:")
            print("-" * 60)
            print(normal_answer)
            print("-" * 60)
            
            normal_quality = evaluate_answer_quality(normal_answer, query)
            normal_avg_score = sum(doc.get('score', 0) for doc in normal_docs) / len(normal_docs) if normal_docs else 0
            
            methods_results['æ­£å¸¸ RAG'] = {
                'docs': normal_docs,
                'count': len(normal_docs),
                'avg_score': normal_avg_score,
                'answer': normal_answer,
                'time': normal_total_time,
                'quality': normal_quality
            }
            
        except Exception as e:
            print(f"âŒ å‡ºéŒ¯: {e}")
            import traceback
            traceback.print_exc()
            methods_results['æ­£å¸¸ RAG'] = {'docs': [], 'count': 0, 'avg_score': 0, 'answer': '', 'time': 0, 'quality': {}}
        
        # === æ–¹æ³• 2: Sub-query RAG ===
        print("\n" + "ğŸŸ¢" * 40)
        print("ã€æ–¹æ³• 2ã€‘Sub-query Decomposition RAG")
        print("ğŸŸ¢" * 40)
        try:
            subquery_result = subquery_rag.query(question=query, top_k=3, return_sub_queries=True)
            
            if subquery_result.get('sub_queries'):
                print(f"\nğŸ” æ‹†è§£çš„å­å•é¡Œ:")
                for i, sq in enumerate(subquery_result['sub_queries'], 1):
                    print(f"   {i}. {sq}")
            
            print(f"\nğŸ“š æª¢ç´¢åˆ°çš„æ–‡æª”ï¼ˆå‰ 3 å€‹ï¼‰:")
            for i, doc in enumerate(subquery_result['results'], 1):
                score = doc.get('rerank_score', doc.get('hybrid_score', doc.get('score', 0)))
                title = doc['metadata'].get('title', 'N/A')
                print(f"\n  {i}. ğŸ“„ {title}")
                print(f"     ç›¸é—œæ€§: {'â­' * int(score * 5)} ({score:.3f})")
                print(f"     å…§å®¹é è¦½: {doc['content'][:150]}...")
            
            # ç”Ÿæˆç­”æ¡ˆ
            subquery_context = formatter.format_context(subquery_result['results'], document_type="general")
            subquery_prompt = formatter.create_prompt(query, subquery_context, document_type="general")
            subquery_answer = llm.generate(prompt=subquery_prompt, temperature=0.7, max_tokens=400)
            
            print(f"\nğŸ’¬ ç”Ÿæˆçš„ç­”æ¡ˆ:")
            print("-" * 60)
            print(subquery_answer)
            print("-" * 60)
            
            subquery_quality = evaluate_answer_quality(subquery_answer, query)
            subquery_avg_score = sum(doc.get('rerank_score', doc.get('hybrid_score', doc.get('score', 0))) 
                                     for doc in subquery_result['results']) / len(subquery_result['results']) if subquery_result['results'] else 0
            
            methods_results['Sub-query RAG'] = {
                'docs': subquery_result['results'],
                'count': subquery_result['total_docs_found'],
                'avg_score': subquery_avg_score,
                'answer': subquery_answer,
                'time': subquery_result['elapsed_time'],
                'quality': subquery_quality
            }
            
        except Exception as e:
            print(f"âŒ å‡ºéŒ¯: {e}")
            import traceback
            traceback.print_exc()
            methods_results['Sub-query RAG'] = {'docs': [], 'count': 0, 'avg_score': 0, 'answer': '', 'time': 0, 'quality': {}}
        
        # === æ–¹æ³• 3: HyDE RAG ===
        print("\n" + "ğŸŸ¡" * 40)
        print("ã€æ–¹æ³• 3ã€‘HyDE RAG")
        print("ğŸŸ¡" * 40)
        try:
            hyde_result = hyde_rag.query(question=query, top_k=3, return_hypothetical=True)
            
            if hyde_result.get('hypothetical_document'):
                print(f"\nğŸ“ ç”Ÿæˆçš„å‡è¨­æ€§æ–‡æª”:")
                print("-" * 60)
                print(hyde_result['hypothetical_document'][:250])
                print("-" * 60)
            
            print(f"\nğŸ“š æª¢ç´¢åˆ°çš„æ–‡æª”ï¼ˆå‰ 3 å€‹ï¼‰:")
            for i, doc in enumerate(hyde_result['results'], 1):
                score = doc.get('score', 0)
                title = doc['metadata'].get('title', 'N/A')
                print(f"\n  {i}. ğŸ“„ {title}")
                print(f"     ç›¸é—œæ€§: {'â­' * int(score * 5)} ({score:.3f})")
                print(f"     å…§å®¹é è¦½: {doc['content'][:150]}...")
            
            # ç”Ÿæˆç­”æ¡ˆ
            hyde_context = formatter.format_context(hyde_result['results'], document_type="general")
            hyde_prompt = formatter.create_prompt(query, hyde_context, document_type="general")
            hyde_answer = llm.generate(prompt=hyde_prompt, temperature=0.7, max_tokens=400)
            
            print(f"\nğŸ’¬ ç”Ÿæˆçš„ç­”æ¡ˆ:")
            print("-" * 60)
            print(hyde_answer)
            print("-" * 60)
            
            hyde_quality = evaluate_answer_quality(hyde_answer, query)
            hyde_avg_score = sum(doc.get('score', 0) for doc in hyde_result['results']) / len(hyde_result['results']) if hyde_result['results'] else 0
            
            methods_results['HyDE RAG'] = {
                'docs': hyde_result['results'],
                'count': hyde_result['total_docs_found'],
                'avg_score': hyde_avg_score,
                'answer': hyde_answer,
                'time': hyde_result['elapsed_time'],
                'quality': hyde_quality
            }
            
        except Exception as e:
            print(f"âŒ å‡ºéŒ¯: {e}")
            import traceback
            traceback.print_exc()
            methods_results['HyDE RAG'] = {'docs': [], 'count': 0, 'avg_score': 0, 'answer': '', 'time': 0, 'quality': {}}
        
        # === æ–¹æ³• 4: Hybrid RAG ===
        print("\n" + "ğŸŸ£" * 40)
        print("ã€æ–¹æ³• 4ã€‘Hybrid (Sub-query + HyDE) RAG")
        print("ğŸŸ£" * 40)
        try:
            hybrid_result = hybrid_rag.query(
                question=query, 
                top_k=3, 
                return_sub_queries=True,
                return_hypothetical=True
            )
            
            if hybrid_result.get('sub_queries'):
                print(f"\nğŸ” æ‹†è§£çš„å­å•é¡Œ:")
                for i, sq in enumerate(hybrid_result['sub_queries'], 1):
                    print(f"   {i}. {sq}")
            
            print(f"\nğŸ“š æª¢ç´¢åˆ°çš„æ–‡æª”ï¼ˆå‰ 3 å€‹ï¼‰:")
            for i, doc in enumerate(hybrid_result['results'], 1):
                score = doc.get('score', 0)
                title = doc['metadata'].get('title', 'N/A')
                print(f"\n  {i}. ğŸ“„ {title}")
                print(f"     ç›¸é—œæ€§: {'â­' * int(score * 5)} ({score:.3f})")
                print(f"     å…§å®¹é è¦½: {doc['content'][:150]}...")
            
            # ç”Ÿæˆç­”æ¡ˆ
            hybrid_context = formatter.format_context(hybrid_result['results'], document_type="general")
            hybrid_prompt = formatter.create_prompt(query, hybrid_context, document_type="general")
            hybrid_answer = llm.generate(prompt=hybrid_prompt, temperature=0.7, max_tokens=400)
            
            print(f"\nğŸ’¬ ç”Ÿæˆçš„ç­”æ¡ˆ:")
            print("-" * 60)
            print(hybrid_answer)
            print("-" * 60)
            
            hybrid_quality = evaluate_answer_quality(hybrid_answer, query)
            hybrid_avg_score = sum(doc.get('score', 0) for doc in hybrid_result['results']) / len(hybrid_result['results']) if hybrid_result['results'] else 0
            
            methods_results['Hybrid RAG'] = {
                'docs': hybrid_result['results'],
                'count': hybrid_result['total_docs_found'],
                'avg_score': hybrid_avg_score,
                'answer': hybrid_answer,
                'time': hybrid_result['elapsed_time'],
                'quality': hybrid_quality
            }
            
        except Exception as e:
            print(f"âŒ å‡ºéŒ¯: {e}")
            import traceback
            traceback.print_exc()
            methods_results['Hybrid RAG'] = {'docs': [], 'count': 0, 'avg_score': 0, 'answer': '', 'time': 0, 'quality': {}}
        
        # === ç¶œåˆå°æ¯”ç¸½çµ ===
        print("\n" + "=" * 80)
        print("ğŸ“Š ç¶œåˆå°æ¯”ç¸½çµ")
        print("=" * 80)
        
        # 1. æ€§èƒ½å°æ¯”è¡¨
        print(f"\nğŸ“ˆ æ€§èƒ½å°æ¯”è¡¨:")
        print(f"{'æ–¹æ³•':<25} {'æ–‡æª”æ•¸':<10} {'å¹³å‡åˆ†æ•¸':<12} {'ç­”æ¡ˆé•·åº¦':<12} {'è€—æ™‚':<10}")
        print("-" * 80)
        for method_name, result in methods_results.items():
            print(f"{method_name:<25} {result['count']:<10} {result['avg_score']:<12.3f} "
                  f"{len(result.get('answer', '')):<12} {result.get('time', 0):<10.2f}s")
        
        # 2. ç­”æ¡ˆè³ªé‡è©•åˆ†
        print(f"\nâ­ ç­”æ¡ˆè³ªé‡è©•åˆ†ï¼ˆæ˜Ÿç´šè¶Šé«˜è¶Šå¥½ï¼‰:")
        for method_name, result in methods_results.items():
            quality = result.get('quality', {})
            if quality:
                overall = quality.get('overall_score', 0)
                stars = "â­" * int(overall * 10)  # 0-10 æ˜Ÿ
                print(f"   {method_name:<25} {stars} ({overall:.2f})")
        
        # 3. é—œéµè©åŒ¹é…åˆ†æ
        query_keywords = set(query.lower().split())
        print(f"\nğŸ”‘ é—œéµè©åŒ¹é…åˆ†æï¼ˆå•é¡Œé—œéµè©: {', '.join(query_keywords)}ï¼‰:")
        for method_name, result in methods_results.items():
            answer = result.get('answer', '')
            if answer:
                answer_lower = answer.lower()
                matched_keywords = [kw for kw in query_keywords if kw in answer_lower]
                match_rate = len(matched_keywords) / len(query_keywords) * 100 if query_keywords else 0
                bars = "â–ˆ" * int(match_rate / 10)  # æ¯ 10% ä¸€å€‹æ–¹å¡Š
                print(f"   {method_name:<25} {bars} {len(matched_keywords)}/{len(query_keywords)} ({match_rate:.0f}%)")
        
        # 4. æ–‡æª”ç›¸é—œæ€§è¦–è¦ºåŒ–
        print(f"\nâ­ æ–‡æª”ç›¸é—œæ€§å°æ¯”ï¼ˆæ˜Ÿç´šè¶Šé«˜è¶Šç›¸é—œï¼‰:")
        for method_name, result in methods_results.items():
            avg_score = result.get('avg_score', 0)
            stars = "â­" * int(avg_score * 10)  # 0-10 æ˜Ÿ
            print(f"   {method_name:<25} {stars} ({avg_score:.3f})")
        
        # 5. æœ€çµ‚å»ºè­°
        print("\n" + "=" * 80)
        print("ğŸ’¡ å¦‚ä½•åˆ¤æ–·å“ªå€‹æœ€å¥½ï¼Ÿ")
        print("=" * 80)
        print("""
    ğŸ“š çœ‹æ–‡æª”ç›¸é—œæ€§ï¼š
       - æª¢æŸ¥æ¯å€‹æ–¹æ³•æ‰¾åˆ°çš„æ–‡æª”æ¨™é¡Œæ˜¯å¦çœŸçš„èˆ‡å•é¡Œç›¸é—œ
       - æŸ¥çœ‹æ–‡æª”å…§å®¹é è¦½ï¼Œçœ‹æ˜¯å¦åŒ…å«å•é¡Œçš„é—œéµè³‡è¨Š
       - æ˜Ÿç´šè¶Šé«˜ï¼Œæ–‡æª”è¶Šç›¸é—œ
    
    ğŸ’¬ çœ‹ç­”æ¡ˆè³ªé‡ï¼š
       - å“ªå€‹ç­”æ¡ˆæ›´æº–ç¢ºåœ°å›ç­”äº†å•é¡Œï¼Ÿ
       - å“ªå€‹ç­”æ¡ˆæ›´è©³ç´°ã€æ›´å®Œæ•´ï¼Ÿ
       - å“ªå€‹ç­”æ¡ˆæ›´å®¹æ˜“ç†è§£ï¼Ÿ
       - é—œéµè©åŒ¹é…ç‡è¶Šé«˜è¶Šå¥½
    
    â±ï¸ çœ‹éŸ¿æ‡‰æ™‚é–“ï¼š
       - å¦‚æœè³ªé‡ç›¸è¿‘ï¼Œé¸æ“‡æ›´å¿«çš„
       - å¦‚æœè³ªé‡å·®ç•°å¤§ï¼Œå„ªå…ˆé¸æ“‡è³ªé‡å¥½çš„
    
    ğŸ† ç¶œåˆå»ºè­°ï¼š
    """)
        
        # æ‰¾å‡ºæœ€ä½³æ–¹æ³•
        best_quality = None
        best_quality_method = None
        best_score = None
        best_score_method = None
        
        for method_name, result in methods_results.items():
            quality = result.get('quality', {})
            if quality:
                overall = quality.get('overall_score', 0)
                if best_quality is None or overall > best_quality:
                    best_quality = overall
                    best_quality_method = method_name
            
            avg_score = result.get('avg_score', 0)
            if best_score is None or avg_score > best_score:
                best_score = avg_score
                best_score_method = method_name
        
        if best_quality_method:
            print(f"   âœ… ç­”æ¡ˆè³ªé‡æœ€ä½³: {best_quality_method} (è³ªé‡åˆ†æ•¸: {best_quality:.2f})")
        if best_score_method:
            print(f"   âœ… æ–‡æª”ç›¸é—œæ€§æœ€ä½³: {best_score_method} (å¹³å‡åˆ†æ•¸: {best_score:.3f})")
        
        print("\n" + "=" * 80)


def main():
    """ä¸»å‡½æ•¸"""
    import argparse
    
    parser = argparse.ArgumentParser(description="æ—¥å¸¸å•é¡Œ RAG å°æ¯”æ¸¬è©¦")
    parser.add_argument(
        "--install-wikipedia",
        action="store_true",
        help="é¡¯ç¤ºå®‰è£ Wikipedia å¥—ä»¶çš„å‘½ä»¤"
    )
    
    args = parser.parse_args()
    
    if args.install_wikipedia:
        print("å®‰è£ Wikipedia å¥—ä»¶:")
        print("  pip install wikipedia")
        print("\næˆ–è€…ä½¿ç”¨ uv:")
        print("  uv pip install wikipedia")
        return
    
    test_everyday_rag_comparison()


if __name__ == "__main__":
    main()

