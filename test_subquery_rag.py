"""
Sub-query Decomposition RAG æ¸¬è©¦è…³æœ¬
ç¤ºç¯„å¦‚ä½•ä½¿ç”¨å­å•é¡Œæ‹†è§£ä¾†æå‡ RAG ç³»çµ±çš„æ•ˆæœ
"""
import os
import sys
import time
import hashlib
from src import (
    DocumentProcessor,
    BM25Retriever,
    VectorRetriever,
    HybridSearch,
    Reranker,
    RAGPipeline,
    PromptFormatter,
    OllamaLLM,
    SubQueryDecompositionRAG
)


def test_subquery_rag_with_papers():
    """ä½¿ç”¨ arXiv è«–æ–‡æ¸¬è©¦ Sub-query Decomposition RAG"""
    print("=" * 60)
    print("Sub-query Decomposition RAG æ¸¬è©¦")
    print("=" * 60)
    
    # 1. åˆå§‹åŒ–æ–‡æª”è™•ç†å™¨
    print("\n[1/7] åˆå§‹åŒ–æ–‡æª”è™•ç†å™¨...")
    processor = DocumentProcessor(chunk_size=1000, chunk_overlap=200)
    
    # 2. ç²å–è«–æ–‡
    print("\n[2/7] å¾ arXiv ç²å–è«–æ–‡...")
    papers = processor.fetch_papers(
        query="cat:cs.AI OR cat:cs.LG OR cat:cs.CL",
        max_results=20
    )
    print(f"âœ… ç²å–äº† {len(papers)} ç¯‡è«–æ–‡")
    
    # 3. è™•ç†æ–‡æª”
    print("\n[3/7] è™•ç†æ–‡æª”ä¸¦åˆ†å‰²æˆ chunks...")
    documents = processor.process_documents(papers)
    print(f"âœ… ç¸½å…±å‰µå»ºäº† {len(documents)} å€‹æ–‡æª” chunks")
    
    # 4. åˆå§‹åŒ–æª¢ç´¢å™¨
    print("\n[4/7] åˆå§‹åŒ–æª¢ç´¢å™¨...")
    bm25_retriever = BM25Retriever(documents)
    vector_retriever = VectorRetriever(
        documents,
        embedding_model="sentence-transformers/all-MiniLM-L6-v2",
        persist_directory="./chroma_db_subquery"
    )
    hybrid_search = HybridSearch(
        sparse_retriever=bm25_retriever,
        dense_retriever=vector_retriever,
        fusion_method="rrf",
        rrf_k=60
    )
    
    # 5. åˆå§‹åŒ– RAG ç®¡ç·š
    print("\n[5/7] åˆå§‹åŒ– RAG ç®¡ç·š...")
    reranker = Reranker(
        model_name="BAAI/bge-reranker-base",
        device=None,
        batch_size=16
    )
    rag_pipeline = RAGPipeline(
        hybrid_search=hybrid_search,
        reranker=reranker,
        recall_k=25,
        adaptive_recall=True
    )
    
    # 6. åˆå§‹åŒ– LLM å’Œæ ¼å¼åŒ–å™¨
    print("\n[6/7] åˆå§‹åŒ– LLM å’Œæ ¼å¼åŒ–å™¨...")
    try:
        llm = OllamaLLM(model_name="llama3.2:3b", timeout=180)
        print(f"âœ… LLM åˆå§‹åŒ–å®Œæˆ: {llm.model_name}")
    except Exception as e:
        print(f"âš ï¸  LLM åˆå§‹åŒ–å¤±æ•—: {e}")
        print("è«‹ç¢ºä¿ Ollama æ­£åœ¨é‹è¡Œä¸¦å·²ä¸‹è¼‰æ¨¡å‹")
        return
    
    formatter = PromptFormatter(
        include_metadata=True,
        format_style="detailed"
    )
    
    # 7. åˆå§‹åŒ– Sub-query Decomposition RAG
    print("\n[7/7] åˆå§‹åŒ– Sub-query Decomposition RAG...")
    subquery_rag = SubQueryDecompositionRAG(
        rag_pipeline=rag_pipeline,
        llm=llm,
        max_sub_queries=3,
        top_k_per_subquery=5,
        enable_parallel=True
    )
    print("âœ… ç³»çµ±åˆå§‹åŒ–å®Œæˆï¼")
    
    # æ¸¬è©¦æŸ¥è©¢
    print("\n" + "=" * 60)
    print("é–‹å§‹æ¸¬è©¦")
    print("=" * 60)
    
    test_queries = [
        "transformer architecture and attention mechanism",
        "æ¯”è¼ƒæ·±åº¦å­¸ç¿’å’Œæ©Ÿå™¨å­¸ç¿’çš„å·®ç•°",
        "How do neural networks learn and optimize?",
    ]
    
    for query in test_queries:
        print("\n" + "-" * 60)
        print(f"æ¸¬è©¦æŸ¥è©¢: '{query}'")
        print("-" * 60)
        
        try:
            # ä½¿ç”¨ Sub-query Decomposition RAG
            result = subquery_rag.generate_answer(
                question=query,
                formatter=formatter,
                top_k=5,
                document_type="paper",
                return_sub_queries=True
            )
            
            # é¡¯ç¤ºçµæœ
            print(f"\nğŸ“Š æŸ¥è©¢çµ±è¨ˆ:")
            print(f"   ç¸½è€—æ™‚: {result['total_time']:.2f}s")
            print(f"   æª¢ç´¢è€—æ™‚: {result['elapsed_time']:.2f}s")
            print(f"   ç”Ÿæˆè€—æ™‚: {result.get('answer_time', 0):.2f}s")
            print(f"   æ‰¾åˆ°æ–‡æª”æ•¸: {result['total_docs_found']}")
            
            if result.get('sub_queries'):
                print(f"\nğŸ” ç”Ÿæˆçš„å­å•é¡Œ ({len(result['sub_queries'])} å€‹):")
                for i, sq in enumerate(result['sub_queries'], 1):
                    print(f"   {i}. {sq}")
            
            print(f"\nğŸ“š æª¢ç´¢åˆ°çš„æ–‡æª” (å‰ 3 å€‹):")
            for i, doc in enumerate(result['results'][:3], 1):
                print(f"\n   {i}. {doc['metadata'].get('title', 'N/A')}")
                print(f"      arXiv ID: {doc['metadata'].get('arxiv_id', 'N/A')}")
                score = doc.get('rerank_score', doc.get('hybrid_score', 0))
                print(f"      ç›¸é—œæ€§åˆ†æ•¸: {score:.4f}")
                print(f"      å…§å®¹é è¦½: {doc['content'][:100]}...")
            
            print(f"\nğŸ¤– ç”Ÿæˆçš„å›ç­”:")
            print("-" * 40)
            print(result['answer'])
            print("-" * 40)
            
        except Exception as e:
            print(f"âŒ æŸ¥è©¢è™•ç†å‡ºéŒ¯: {e}")
            import traceback
            traceback.print_exc()
            continue


def test_subquery_vs_normal_rag():
    """å°æ¯”æ¸¬è©¦ï¼šSub-query Decomposition vs æ­£å¸¸ RAG"""
    print("\n" + "=" * 60)
    print("å°æ¯”æ¸¬è©¦ï¼šSub-query Decomposition vs æ­£å¸¸ RAG")
    print("=" * 60)
    
    # 1. åˆå§‹åŒ–ç³»çµ±ï¼ˆèˆ‡ test_subquery_rag_with_papers ç›¸åŒï¼‰
    print("\n[åˆå§‹åŒ–ç³»çµ±]")
    processor = DocumentProcessor(chunk_size=1000, chunk_overlap=200)
    
    print("å¾ arXiv ç²å–è«–æ–‡...")
    papers = processor.fetch_papers(
        query="cat:cs.AI OR cat:cs.LG OR cat:cs.CL",
        max_results=20
    )
    print(f"âœ… ç²å–äº† {len(papers)} ç¯‡è«–æ–‡")
    
    documents = processor.process_documents(papers)
    print(f"âœ… ç¸½å…±å‰µå»ºäº† {len(documents)} å€‹æ–‡æª” chunks")
    
    # åˆå§‹åŒ–æª¢ç´¢å™¨
    print("åˆå§‹åŒ–æª¢ç´¢å™¨...")
    bm25_retriever = BM25Retriever(documents)
    vector_retriever = VectorRetriever(
        documents,
        embedding_model="sentence-transformers/all-MiniLM-L6-v2",
        persist_directory="./chroma_db_compare"
    )
    hybrid_search = HybridSearch(
        sparse_retriever=bm25_retriever,
        dense_retriever=vector_retriever,
        fusion_method="rrf",
        rrf_k=60
    )
    
    reranker = Reranker(
        model_name="BAAI/bge-reranker-base",
        device=None,
        batch_size=16
    )
    rag_pipeline = RAGPipeline(
        hybrid_search=hybrid_search,
        reranker=reranker,
        recall_k=25,
        adaptive_recall=True
    )
    
    # åˆå§‹åŒ– LLM å’Œæ ¼å¼åŒ–å™¨
    print("åˆå§‹åŒ– LLM å’Œæ ¼å¼åŒ–å™¨...")
    try:
        llm = OllamaLLM(model_name="llama3.2:3b", timeout=180)
        print(f"âœ… LLM åˆå§‹åŒ–å®Œæˆ: {llm.model_name}")
    except Exception as e:
        print(f"âš ï¸  LLM åˆå§‹åŒ–å¤±æ•—: {e}")
        print("è«‹ç¢ºä¿ Ollama æ­£åœ¨é‹è¡Œä¸¦å·²ä¸‹è¼‰æ¨¡å‹")
        return
    
    formatter = PromptFormatter(
        include_metadata=True,
        format_style="detailed"
    )
    
    # åˆå§‹åŒ– Sub-query Decomposition RAG
    print("åˆå§‹åŒ– Sub-query Decomposition RAG...")
    subquery_rag = SubQueryDecompositionRAG(
        rag_pipeline=rag_pipeline,
        llm=llm,
        max_sub_queries=3,
        top_k_per_subquery=5,
        enable_parallel=True
    )
    print("âœ… ç³»çµ±åˆå§‹åŒ–å®Œæˆï¼")
    
    # 2. æ¸¬è©¦æŸ¥è©¢ï¼ˆé¸æ“‡è¤‡é›œçš„æŸ¥è©¢ä»¥çªå‡ºå·®ç•°ï¼‰
    test_queries = [
        "transformer architecture, attention mechanism, and optimization techniques",
        "æ¯”è¼ƒæ·±åº¦å­¸ç¿’å’Œæ©Ÿå™¨å­¸ç¿’çš„å·®ç•°ã€å„ªç¼ºé»å’Œæ‡‰ç”¨å ´æ™¯",
        "How do neural networks learn, optimize, and generalize?",
    ]
    
    print("\n" + "=" * 60)
    print("é–‹å§‹å°æ¯”æ¸¬è©¦")
    print("=" * 60)
    
    for query in test_queries:
        print("\n" + "=" * 60)
        print(f"æ¸¬è©¦æŸ¥è©¢: '{query}'")
        print("=" * 60)
        
        # === æ–¹æ³• 1: æ­£å¸¸ RAG ===
        print("\n[æ–¹æ³• 1] æ­£å¸¸ RAG")
        print("-" * 60)
        
        normal_start = time.time()
        try:
            # æ­£å¸¸ RAGï¼šç›´æ¥ä½¿ç”¨ RAGPipeline
            normal_results = rag_pipeline.query(
                text=query,
                top_k=5,
                enable_rerank=True
            )
            
            # æ ¼å¼åŒ–ä¸¦ç”Ÿæˆç­”æ¡ˆ
            normal_context = formatter.format_context(
                normal_results,
                document_type="paper"
            )
            normal_prompt = formatter.create_prompt(
                query,
                normal_context,
                document_type="paper"
            )
            
            normal_answer_start = time.time()
            normal_answer = llm.generate(
                prompt=normal_prompt,
                temperature=0.7,
                max_tokens=2048
            )
            normal_answer_time = time.time() - normal_answer_start
            normal_total_time = time.time() - normal_start
            
            print(f"âœ… æ­£å¸¸ RAG å®Œæˆ")
            print(f"   æª¢ç´¢è€—æ™‚: {normal_total_time - normal_answer_time:.2f}s")
            print(f"   ç”Ÿæˆè€—æ™‚: {normal_answer_time:.2f}s")
            print(f"   ç¸½è€—æ™‚: {normal_total_time:.2f}s")
            print(f"   æ‰¾åˆ°æ–‡æª”æ•¸: {len(normal_results)}")
            
            # é¡¯ç¤ºæª¢ç´¢çµæœ
            print(f"\n   æª¢ç´¢åˆ°çš„æ–‡æª” (å‰ 3 å€‹):")
            for i, doc in enumerate(normal_results[:3], 1):
                score = doc.get('rerank_score', doc.get('hybrid_score', 0))
                title = doc['metadata'].get('title', 'N/A')
                if len(title) > 50:
                    title = title[:47] + "..."
                print(f"   {i}. {title} (åˆ†æ•¸: {score:.4f})")
            
        except Exception as e:
            print(f"âŒ æ­£å¸¸ RAG å‡ºéŒ¯: {e}")
            normal_answer = None
            normal_total_time = 0
            normal_results = []
            import traceback
            traceback.print_exc()
        
        # === æ–¹æ³• 2: Sub-query Decomposition RAG ===
        print("\n[æ–¹æ³• 2] Sub-query Decomposition RAG")
        print("-" * 60)
        
        try:
            subquery_result = subquery_rag.generate_answer(
                question=query,
                formatter=formatter,
                top_k=5,
                document_type="paper",
                return_sub_queries=True
            )
            
            print(f"âœ… Sub-query RAG å®Œæˆ")
            print(f"   æª¢ç´¢è€—æ™‚: {subquery_result['elapsed_time']:.2f}s")
            print(f"   ç”Ÿæˆè€—æ™‚: {subquery_result.get('answer_time', 0):.2f}s")
            print(f"   ç¸½è€—æ™‚: {subquery_result['total_time']:.2f}s")
            print(f"   æ‰¾åˆ°æ–‡æª”æ•¸: {subquery_result['total_docs_found']}")
            
            if subquery_result.get('sub_queries'):
                print(f"\n   ç”Ÿæˆçš„å­å•é¡Œ ({len(subquery_result['sub_queries'])} å€‹):")
                for i, sq in enumerate(subquery_result['sub_queries'], 1):
                    print(f"   {i}. {sq}")
            
            # é¡¯ç¤ºæª¢ç´¢çµæœ
            print(f"\n   æª¢ç´¢åˆ°çš„æ–‡æª” (å‰ 3 å€‹):")
            for i, doc in enumerate(subquery_result['results'][:3], 1):
                score = doc.get('rerank_score', doc.get('hybrid_score', 0))
                title = doc['metadata'].get('title', 'N/A')
                if len(title) > 50:
                    title = title[:47] + "..."
                print(f"   {i}. {title} (åˆ†æ•¸: {score:.4f})")
            
        except Exception as e:
            print(f"âŒ Sub-query RAG å‡ºéŒ¯: {e}")
            subquery_result = None
            import traceback
            traceback.print_exc()
        
        # === å°æ¯”ç¸½çµ ===
        print("\n" + "-" * 60)
        print("[å°æ¯”ç¸½çµ]")
        print("-" * 60)
        
        if normal_answer and subquery_result:
            print(f"\nğŸ“Š æ€§èƒ½å°æ¯”:")
            print(f"   æ­£å¸¸ RAG ç¸½è€—æ™‚: {normal_total_time:.2f}s")
            print(f"   Sub-query RAG ç¸½è€—æ™‚: {subquery_result['total_time']:.2f}s")
            time_diff = abs(normal_total_time - subquery_result['total_time'])
            time_ratio = (subquery_result['total_time'] / normal_total_time * 100) if normal_total_time > 0 else 0
            print(f"   æ™‚é–“å·®ç•°: {time_diff:.2f}s (Sub-query æ˜¯æ­£å¸¸ RAG çš„ {time_ratio:.1f}%)")
            
            print(f"\nğŸ“š æª¢ç´¢çµæœå°æ¯”:")
            print(f"   æ­£å¸¸ RAG æ–‡æª”æ•¸: {len(normal_results)}")
            print(f"   Sub-query RAG æ–‡æª”æ•¸: {subquery_result['total_docs_found']}")
            
            # æª¢æŸ¥æ–‡æª”é‡ç–Šåº¦
            normal_doc_ids = set()
            for doc in normal_results:
                metadata = doc.get('metadata', {})
                if 'arxiv_id' in metadata and 'chunk_index' in metadata:
                    normal_doc_ids.add(f"{metadata['arxiv_id']}_{metadata['chunk_index']}")
                else:
                    # å›é€€åˆ°å…§å®¹ hash
                    content = doc.get('content', '')
                    content_hash = hashlib.md5(content.encode()).hexdigest()[:16]
                    normal_doc_ids.add(f"doc_{content_hash}")
            
            subquery_doc_ids = set()
            for doc in subquery_result['results']:
                metadata = doc.get('metadata', {})
                if 'arxiv_id' in metadata and 'chunk_index' in metadata:
                    subquery_doc_ids.add(f"{metadata['arxiv_id']}_{metadata['chunk_index']}")
                else:
                    # å›é€€åˆ°å…§å®¹ hash
                    content = doc.get('content', '')
                    content_hash = hashlib.md5(content.encode()).hexdigest()[:16]
                    subquery_doc_ids.add(f"doc_{content_hash}")
            
            overlap = len(normal_doc_ids & subquery_doc_ids)
            total_unique = len(normal_doc_ids | subquery_doc_ids)
            overlap_ratio = overlap / total_unique if total_unique > 0 else 0
            
            print(f"   æ–‡æª”é‡ç–Šæ•¸: {overlap}/{total_unique}")
            print(f"   æ–‡æª”é‡ç–Šç‡: {overlap_ratio:.2%}")
            
            print(f"\nğŸ’¡ è§€å¯Ÿ:")
            if subquery_result['total_docs_found'] > len(normal_results):
                print("   âœ“ Sub-query RAG æ‰¾åˆ°äº†æ›´å¤šæ–‡æª”ï¼ˆå¯èƒ½è¦†è“‹æ›´å…¨é¢ï¼‰")
            elif subquery_result['total_docs_found'] < len(normal_results):
                print("   - æ­£å¸¸ RAG æ‰¾åˆ°äº†æ›´å¤šæ–‡æª”")
            else:
                print("   = å…©ç¨®æ–¹æ³•æ‰¾åˆ°çš„æ–‡æª”æ•¸é‡ç›¸åŒ")
            
            if overlap_ratio < 0.5:
                print("   âœ“ å…©ç¨®æ–¹æ³•æ‰¾åˆ°çš„æ–‡æª”å·®ç•°è¼ƒå¤§ï¼ˆSub-query å¯èƒ½å¾ä¸åŒè§’åº¦æª¢ç´¢ï¼‰")
            elif overlap_ratio > 0.8:
                print("   = å…©ç¨®æ–¹æ³•æ‰¾åˆ°çš„æ–‡æª”é«˜åº¦é‡ç–Š")
            else:
                print("   ~ å…©ç¨®æ–¹æ³•æ‰¾åˆ°çš„æ–‡æª”æœ‰éƒ¨åˆ†é‡ç–Š")
            
            if subquery_result['total_time'] > normal_total_time * 1.5:
                print(f"   âš  Sub-query RAG è€—æ™‚è¼ƒé•·ï¼ˆå› ç‚ºéœ€è¦ç”Ÿæˆ {len(subquery_result.get('sub_queries', []))} å€‹å­å•é¡Œï¼‰")
            elif subquery_result['total_time'] < normal_total_time:
                print("   âœ“ Sub-query RAG è€—æ™‚æ›´çŸ­ï¼ˆå¯èƒ½å› ç‚ºä¸¦è¡Œè™•ç†ï¼‰")
            else:
                print("   = å…©ç¨®æ–¹æ³•è€—æ™‚ç›¸è¿‘")
            
            # é¡¯ç¤ºç­”æ¡ˆé•·åº¦å°æ¯”ï¼ˆç°¡å–®çš„è³ªé‡æŒ‡æ¨™ï¼‰
            normal_answer_len = len(normal_answer) if normal_answer else 0
            subquery_answer_len = len(subquery_result.get('answer', ''))
            print(f"\nğŸ“ ç­”æ¡ˆé•·åº¦å°æ¯”:")
            print(f"   æ­£å¸¸ RAG ç­”æ¡ˆé•·åº¦: {normal_answer_len} å­—ç¬¦")
            print(f"   Sub-query RAG ç­”æ¡ˆé•·åº¦: {subquery_answer_len} å­—ç¬¦")
        
        print("\n" + "=" * 60)


def main():
    """ä¸»å‡½æ•¸"""
    import argparse
    
    parser = argparse.ArgumentParser(description="æ¸¬è©¦ Sub-query Decomposition RAG")
    parser.add_argument(
        "--compare",
        action="store_true",
        help="åŸ·è¡Œå°æ¯”æ¸¬è©¦ï¼ˆSub-query vs æ­£å¸¸ RAGï¼‰"
    )
    
    args = parser.parse_args()
    
    if args.compare:
        test_subquery_vs_normal_rag()
    else:
        test_subquery_rag_with_papers()


if __name__ == "__main__":
    main()

