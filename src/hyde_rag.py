"""
HyDE (Hypothetical Document Embeddings) RAGï¼šä½¿ç”¨å‡è¨­æ€§æ–‡æª”æ”¹å–„æª¢ç´¢
"""
from typing import List, Dict, Optional
from .retrievers.reranker import RAGPipeline
from .retrievers.vector_retriever import VectorRetriever
from .prompt_formatter import PromptFormatter
from .llm_integration import OllamaLLM
import time
import logging

logger = logging.getLogger(__name__)


class HyDERAG:
    """ä½¿ç”¨ HyDE (Hypothetical Document Embeddings) çš„ RAG ç³»çµ±"""
    
    def __init__(
        self,
        rag_pipeline: RAGPipeline,
        vector_retriever: VectorRetriever,
        llm: OllamaLLM,
        hypothetical_length: int = 200,
        temperature: float = 0.7
    ):
        """
        åˆå§‹åŒ– HyDE RAG
        
        Args:
            rag_pipeline: RAG ç®¡ç·šå¯¦ä¾‹ï¼ˆç”¨æ–¼æœ€çµ‚ç­”æ¡ˆç”Ÿæˆï¼‰
            vector_retriever: å‘é‡æª¢ç´¢å™¨ï¼ˆç”¨æ–¼åŸºæ–¼å‡è¨­æ€§æ–‡æª”çš„æª¢ç´¢ï¼‰
            llm: LLM å¯¦ä¾‹ï¼ˆç”¨æ–¼ç”Ÿæˆå‡è¨­æ€§æ–‡æª”ï¼‰
            hypothetical_length: å‡è¨­æ€§æ–‡æª”çš„ç›®æ¨™é•·åº¦ï¼ˆå­—ç¬¦æ•¸ï¼‰
            temperature: ç”Ÿæˆå‡è¨­æ€§æ–‡æª”æ™‚çš„æº«åº¦åƒæ•¸ï¼ˆå»ºè­° 0.7ï¼Œä»¥ç²å¾—æ›´å¤šå°ˆæ¥­è¡“èªï¼‰
        """
        self.rag_pipeline = rag_pipeline
        self.vector_retriever = vector_retriever
        self.llm = llm
        self.hypothetical_length = hypothetical_length
        self.temperature = temperature
    
    def _generate_hypothetical_document(self, question: str) -> str:
        """
        ç”Ÿæˆå‡è¨­æ€§æ–‡æª”ï¼ˆHypothetical Documentï¼‰
        
        Args:
            question: ç”¨æˆ¶å•é¡Œ
            
        Returns:
            å‡è¨­æ€§æ–‡æª”æ–‡æœ¬
        """
        # æª¢æ¸¬èªè¨€
        is_chinese = PromptFormatter.detect_language(question) == "zh"
        
        if is_chinese:
            prompt = f"""è«‹é‡å°ä»¥ä¸‹å•é¡Œï¼Œå¯«å‡ºä¸€æ®µç´„ {self.hypothetical_length} å­—çš„å°ˆæ¥­æŠ€è¡“æ–‡ä»¶å…§å®¹ã€‚
é€™æ®µå…§å®¹æ‡‰åŒ…å«è©²é ˜åŸŸå¸¸è¦‹çš„å°ˆæ¥­è¡“èªèˆ‡åŸç†èªªæ˜ï¼Œä»¥ä¾¿ç”¨æ–¼å¾ŒçºŒçš„èªç¾©æª¢ç´¢ã€‚
è«‹ä½¿ç”¨å°ˆæ¥­çš„è¡“èªå’Œæ¦‚å¿µï¼Œå³ä½¿ä½ å°æŸäº›ç´°ç¯€ä¸ç¢ºå®šï¼Œä¹Ÿè¦åŒ…å«ç›¸é—œçš„å°ˆæ¥­è©å½™ã€‚

å•é¡Œ: {question}

å°ˆæ¥­æŠ€è¡“å…§å®¹ï¼š"""
        else:
            prompt = f"""Please write a professional technical document of approximately {self.hypothetical_length} words in response to the following question.
This content should include common professional terminology and principle explanations in this field, to be used for subsequent semantic retrieval.
Please use professional terms and concepts, and include relevant professional vocabulary even if you are uncertain about some details.

Question: {question}

Professional technical content:"""
        
        try:
            hypothetical_doc = self.llm.generate(
                prompt=prompt,
                temperature=self.temperature,  # è¼ƒé«˜çš„æº«åº¦ä»¥ç²å¾—æ›´å¤šå°ˆæ¥­è¡“èª
                max_tokens=500
            )
            
            # æ¸…ç†è¼¸å‡º
            hypothetical_doc = hypothetical_doc.strip()
            
            if not hypothetical_doc:
                logger.warning("âš ï¸  ç”Ÿæˆçš„å‡è¨­æ€§æ–‡æª”ç‚ºç©ºï¼Œä½¿ç”¨åŸå§‹å•é¡Œ")
                return question
            
            logger.info(f"âœ… ç”Ÿæˆå‡è¨­æ€§æ–‡æª”ï¼ˆé•·åº¦: {len(hypothetical_doc)} å­—ç¬¦ï¼‰")
            return hypothetical_doc
            
        except Exception as e:
            logger.error(f"âš ï¸  ç”Ÿæˆå‡è¨­æ€§æ–‡æª”æ™‚å‡ºéŒ¯: {e}")
            # å›é€€åˆ°ä½¿ç”¨åŸå§‹å•é¡Œ
            return question
    
    def query(
        self,
        question: str,
        top_k: int = 5,
        metadata_filter: Optional[Dict] = None,
        return_hypothetical: bool = False
    ) -> Dict:
        """
        åŸ·è¡Œ HyDE æª¢ç´¢ï¼ˆä¸ç”Ÿæˆç­”æ¡ˆï¼‰
        
        Args:
            question: åŸå§‹å•é¡Œ
            top_k: è¿”å›å‰ k å€‹çµæœ
            metadata_filter: å¯é¸çš„ metadata éæ¿¾æ¢ä»¶
            return_hypothetical: æ˜¯å¦åœ¨çµæœä¸­åŒ…å«å‡è¨­æ€§æ–‡æª”
            
        Returns:
            åŒ…å«æª¢ç´¢çµæœå’Œçµ±è¨ˆä¿¡æ¯çš„å­—å…¸
        """
        start_time = time.time()
        
        # ç¬¬ä¸€æ­¥ï¼šç”Ÿæˆå‡è¨­æ€§æ–‡æª”
        logger.info(f"ğŸ” ç”Ÿæˆå‡è¨­æ€§æ–‡æª”: '{question}'")
        hypothetical_doc = self._generate_hypothetical_document(question)
        
        # ç¬¬äºŒæ­¥ï¼šä½¿ç”¨å‡è¨­æ€§æ–‡æª”é€²è¡Œæª¢ç´¢
        logger.info(f"ğŸ“š ä½¿ç”¨å‡è¨­æ€§æ–‡æª”é€²è¡Œæª¢ç´¢...")
        results = self.vector_retriever.retrieve(
            query=hypothetical_doc,  # ä½¿ç”¨å‡è¨­æ€§æ–‡æª”è€Œä¸æ˜¯åŸå§‹å•é¡Œ
            top_k=top_k,
            metadata_filter=metadata_filter
        )
        
        elapsed_time = time.time() - start_time
        logger.info(f"âœ… æ‰¾åˆ° {len(results)} å€‹çµæœï¼ˆè€—æ™‚: {elapsed_time:.2f}sï¼‰")
        
        result = {
            "results": results,
            "total_docs_found": len(results),
            "hypothetical_document": hypothetical_doc if return_hypothetical else None,
            "elapsed_time": elapsed_time
        }
        
        return result
    
    def generate_answer(
        self,
        question: str,
        formatter: PromptFormatter,
        top_k: int = 5,
        metadata_filter: Optional[Dict] = None,
        document_type: str = "general",
        return_hypothetical: bool = False
    ) -> Dict:
        """
        å®Œæ•´çš„ HyDE RAG æµç¨‹ï¼šç”Ÿæˆå‡è¨­æ€§æ–‡æª” -> æª¢ç´¢ -> ç”Ÿæˆç­”æ¡ˆ
        
        Args:
            question: åŸå§‹å•é¡Œ
            formatter: Prompt æ ¼å¼åŒ–å™¨
            top_k: ç”¨æ–¼ç”Ÿæˆç­”æ¡ˆçš„æ–‡æª”æ•¸é‡
            metadata_filter: å¯é¸çš„ metadata éæ¿¾æ¢ä»¶
            document_type: æ–‡æª”é¡å‹ ("paper", "cv", "general")
            return_hypothetical: æ˜¯å¦åœ¨çµæœä¸­åŒ…å«å‡è¨­æ€§æ–‡æª”
            
        Returns:
            åŒ…å«æª¢ç´¢çµæœã€ç”Ÿæˆçš„ç­”æ¡ˆå’Œçµ±è¨ˆä¿¡æ¯çš„å­—å…¸
        """
        start_time = time.time()
        
        # ç¬¬ä¸€æ­¥ï¼šç”Ÿæˆå‡è¨­æ€§æ–‡æª”
        logger.info(f"ğŸ” ç”Ÿæˆå‡è¨­æ€§æ–‡æª”: '{question}'")
        hypothetical_start = time.time()
        hypothetical_doc = self._generate_hypothetical_document(question)
        hypothetical_time = time.time() - hypothetical_start
        
        # ç¬¬äºŒæ­¥ï¼šä½¿ç”¨å‡è¨­æ€§æ–‡æª”é€²è¡Œæª¢ç´¢
        logger.info(f"ğŸ“š ä½¿ç”¨å‡è¨­æ€§æ–‡æª”é€²è¡Œæª¢ç´¢...")
        retrieval_start = time.time()
        results = self.vector_retriever.retrieve(
            query=hypothetical_doc,  # ä½¿ç”¨å‡è¨­æ€§æ–‡æª”è€Œä¸æ˜¯åŸå§‹å•é¡Œ
            top_k=top_k,
            metadata_filter=metadata_filter
        )
        retrieval_time = time.time() - retrieval_start
        
        if not results:
            return {
                "results": [],
                "total_docs_found": 0,
                "hypothetical_document": hypothetical_doc if return_hypothetical else None,
                "elapsed_time": retrieval_time + hypothetical_time,
                "answer": "æŠ±æ­‰ï¼Œæœªæ‰¾åˆ°ç›¸é—œæ–‡æª”ä¾†å›ç­”æ­¤å•é¡Œã€‚",
                "formatted_context": None,
                "answer_time": 0.0,
                "total_time": retrieval_time + hypothetical_time
            }
        
        # ç¬¬ä¸‰æ­¥ï¼šæ ¼å¼åŒ–ä¸Šä¸‹æ–‡
        formatted_context = formatter.format_context(
            results,
            document_type=document_type
        )
        
        # ç¬¬å››æ­¥ï¼šå‰µå»º promptï¼ˆä½¿ç”¨åŸå§‹å•é¡Œï¼Œè€Œä¸æ˜¯å‡è¨­æ€§æ–‡æª”ï¼‰
        prompt = formatter.create_prompt(
            question,  # ä½¿ç”¨åŸå§‹å•é¡Œç”Ÿæˆç­”æ¡ˆ
            formatted_context,
            document_type=document_type
        )
        
        # ç¬¬äº”æ­¥ï¼šç”Ÿæˆå›ç­”
        logger.info("ğŸ¤– ç”Ÿæˆå›ç­”ä¸­...")
        answer_start = time.time()
        try:
            answer = self.llm.generate(
                prompt=prompt,
                temperature=0.7,
                max_tokens=2048
            )
            answer_time = time.time() - answer_start
            logger.info(f"âœ… å›ç­”ç”Ÿæˆå®Œæˆï¼ˆè€—æ™‚: {answer_time:.2f}sï¼‰")
        except Exception as e:
            logger.error(f"âŒ ç”Ÿæˆå›ç­”æ™‚å‡ºéŒ¯: {e}")
            answer = f"ç”Ÿæˆå›ç­”æ™‚å‡ºéŒ¯: {e}"
            answer_time = time.time() - answer_start
        
        total_time = time.time() - start_time
        
        return {
            "results": results,
            "total_docs_found": len(results),
            "hypothetical_document": hypothetical_doc if return_hypothetical else None,
            "elapsed_time": retrieval_time + hypothetical_time,
            "hypothetical_time": hypothetical_time,
            "retrieval_time": retrieval_time,
            "answer": answer,
            "formatted_context": formatted_context,
            "answer_time": answer_time,
            "total_time": total_time
        }

