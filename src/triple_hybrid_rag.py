"""
Triple Hybrid RAGï¼šèåˆ SubQuery + HyDE + Step-back Prompting
çµåˆä¸‰ç¨®æŠ€è¡“çš„å„ªå‹¢ï¼Œå¯¦ç¾æœ€å¼·å¤§çš„ RAG ç³»çµ±
"""
from typing import List, Dict, Optional
from .retrievers.reranker import RAGPipeline
from .retrievers.vector_retriever import VectorRetriever
from .prompt_formatter import PromptFormatter
from .llm_integration import OllamaLLM
import hashlib
import time
import logging
from concurrent.futures import ThreadPoolExecutor, as_completed

logger = logging.getLogger(__name__)


class TripleHybridRAG:
    """èåˆ SubQuery + HyDE + Step-back çš„ä¸‰é‡æ··åˆ RAG ç³»çµ±"""
    
    def __init__(
        self,
        rag_pipeline: RAGPipeline,
        vector_retriever: VectorRetriever,
        llm: OllamaLLM,
        max_sub_queries: int = 3,
        top_k_per_subquery: int = 5,
        hypothetical_length: int = 200,
        temperature_subquery: float = 0.3,
        temperature_hyde: float = 0.7,
        temperature_stepback: float = 0.3,
        answer_temperature: float = 0.7,
        enable_parallel: bool = True
    ):
        """
        åˆå§‹åŒ–ä¸‰é‡æ··åˆ RAG
        
        Args:
            rag_pipeline: RAG ç®¡ç·šå¯¦ä¾‹
            vector_retriever: å‘é‡æª¢ç´¢å™¨
            llm: LLM å¯¦ä¾‹
            max_sub_queries: æœ€å¤šç”Ÿæˆçš„å­å•é¡Œæ•¸é‡
            top_k_per_subquery: æ¯å€‹å­å•é¡Œæª¢ç´¢çš„çµæœæ•¸é‡
            hypothetical_length: å‡è¨­æ€§æ–‡æª”ç›®æ¨™é•·åº¦ï¼ˆå­—ç¬¦æ•¸ï¼‰
            temperature_subquery: ç”Ÿæˆå­å•é¡Œçš„æº«åº¦ï¼ˆè¼ƒä½ï¼Œæ›´ç©©å®šï¼‰
            temperature_hyde: ç”Ÿæˆå‡è¨­æ€§æ–‡æª”çš„æº«åº¦ï¼ˆè¼ƒé«˜ï¼Œæ›´å¤šå°ˆæ¥­è¡“èªï¼‰
            temperature_stepback: ç”ŸæˆæŠ½è±¡å•é¡Œçš„æº«åº¦ï¼ˆè¼ƒä½ï¼Œæ›´ç©©å®šï¼‰
            answer_temperature: ç”Ÿæˆç­”æ¡ˆçš„æº«åº¦
            enable_parallel: æ˜¯å¦ä¸¦è¡Œè™•ç†
        """
        self.rag_pipeline = rag_pipeline
        self.vector_retriever = vector_retriever
        self.llm = llm
        self.max_sub_queries = max_sub_queries
        self.top_k_per_subquery = top_k_per_subquery
        self.hypothetical_length = hypothetical_length
        self.temperature_subquery = temperature_subquery
        self.temperature_hyde = temperature_hyde
        self.temperature_stepback = temperature_stepback
        self.answer_temperature = answer_temperature
        self.enable_parallel = enable_parallel
    
    def _generate_sub_queries(self, question: str) -> List[str]:
        """ç”Ÿæˆå­å•é¡Œï¼ˆSubQueryï¼‰"""
        is_chinese = PromptFormatter.detect_language(question) == "zh"
        
        if is_chinese:
            prompt = f"""ä½ æ˜¯ä¸€å€‹å°ˆæ¥­åŠ©ç†ã€‚è«‹å°‡ä»¥ä¸‹åŸå§‹å•é¡Œæ‹†è§£æˆæœ€å¤š {self.max_sub_queries} å€‹å…·é«”çš„å­å•é¡Œï¼Œä»¥ä¾¿é€²è¡Œè³‡æ–™æœå°‹ã€‚
æ¯å€‹å­å•é¡Œæ‡‰å°ˆæ³¨æ–¼åŸå§‹å•é¡Œçš„ä¸€å€‹ç‰¹å®šé¢å‘ã€‚è«‹ä»¥æ›è¡Œç¬¦è™Ÿåˆ†éš”å•é¡Œã€‚

åŸå§‹å•é¡Œ: {question}

å­å•é¡Œæ¸…å–®:"""
        else:
            prompt = f"""You are a professional assistant. Please decompose the following original question into at most {self.max_sub_queries} specific sub-questions for information retrieval.
Each sub-question should focus on a specific aspect of the original question. Please separate questions with newlines.

Original question: {question}

Sub-question list:"""
        
        try:
            response = self.llm.generate(
                prompt=prompt,
                temperature=self.temperature_subquery,
                max_tokens=500
            )
            
            sub_queries = [
                q.strip() 
                for q in response.strip().split("\n") 
                if q.strip() and not q.strip().startswith("#")
            ]
            
            # ç§»é™¤ç·¨è™Ÿå‰ç¶´
            cleaned_queries = []
            for q in sub_queries:
                q = q.lstrip("0123456789. )")
                q = q.strip()
                if q:
                    cleaned_queries.append(q)
            
            cleaned_queries = cleaned_queries[:self.max_sub_queries]
            
            if not cleaned_queries:
                logger.warning("âš ï¸  æœªç”Ÿæˆå­å•é¡Œï¼Œä½¿ç”¨åŸå§‹å•é¡Œ")
                cleaned_queries = [question]
            
            return cleaned_queries
            
        except Exception as e:
            logger.error(f"âš ï¸  ç”Ÿæˆå­å•é¡Œæ™‚å‡ºéŒ¯: {e}")
            return [question]
    
    def _generate_hypothetical_document(self, sub_query: str) -> str:
        """ç‚ºå­å•é¡Œç”Ÿæˆå‡è¨­æ€§æ–‡æª”ï¼ˆHyDEï¼‰"""
        is_chinese = PromptFormatter.detect_language(sub_query) == "zh"
        
        if is_chinese:
            prompt = f"""è«‹é‡å°ä»¥ä¸‹å•é¡Œï¼Œå¯«å‡ºä¸€æ®µç´„ {self.hypothetical_length} å­—çš„å°ˆæ¥­æŠ€è¡“æª”æ¡ˆå…§å®¹ã€‚
é€™æ®µå…§å®¹æ‡‰åŒ…å«è©²é ˜åŸŸå¸¸è¦‹çš„å°ˆæ¥­è¡“èªèˆ‡åŸç†èªªæ˜ï¼Œä»¥ä¾¿ç”¨æ–¼å¾ŒçºŒçš„èªç¾©æª¢ç´¢ã€‚
è«‹ä½¿ç”¨å°ˆæ¥­çš„è¡“èªå’Œæ¦‚å¿µï¼Œå³ä½¿ä½ å°æŸäº›ç´°ç¯€ä¸ç¢ºå®šï¼Œä¹Ÿè¦åŒ…å«ç›¸é—œçš„å°ˆæ¥­è©å½™ã€‚

å•é¡Œ: {sub_query}

å°ˆæ¥­æŠ€è¡“å…§å®¹ï¼š"""
        else:
            prompt = f"""Please write a professional technical document of approximately {self.hypothetical_length} words in response to the following question.
This content should include common professional terminology and principle explanations in this field, to be used for subsequent semantic retrieval.
Please use professional terms and concepts, and include relevant professional vocabulary even if you are uncertain about some details.

Question: {sub_query}

Professional technical content:"""
        
        try:
            hypothetical_doc = self.llm.generate(
                prompt=prompt,
                temperature=self.temperature_hyde,
                max_tokens=500
            )
            
            hypothetical_doc = hypothetical_doc.strip()
            
            if not hypothetical_doc:
                logger.warning(f"âš ï¸  å­å•é¡Œ '{sub_query}' çš„å‡è¨­æ€§æ–‡æª”ç‚ºç©ºï¼Œä½¿ç”¨å­å•é¡Œæœ¬èº«")
                return sub_query
            
            return hypothetical_doc
            
        except Exception as e:
            logger.error(f"âš ï¸  ç”Ÿæˆå‡è¨­æ€§æ–‡æª”æ™‚å‡ºéŒ¯: {e}")
            return sub_query
    
    def _generate_step_back_question(self, question: str) -> str:
        """ç”Ÿæˆ Step-back æŠ½è±¡å•é¡Œ"""
        is_chinese = PromptFormatter.detect_language(question) == "zh"
        
        if is_chinese:
            prompt = f"""ä½ æ˜¯ä¸€å€‹è³‡æ·±å°ˆå®¶ã€‚è«‹å°‡ä»¥ä¸‹å…·é«”å•é¡Œè½‰æ›ç‚ºä¸€å€‹æ›´æŠ½è±¡ã€æ›´åŸºç¤çš„åŸç†æ€§å•é¡Œã€‚
é€™å€‹æŠ½è±¡å•é¡Œæ‡‰è©²å¹«åŠ©ç†è§£è©²é ˜åŸŸçš„åŸºç¤æ¦‚å¿µå’ŒåŸç†ï¼Œè€Œä¸æ˜¯ç›´æ¥å›ç­”å…·é«”å•é¡Œã€‚

å…·é«”å•é¡Œ: {question}

è«‹ç”Ÿæˆä¸€å€‹æŠ½è±¡å•é¡Œï¼Œç”¨æ–¼æª¢ç´¢ç›¸é—œçš„åŸç†å’ŒèƒŒæ™¯çŸ¥è­˜ï¼š
"""
        else:
            prompt = f"""You are a senior expert. Please convert the following specific question into a more abstract, fundamental question about principles and concepts.
This abstract question should help understand the basic concepts and principles in this field, rather than directly answering the specific question.

Specific question: {question}

Please generate an abstract question for retrieving relevant principles and background knowledge:
"""
        
        try:
            abstract_question = self.llm.generate(
                prompt=prompt,
                temperature=self.temperature_stepback,
                max_tokens=200
            )
            
            abstract_question = abstract_question.strip()
            
            if not abstract_question:
                logger.warning("âš ï¸  ç”Ÿæˆçš„æŠ½è±¡å•é¡Œç‚ºç©ºï¼Œä½¿ç”¨åŸå§‹å•é¡Œ")
                return question
            
            return abstract_question
            
        except Exception as e:
            logger.error(f"âš ï¸  ç”ŸæˆæŠ½è±¡å•é¡Œæ™‚å‡ºéŒ¯: {e}")
            return question
    
    def _get_doc_id(self, doc: Dict) -> str:
        """ç”Ÿæˆæ–‡æª”çš„å”¯ä¸€æ¨™è­˜ç¬¦"""
        metadata = doc.get("metadata", {})
        content = doc.get("content", "")
        
        if "arxiv_id" in metadata and "chunk_index" in metadata:
            return f"{metadata['arxiv_id']}_{metadata['chunk_index']}"
        elif "file_path" in metadata and "chunk_index" in metadata:
            return f"{metadata['file_path']}_{metadata['chunk_index']}"
        else:
            content_hash = hashlib.md5(content.encode()).hexdigest()[:16]
            return f"doc_{content_hash}"
    
    def _process_subquery_with_hyde(
        self, 
        sub_query: str, 
        metadata_filter: Optional[Dict] = None
    ) -> tuple:
        """è™•ç†å–®å€‹å­å•é¡Œï¼šç”Ÿæˆå‡è¨­æ€§æ–‡æª”ä¸¦æª¢ç´¢"""
        try:
            hypothetical_doc = self._generate_hypothetical_document(sub_query)
            results = self.vector_retriever.retrieve(
                query=hypothetical_doc,
                top_k=self.top_k_per_subquery,
                metadata_filter=metadata_filter
            )
            return results, hypothetical_doc
        except Exception as e:
            logger.error(f"âš ï¸  è™•ç†å­å•é¡Œ '{sub_query}' æ™‚å‡ºéŒ¯: {e}")
            return [], ""
    
    def query(
        self,
        question: str,
        top_k: int = 5,
        metadata_filter: Optional[Dict] = None,
        return_sub_queries: bool = False,
        return_hypothetical: bool = False,
        return_abstract_question: bool = False
    ) -> Dict:
        """
        åŸ·è¡Œä¸‰é‡æ··åˆ RAG æª¢ç´¢
        
        æµç¨‹ï¼š
        1. æ‹†è§£æˆå­å•é¡Œï¼ˆSubQueryï¼‰
        2. å°æ¯å€‹å­å•é¡Œç”Ÿæˆå‡è¨­æ€§æ–‡æª”ä¸¦æª¢ç´¢ï¼ˆHyDEï¼‰
        3. ç›´æ¥æª¢ç´¢åŸå§‹å•é¡Œï¼ˆå…·é«”äº‹å¯¦ï¼‰
        4. ç”ŸæˆæŠ½è±¡å•é¡Œä¸¦æª¢ç´¢ï¼ˆStep-backï¼ŒæŠ½è±¡åŸç†ï¼‰
        5. åˆä½µæ‰€æœ‰çµæœä¸¦å»é‡
        """
        start_time = time.time()
        
        # ç¬¬ä¸€æ­¥ï¼šç”Ÿæˆå­å•é¡Œ
        logger.info(f"ğŸ” [SubQuery] æ‹†è§£å•é¡Œ: '{question}'")
        sub_queries = self._generate_sub_queries(question)
        logger.info(f"âœ… ç”Ÿæˆ {len(sub_queries)} å€‹å­å•é¡Œ")
        
        # ç¬¬äºŒæ­¥ï¼šç‚ºæ¯å€‹å­å•é¡Œç”Ÿæˆå‡è¨­æ€§æ–‡æª”ä¸¦æª¢ç´¢ï¼ˆHyDEï¼‰
        logger.info(f"ğŸ“š [HyDE] ç‚ºæ¯å€‹å­å•é¡Œç”Ÿæˆå‡è¨­æ€§æ–‡æª”ä¸¦æª¢ç´¢...")
        subquery_results = []
        hypothetical_docs = {}
        
        if self.enable_parallel and len(sub_queries) > 1:
            with ThreadPoolExecutor(max_workers=min(len(sub_queries), 5)) as executor:
                future_to_query = {
                    executor.submit(self._process_subquery_with_hyde, sq, metadata_filter): sq
                    for sq in sub_queries
                }
                
                for future in as_completed(future_to_query):
                    sub_query = future_to_query[future]
                    try:
                        results, hypo_doc = future.result()
                        hypothetical_docs[sub_query] = hypo_doc
                        subquery_results.extend(results)
                    except Exception as e:
                        logger.error(f"âš ï¸  è™•ç†å­å•é¡Œ '{sub_query}' æ™‚å‡ºéŒ¯: {e}")
        else:
            for sub_query in sub_queries:
                results, hypo_doc = self._process_subquery_with_hyde(sub_query, metadata_filter)
                hypothetical_docs[sub_query] = hypo_doc
                subquery_results.extend(results)
        
        # ç¬¬ä¸‰æ­¥ï¼šStep-back é›™è»Œæª¢ç´¢
        logger.info(f"ğŸ” [Step-back] åŸ·è¡Œé›™è»Œæª¢ç´¢...")
        
        if self.enable_parallel:
            with ThreadPoolExecutor(max_workers=2) as executor:
                direct_future = executor.submit(
                    self.vector_retriever.retrieve,
                    question, top_k, metadata_filter
                )
                abstract_question = self._generate_step_back_question(question)
                step_back_future = executor.submit(
                    self.vector_retriever.retrieve,
                    abstract_question, top_k, metadata_filter
                )
                
                specific_results = direct_future.result()
                abstract_results = step_back_future.result()
        else:
            specific_results = self.vector_retriever.retrieve(
                query=question,
                top_k=top_k,
                metadata_filter=metadata_filter
            )
            abstract_question = self._generate_step_back_question(question)
            abstract_results = self.vector_retriever.retrieve(
                query=abstract_question,
                top_k=top_k,
                metadata_filter=metadata_filter
            )
        
        # ç¬¬å››æ­¥ï¼šåˆä½µæ‰€æœ‰çµæœä¸¦å»é‡
        logger.info(f"ğŸ”„ åˆä½µä¸¦å»é‡æ‰€æœ‰æª¢ç´¢çµæœ...")
        all_results = subquery_results + specific_results + abstract_results
        unique_docs = {}
        
        for doc in all_results:
            doc_id = self._get_doc_id(doc)
            if doc_id not in unique_docs:
                unique_docs[doc_id] = doc
            else:
                # ä¿ç•™åˆ†æ•¸æ›´é«˜çš„
                existing_score = unique_docs[doc_id].get('score', 0)
                new_score = doc.get('score', 0)
                if new_score > existing_score:
                    unique_docs[doc_id] = doc
        
        # æ’åºä¸¦è¿”å›å‰ top_k
        result_list = list(unique_docs.values())
        result_list.sort(key=lambda x: x.get('score', 0), reverse=True)
        final_results = result_list[:top_k]
        
        elapsed_time = time.time() - start_time
        logger.info(
            f"âœ… ä¸‰é‡æ··åˆæª¢ç´¢å®Œæˆï¼ˆè€—æ™‚: {elapsed_time:.2f}sï¼‰\n"
            f"   å­å•é¡Œæª¢ç´¢: {len(subquery_results)} å€‹çµæœ\n"
            f"   å…·é«”äº‹å¯¦: {len(specific_results)} å€‹çµæœ\n"
            f"   æŠ½è±¡åŸç†: {len(abstract_results)} å€‹çµæœ\n"
            f"   å»é‡å¾Œç¸½è¨ˆ: {len(result_list)} å€‹ï¼Œè¿”å›å‰ {len(final_results)} å€‹"
        )
        
        return {
            "results": final_results,
            "total_docs_found": len(result_list),
            "sub_queries": sub_queries if return_sub_queries else None,
            "hypothetical_documents": hypothetical_docs if return_hypothetical else None,
            "abstract_question": abstract_question if return_abstract_question else None,
            "subquery_results": subquery_results,
            "specific_context": specific_results,
            "abstract_context": abstract_results,
            "question": question,
            "elapsed_time": elapsed_time
        }
    
    def generate_answer(
        self,
        question: str,
        formatter: PromptFormatter,
        top_k: int = 5,
        metadata_filter: Optional[Dict] = None,
        document_type: str = "general",
        return_sub_queries: bool = False,
        return_hypothetical: bool = False,
        return_abstract_question: bool = False
    ) -> Dict:
        """
        å®Œæ•´çš„ä¸‰é‡æ··åˆ RAG æµç¨‹ï¼šæª¢ç´¢ + ç”Ÿæˆç­”æ¡ˆ
        """
        start_time = time.time()
        
        # æª¢ç´¢
        retrieval_result = self.query(
            question=question,
            top_k=top_k,
            metadata_filter=metadata_filter,
            return_sub_queries=return_sub_queries,
            return_hypothetical=return_hypothetical,
            return_abstract_question=return_abstract_question
        )
        
        if not retrieval_result["results"]:
            return {
                **retrieval_result,
                "answer": "æŠ±æ­‰ï¼Œæœªæ‰¾åˆ°ç›¸é—œæ–‡æª”ä¾†å›ç­”æ­¤å•é¡Œã€‚",
                "formatted_context": None,
                "answer_time": 0.0,
                "total_time": retrieval_result["elapsed_time"]
            }
        
        # æ ¼å¼åŒ–ä¸‰é¡ä¸Šä¸‹æ–‡
        subquery_context = formatter.format_context(
            retrieval_result["subquery_results"][:top_k],
            document_type=document_type
        ) if retrieval_result.get("subquery_results") else "æœªæ‰¾åˆ°ç›¸é—œçš„å­å•é¡Œæª¢ç´¢çµæœã€‚"
        
        specific_context = formatter.format_context(
            retrieval_result["specific_context"],
            document_type=document_type
        ) if retrieval_result.get("specific_context") else "æœªæ‰¾åˆ°ç›¸é—œçš„å…·é«”äº‹å¯¦è³‡æ–™ã€‚"
        
        abstract_context = formatter.format_context(
            retrieval_result["abstract_context"],
            document_type=document_type
        ) if retrieval_result.get("abstract_context") else "æœªæ‰¾åˆ°ç›¸é—œçš„åŸºç¤åŸç†è³‡æ–™ã€‚"
        
        # å‰µå»ºèåˆæç¤ºè©ï¼ˆé—œéµæ­¥é©Ÿï¼‰
        is_chinese = PromptFormatter.detect_language(question) == "zh"
        
        if is_chinese:
            final_prompt = f"""ä½ æ˜¯ä¸€å€‹è³‡æ·±å°ˆå®¶ã€‚è«‹çµåˆä»¥ä¸‹ä¸‰é¡è³‡è¨Šä¾†å›ç­”ä½¿ç”¨è€…çš„å…·é«”å•é¡Œã€‚

ã€åŸºç¤åŸç†èˆ‡èƒŒæ™¯ã€‘ï¼ˆä¾†è‡ª Step-back æŠ½è±¡å•é¡Œæª¢ç´¢ï¼‰
{abstract_context}

ã€å…·é«”äº‹å¯¦è³‡æ–™ã€‘ï¼ˆä¾†è‡ªç›´æ¥å•é¡Œæª¢ç´¢ï¼‰
{specific_context}

ã€å­å•é¡Œç›¸é—œè³‡æ–™ã€‘ï¼ˆä¾†è‡ª SubQuery + HyDE æª¢ç´¢ï¼‰
{subquery_context}

ä½¿ç”¨è€…å•é¡Œï¼š{question}

è«‹æ ¹æ“šåŸç†æ¨å°ã€çµåˆå…·é«”äº‹å¯¦ï¼Œä¸¦åƒè€ƒå­å•é¡Œçš„ç›¸é—œè³‡æ–™ï¼Œçµ¦å‡ºä¸€å€‹å°ˆæ¥­ã€å…¨é¢ä¸”å…·å‚™é‚è¼¯çš„å›ç­”ï¼š
"""
        else:
            final_prompt = f"""You are a senior expert. Please answer the user's specific question by combining the following three types of information.

ã€Fundamental Principles and Backgroundã€‘(from Step-back abstract question retrieval)
{abstract_context}

ã€Specific Facts and Dataã€‘(from direct question retrieval)
{specific_context}

ã€Sub-question Related Informationã€‘(from SubQuery + HyDE retrieval)
{subquery_context}

User question: {question}

Please provide a professional, comprehensive, and logical answer based on principles, facts, and sub-question related information:
"""
        
        # ç”Ÿæˆå›ç­”
        logger.info("ğŸ¤– ç”Ÿæˆå›ç­”ä¸­...")
        answer_start = time.time()
        try:
            answer = self.llm.generate(
                prompt=final_prompt,
                temperature=self.answer_temperature,
                max_tokens=2048
            )
            answer_time = time.time() - answer_start
            logger.info(f"âœ… å›ç­”ç”Ÿæˆå®Œæˆï¼ˆè€—æ™‚: {answer_time:.2f}sï¼‰")
        except Exception as e:
            logger.error(f"âŒ ç”Ÿæˆå›ç­”æ™‚å‡ºéŒ¯: {e}")
            answer = f"ç”Ÿæˆå›ç­”æ™‚å‡ºéŒ¯: {e}"
            answer_time = time.time() - answer_start
        
        total_time = time.time() - start_time
        
        return {
            **retrieval_result,
            "answer": answer,
            "formatted_context": {
                "subquery": subquery_context,
                "specific": specific_context,
                "abstract": abstract_context
            },
            "answer_time": answer_time,
            "total_time": total_time
        }

