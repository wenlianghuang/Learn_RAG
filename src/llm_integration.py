"""
LLM é›†æˆæ¨¡çµ„ï¼šä½¿ç”¨ Ollama é€²è¡Œæœ¬åœ° LLM æ¨ç†
"""
from typing import Optional, Dict, List
import logging
import requests
import json

logger = logging.getLogger(__name__)


class OllamaLLM:
    """ä½¿ç”¨ Ollama é€²è¡Œæœ¬åœ° LLM æ¨ç†"""
    
    # é©åˆ 16GB MacBook Air çš„æ¨¡å‹æ¨è–¦
    RECOMMENDED_MODELS = {
        "llama3.2:3b": {
            "name": "llama3.2:3b",
            "description": "Meta Llama 3.2 3B - è¼•é‡ç´šï¼Œé©åˆ 16GB å…§å­˜",
            "memory_required": "~4GB",
            "quality": "è‰¯å¥½"
        },
        "llama3.2:1b": {
            "name": "llama3.2:1b",
            "description": "Meta Llama 3.2 1B - æ¥µè¼•é‡ç´šï¼Œå¿«é€ŸéŸ¿æ‡‰",
            "memory_required": "~2GB",
            "quality": "åŸºç¤"
        },
        "phi3:mini": {
            "name": "phi3:mini",
            "description": "Microsoft Phi-3 Mini - å°æ¨¡å‹ï¼Œé«˜è³ªé‡",
            "memory_required": "~3GB",
            "quality": "è‰¯å¥½"
        },
        "gemma:2b": {
            "name": "gemma:2b",
            "description": "Google Gemma 2B - è¼•é‡ç´šï¼Œé–‹æº",
            "memory_required": "~3GB",
            "quality": "è‰¯å¥½"
        },
        "mistral:7b": {
            "name": "mistral:7b",
            "description": "Mistral 7B - è¼ƒå¤§ä½†è³ªé‡é«˜ï¼ˆå¦‚æœå…§å­˜è¶³å¤ ï¼‰",
            "memory_required": "~8GB",
            "quality": "å„ªç§€"
        }
    }
    
    def __init__(
        self,
        model_name: str = "llama3.2:3b",
        base_url: str = "http://localhost:11434",
        timeout: int = 120
    ):
        """
        åˆå§‹åŒ– Ollama LLM
        
        Args:
            model_name: Ollama æ¨¡å‹åç¨±ï¼ˆé è¨­: llama3.2:3bï¼‰
            base_url: Ollama API åŸºç¤ URL
            timeout: è«‹æ±‚è¶…æ™‚æ™‚é–“ï¼ˆç§’ï¼‰
        """
        self.model_name = model_name
        self.base_url = base_url.rstrip('/')
        self.timeout = timeout
        self.api_url = f"{self.base_url}/api"
        
        # æª¢æŸ¥æ¨¡å‹æ˜¯å¦åœ¨æ¨è–¦åˆ—è¡¨ä¸­
        if model_name not in self.RECOMMENDED_MODELS:
            logger.warning(
                f"âš ï¸  æ¨¡å‹ '{model_name}' ä¸åœ¨æ¨è–¦åˆ—è¡¨ä¸­ã€‚"
                f"æ¨è–¦çš„æ¨¡å‹: {', '.join(self.RECOMMENDED_MODELS.keys())}"
            )
        
        logger.info(f"âœ… Ollama LLM åˆå§‹åŒ–å®Œæˆ (æ¨¡å‹: {model_name})")
    
    def _check_ollama_connection(self) -> bool:
        """
        æª¢æŸ¥ Ollama æœå‹™æ˜¯å¦å¯ç”¨
        
        Returns:
            æ˜¯å¦é€£æ¥æˆåŠŸ
        """
        try:
            response = requests.get(f"{self.base_url}/api/tags", timeout=5)
            return response.status_code == 200
        except Exception as e:
            logger.error(f"âŒ ç„¡æ³•é€£æ¥åˆ° Ollama: {e}")
            logger.error(f"   è«‹ç¢ºä¿ Ollama æ­£åœ¨é‹è¡Œ: ollama serve")
            return False
    
    def _check_model_available(self) -> bool:
        """
        æª¢æŸ¥æ¨¡å‹æ˜¯å¦å·²ä¸‹è¼‰
        
        Returns:
            æ¨¡å‹æ˜¯å¦å¯ç”¨
        """
        try:
            response = requests.get(f"{self.base_url}/api/tags", timeout=5)
            if response.status_code == 200:
                models = response.json().get('models', [])
                model_names = [m.get('name', '') for m in models]
                return any(self.model_name in name for name in model_names)
            return False
        except Exception as e:
            logger.error(f"âŒ æª¢æŸ¥æ¨¡å‹æ™‚å‡ºéŒ¯: {e}")
            return False
    
    def generate(
        self,
        prompt: str,
        temperature: float = 0.7,
        max_tokens: Optional[int] = None,
        stream: bool = False
    ) -> str:
        """
        ç”Ÿæˆå›ç­”
        
        Args:
            prompt: è¼¸å…¥ prompt
            temperature: æº«åº¦åƒæ•¸ï¼ˆ0.0-1.0ï¼‰ï¼Œæ§åˆ¶éš¨æ©Ÿæ€§
            max_tokens: æœ€å¤§ç”Ÿæˆ token æ•¸ï¼ˆNone è¡¨ç¤ºä½¿ç”¨æ¨¡å‹é è¨­ï¼‰
            stream: æ˜¯å¦ä½¿ç”¨æµå¼è¼¸å‡º
            
        Returns:
            ç”Ÿæˆçš„å›ç­”
        """
        # æª¢æŸ¥é€£æ¥
        if not self._check_ollama_connection():
            raise ConnectionError(
                f"ç„¡æ³•é€£æ¥åˆ° Ollama æœå‹™ ({self.base_url})\n"
                f"è«‹ç¢ºä¿ Ollama æ­£åœ¨é‹è¡Œï¼š\n"
                f"  1. å®‰è£ Ollama: https://ollama.ai\n"
                f"  2. å•Ÿå‹•æœå‹™: ollama serve\n"
                f"  3. ä¸‹è¼‰æ¨¡å‹: ollama pull {self.model_name}"
            )
        
        # æª¢æŸ¥æ¨¡å‹
        if not self._check_model_available():
            logger.warning(
                f"âš ï¸  æ¨¡å‹ '{self.model_name}' å¯èƒ½æœªä¸‹è¼‰ã€‚"
                f"è«‹é‹è¡Œ: ollama pull {self.model_name}"
            )
        
        # æº–å‚™è«‹æ±‚åƒæ•¸
        payload = {
            "model": self.model_name,
            "prompt": prompt,
            "stream": stream,
            "options": {
                "temperature": temperature,
            }
        }
        
        if max_tokens:
            payload["options"]["num_predict"] = max_tokens
        
        try:
            # ç™¼é€è«‹æ±‚
            response = requests.post(
                f"{self.api_url}/generate",
                json=payload,
                timeout=self.timeout,
                stream=stream
            )
            
            if response.status_code != 200:
                error_msg = response.text
                raise RuntimeError(f"Ollama API éŒ¯èª¤: {error_msg}")
            
            if stream:
                # æµå¼è™•ç†
                full_response = ""
                for line in response.iter_lines():
                    if line:
                        try:
                            data = json.loads(line)
                            if 'response' in data:
                                chunk = data['response']
                                full_response += chunk
                                print(chunk, end='', flush=True)
                            if data.get('done', False):
                                break
                        except json.JSONDecodeError:
                            continue
                print()  # æ›è¡Œ
                return full_response
            else:
                # éæµå¼è™•ç†
                data = response.json()
                return data.get('response', '')
                
        except requests.exceptions.Timeout:
            raise TimeoutError(
                f"è«‹æ±‚è¶…æ™‚ï¼ˆ{self.timeout}ç§’ï¼‰ã€‚"
                f"å¯ä»¥å˜—è©¦å¢åŠ  timeout æˆ–ä½¿ç”¨æ›´å°çš„æ¨¡å‹ã€‚"
            )
        except requests.exceptions.ConnectionError:
            raise ConnectionError(
                f"ç„¡æ³•é€£æ¥åˆ° Ollama æœå‹™ã€‚"
                f"è«‹ç¢ºä¿ Ollama æ­£åœ¨é‹è¡Œï¼šollama serve"
            )
        except Exception as e:
            logger.error(f"âŒ ç”Ÿæˆå›ç­”æ™‚å‡ºéŒ¯: {e}")
            raise
    
    def list_available_models(self) -> List[str]:
        """
        åˆ—å‡ºæœ¬åœ°å¯ç”¨çš„æ¨¡å‹
        
        Returns:
            å¯ç”¨æ¨¡å‹åç¨±åˆ—è¡¨
        """
        try:
            response = requests.get(f"{self.base_url}/api/tags", timeout=5)
            if response.status_code == 200:
                models = response.json().get('models', [])
                return [m.get('name', '') for m in models]
            return []
        except Exception as e:
            logger.error(f"âŒ ç²å–æ¨¡å‹åˆ—è¡¨æ™‚å‡ºéŒ¯: {e}")
            return []
    
    @classmethod
    def print_recommended_models(cls):
        """æ‰“å°æ¨è–¦çš„æ¨¡å‹åˆ—è¡¨"""
        print("\n" + "="*60)
        print("é©åˆ 16GB MacBook Air çš„ Ollama æ¨¡å‹æ¨è–¦")
        print("="*60)
        print()
        
        for model_key, info in cls.RECOMMENDED_MODELS.items():
            print(f"ğŸ“¦ {info['name']}")
            print(f"   æè¿°: {info['description']}")
            print(f"   å…§å­˜éœ€æ±‚: {info['memory_required']}")
            print(f"   è³ªé‡: {info['quality']}")
            print(f"   ä¸‹è¼‰å‘½ä»¤: ollama pull {info['name']}")
            print()

