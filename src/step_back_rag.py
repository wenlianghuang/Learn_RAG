"""
Step-back Prompting é›™è»Œ RAGï¼šçµåˆå…·é«”äº‹å¯¦èˆ‡æŠ½è±¡åŸç†
ä½¿ç”¨ Step-back Prompting æŠ€è¡“ï¼ŒåŒæ™‚æª¢ç´¢å…·é«”äº‹å¯¦å’ŒæŠ½è±¡åŸç†ï¼Œæå‡å›ç­”è³ªé‡
"""
from typing import List, Dict, Optional
from .retrievers.reranker import RAGPipeline
from .retrievers.vector_retriever import VectorRetriever
from .prompt_formatter import PromptFormatter
from .llm_integration import OllamaLLM
import time
import logging
import hashlib
from concurrent.futures import ThreadPoolExecutor, as_completed

logger = logging.getLogger(__name__)


class StepBackRAG:
    """ä½¿ç”¨ Step-back Prompting çš„é›™è»Œ RAG ç³»çµ±"""
    
    def __init__(
        self,
        rag_pipeline: RAGPipeline,
        vector_retriever: VectorRetriever,
        llm: OllamaLLM,
        step_back_temperature: float = 0.3,  # ç”ŸæˆæŠ½è±¡å•é¡Œæ™‚ä½¿ç”¨è¼ƒä½æº«åº¦
        answer_temperature: float = 0.7,
        enable_parallel: bool = True
    ):
        """
        åˆå§‹åŒ– Step-back RAG
        
        Args:
            rag_pipeline: RAG ç®¡ç·šå¯¦ä¾‹ï¼ˆç”¨æ–¼æœ€çµ‚ç­”æ¡ˆç”Ÿæˆï¼‰
            vector_retriever: å‘é‡æª¢ç´¢å™¨
            llm: LLM å¯¦ä¾‹
            step_back_temperature: ç”ŸæˆæŠ½è±¡å•é¡Œçš„æº«åº¦ï¼ˆè¼ƒä½ï¼Œæ›´ç©©å®šï¼‰
            answer_temperature: ç”Ÿæˆç­”æ¡ˆçš„æº«åº¦
            enable_parallel: æ˜¯å¦ä¸¦è¡ŒåŸ·è¡Œé›™è»Œæª¢ç´¢
        """
        self.rag_pipeline = rag_pipeline
        self.vector_retriever = vector_retriever
        self.llm = llm
        self.step_back_temperature = step_back_temperature
        self.answer_temperature = answer_temperature
        self.enable_parallel = enable_parallel
    
    def _generate_step_back_question(self, question: str) -> str:
        """
        ç”Ÿæˆ Step-back æŠ½è±¡å•é¡Œ
        
        Args:
            question: åŸå§‹å…·é«”å•é¡Œ
            
        Returns:
            æŠ½è±¡å•é¡Œ
        """
        is_chinese = PromptFormatter.detect_language(question) == "zh"
        
        if is_chinese:
            prompt = f"""ä½ æ˜¯ä¸€å€‹è³‡æ·±å°ˆå®¶ã€‚è«‹å°‡ä»¥ä¸‹å…·é«”å•é¡Œè½‰æ›ç‚ºä¸€å€‹æ›´æŠ½è±¡ã€æ›´åŸºç¤çš„åŸç†æ€§å•é¡Œã€‚
é€™å€‹æŠ½è±¡å•é¡Œæ‡‰è©²å¹«åŠ©ç†è§£è©²é ˜åŸŸçš„åŸºç¤æ¦‚å¿µå’ŒåŸç†ï¼Œè€Œä¸æ˜¯ç›´æ¥å›ç­”å…·é«”å•é¡Œã€‚

å…·é«”å•é¡Œ: {question}

è«‹ç”Ÿæˆä¸€å€‹æŠ½è±¡å•é¡Œï¼Œç”¨æ–¼æª¢ç´¢ç›¸é—œçš„åŸç†å’ŒèƒŒæ™¯çŸ¥è­˜ï¼š
"""
        else:
            prompt = f"""You are a senior expert. Please convert the following specific question into a more abstract, fundamental question about principles and concepts.
This abstract question should help understand the basic concepts and principles in this field, rather than directly answering the specific question.

Specific question: {question}

Please generate an abstract question for retrieving relevant principles and background knowledge:
"""
        
        try:
            abstract_question = self.llm.generate(
                prompt=prompt,
                temperature=self.step_back_temperature,
                max_tokens=200
            )
            
            abstract_question = abstract_question.strip()
            
            if not abstract_question:
                logger.warning("âš ï¸  ç”Ÿæˆçš„æŠ½è±¡å•é¡Œç‚ºç©ºï¼Œä½¿ç”¨åŸå§‹å•é¡Œ")
                return question
            
            logger.info(f"âœ… ç”ŸæˆæŠ½è±¡å•é¡Œ: '{abstract_question}'")
            return abstract_question
            
        except Exception as e:
            logger.error(f"âš ï¸  ç”ŸæˆæŠ½è±¡å•é¡Œæ™‚å‡ºéŒ¯: {e}")
            return question
    
    def _get_doc_id(self, doc: Dict) -> str:
        """
        ç”Ÿæˆæ–‡æª”çš„å”¯ä¸€æ¨™è­˜ç¬¦
        
        Args:
            doc: æ–‡æª”å­—å…¸
            
        Returns:
            å”¯ä¸€ ID
        """
        metadata = doc.get("metadata", {})
        content = doc.get("content", "")
        
        if "arxiv_id" in metadata and "chunk_index" in metadata:
            return f"{metadata['arxiv_id']}_{metadata['chunk_index']}"
        elif "file_path" in metadata and "chunk_index" in metadata:
            return f"{metadata['file_path']}_{metadata['chunk_index']}"
        else:
            content_hash = hashlib.md5(content.encode()).hexdigest()[:16]
            return f"doc_{content_hash}"
    
    def _retrieve_direct(self, question: str, top_k: int, metadata_filter: Optional[Dict] = None) -> List[Dict]:
        """ç›´æ¥æª¢ç´¢åŸå§‹å•é¡Œï¼ˆå…·é«”äº‹å¯¦ï¼‰"""
        return self.vector_retriever.retrieve(
            query=question,
            top_k=top_k,
            metadata_filter=metadata_filter
        )
    
    def _retrieve_step_back(self, question: str, top_k: int, metadata_filter: Optional[Dict] = None) -> tuple:
        """Step-back æª¢ç´¢ï¼ˆæŠ½è±¡åŸç†ï¼‰"""
        abstract_question = self._generate_step_back_question(question)
        results = self.vector_retriever.retrieve(
            query=abstract_question,
            top_k=top_k,
            metadata_filter=metadata_filter
        )
        return results, abstract_question
    
    def query(
        self,
        question: str,
        top_k: int = 5,
        metadata_filter: Optional[Dict] = None,
        return_abstract_question: bool = False
    ) -> Dict:
        """
        åŸ·è¡Œé›™è»Œæª¢ç´¢ï¼ˆä¸ç”Ÿæˆç­”æ¡ˆï¼‰
        
        Args:
            question: åŸå§‹å•é¡Œ
            top_k: æ¯è»Œè¿”å›çš„çµæœæ•¸é‡
            metadata_filter: å¯é¸çš„ metadata éæ¿¾æ¢ä»¶
            return_abstract_question: æ˜¯å¦è¿”å›æŠ½è±¡å•é¡Œ
            
        Returns:
            åŒ…å«é›™è»Œæª¢ç´¢çµæœçš„å­—å…¸
        """
        start_time = time.time()
        
        if self.enable_parallel:
            # ä¸¦è¡ŒåŸ·è¡Œé›™è»Œæª¢ç´¢
            logger.info(f"ğŸ”„ ä¸¦è¡ŒåŸ·è¡Œé›™è»Œæª¢ç´¢: '{question}'")
            with ThreadPoolExecutor(max_workers=2) as executor:
                direct_future = executor.submit(
                    self._retrieve_direct, question, top_k, metadata_filter
                )
                step_back_future = executor.submit(
                    self._retrieve_step_back, question, top_k, metadata_filter
                )
                
                specific_results = direct_future.result()
                abstract_results, abstract_question = step_back_future.result()
        else:
            # ä¸²è¡ŒåŸ·è¡Œ
            logger.info(f"ğŸ”„ ä¸²è¡ŒåŸ·è¡Œé›™è»Œæª¢ç´¢: '{question}'")
            specific_results = self._retrieve_direct(question, top_k, metadata_filter)
            abstract_results, abstract_question = self._retrieve_step_back(question, top_k, metadata_filter)
        
        elapsed_time = time.time() - start_time
        logger.info(
            f"âœ… é›™è»Œæª¢ç´¢å®Œæˆï¼ˆè€—æ™‚: {elapsed_time:.2f}sï¼‰\n"
            f"   å…·é«”äº‹å¯¦: {len(specific_results)} å€‹çµæœ\n"
            f"   æŠ½è±¡åŸç†: {len(abstract_results)} å€‹çµæœ"
        )
        
        return {
            "specific_context": specific_results,
            "abstract_context": abstract_results,
            "abstract_question": abstract_question if return_abstract_question else None,
            "question": question,
            "elapsed_time": elapsed_time
        }
    
    def generate_answer(
        self,
        question: str,
        formatter: PromptFormatter,
        top_k: int = 5,
        metadata_filter: Optional[Dict] = None,
        document_type: str = "general",
        return_abstract_question: bool = False
    ) -> Dict:
        """
        å®Œæ•´çš„ Step-back RAG æµç¨‹ï¼šé›™è»Œæª¢ç´¢ -> ç”Ÿæˆç­”æ¡ˆ
        
        Args:
            question: åŸå§‹å•é¡Œ
            formatter: Prompt æ ¼å¼åŒ–å™¨
            top_k: æ¯è»Œç”¨æ–¼ç”Ÿæˆç­”æ¡ˆçš„æ–‡æª”æ•¸é‡
            metadata_filter: å¯é¸çš„ metadata éæ¿¾æ¢ä»¶
            document_type: æ–‡æª”é¡å‹ ("paper", "cv", "general")
            return_abstract_question: æ˜¯å¦è¿”å›æŠ½è±¡å•é¡Œ
            
        Returns:
            åŒ…å«æª¢ç´¢çµæœã€ç”Ÿæˆçš„ç­”æ¡ˆå’Œçµ±è¨ˆè³‡è¨Šçš„å­—å…¸
        """
        start_time = time.time()
        
        # ç¬¬ä¸€æ­¥ï¼šé›™è»Œæª¢ç´¢
        retrieval_result = self.query(
            question=question,
            top_k=top_k,
            metadata_filter=metadata_filter,
            return_abstract_question=return_abstract_question
        )
        
        specific_results = retrieval_result["specific_context"]
        abstract_results = retrieval_result["abstract_context"]
        
        if not specific_results and not abstract_results:
            return {
                **retrieval_result,
                "answer": "æŠ±æ­‰ï¼Œæœªæ‰¾åˆ°ç›¸é—œæ–‡æª”ä¾†å›ç­”æ­¤å•é¡Œã€‚",
                "formatted_context": None,
                "answer_time": 0.0,
                "total_time": retrieval_result["elapsed_time"]
            }
        
        # ç¬¬äºŒæ­¥ï¼šæ ¼å¼åŒ–é›™è»Œä¸Šä¸‹æ–‡
        specific_context = formatter.format_context(
            specific_results,
            document_type=document_type
        ) if specific_results else "æœªæ‰¾åˆ°ç›¸é—œçš„å…·é«”äº‹å¯¦è³‡æ–™ã€‚"
        
        abstract_context = formatter.format_context(
            abstract_results,
            document_type=document_type
        ) if abstract_results else "æœªæ‰¾åˆ°ç›¸é—œçš„åŸºç¤åŸç†è³‡æ–™ã€‚"
        
        # ç¬¬ä¸‰æ­¥ï¼šå‰µå»ºèåˆæç¤ºè©ï¼ˆé—œéµæ­¥é©Ÿï¼‰
        is_chinese = PromptFormatter.detect_language(question) == "zh"
        
        if is_chinese:
            final_prompt = f"""ä½ æ˜¯ä¸€å€‹è³‡æ·±å°ˆå®¶ã€‚è«‹çµåˆä»¥ä¸‹å…©é¡è³‡è¨Šä¾†å›ç­”ä½¿ç”¨è€…çš„å…·é«”å•é¡Œã€‚

ã€åŸºç¤åŸç†èˆ‡èƒŒæ™¯ã€‘
{abstract_context}

ã€å…·é«”äº‹å¯¦è³‡æ–™ã€‘
{specific_context}

ä½¿ç”¨è€…å•é¡Œï¼š{question}

è«‹æ ¹æ“šåŸç†æ¨å°ä¸¦çµåˆäº‹å¯¦ï¼Œçµ¦å‡ºä¸€å€‹å°ˆæ¥­ä¸”å…·å‚™é‚è¼¯çš„å›ç­”ï¼š
"""
        else:
            final_prompt = f"""You are a senior expert. Please answer the user's specific question by combining the following two types of information.

ã€Fundamental Principles and Backgroundã€‘
{abstract_context}

ã€Specific Facts and Dataã€‘
{specific_context}

User question: {question}

Please provide a professional and logical answer based on principles and facts:
"""
        
        # ç¬¬å››æ­¥ï¼šç”Ÿæˆå›ç­”
        logger.info("ğŸ¤– ç”Ÿæˆå›ç­”ä¸­...")
        answer_start = time.time()
        try:
            answer = self.llm.generate(
                prompt=final_prompt,
                temperature=self.answer_temperature,
                max_tokens=2048
            )
            answer_time = time.time() - answer_start
            logger.info(f"âœ… å›ç­”ç”Ÿæˆå®Œæˆï¼ˆè€—æ™‚: {answer_time:.2f}sï¼‰")
        except Exception as e:
            logger.error(f"âŒ ç”Ÿæˆå›ç­”æ™‚å‡ºéŒ¯: {e}")
            answer = f"ç”Ÿæˆå›ç­”æ™‚å‡ºéŒ¯: {e}"
            answer_time = time.time() - answer_start
        
        total_time = time.time() - start_time
        
        return {
            **retrieval_result,
            "answer": answer,
            "formatted_context": {
                "specific": specific_context,
                "abstract": abstract_context
            },
            "answer_time": answer_time,
            "total_time": total_time
        }

